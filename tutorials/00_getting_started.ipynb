{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started with `sbi`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, you can find the original version of this notebook at [https://github.com/sbi-dev/sbi/blob/main/tutorials/00_getting_started.ipynb](https://github.com/sbi-dev/sbi/blob/main/tutorials/00_getting_started.ipynb) in the `sbi` repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sbi import utils as utils\n",
    "from sbi import analysis as analysis\n",
    "from sbi.inference.base import infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import pytest\n",
    "import torch\n",
    "from torch import eye, ones, zeros\n",
    "from torch.distributions import MultivariateNormal\n",
    "\n",
    "from sbi import utils\n",
    "from sbi.inference import SNLE, SNPE, SNRE\n",
    "from sbi.neural_nets.embedding_nets import (\n",
    "    CNNEmbedding,\n",
    "    FCEmbedding,\n",
    "    PermutationInvariantEmbedding,\n",
    ")\n",
    "from sbi.simulators.linear_gaussian import (\n",
    "    linear_gaussian,\n",
    "    true_posterior_linear_gaussian_mvn_prior,\n",
    ")\n",
    "from sbi.utils import classifier_nn, likelihood_nn, posterior_nn\n",
    "\n",
    "import pytest\n",
    "from torch import eye, ones, zeros\n",
    "from torch.distributions import MultivariateNormal\n",
    "\n",
    "from sbi.inference import (\n",
    "    SNPE_A,\n",
    "    SNPE_C,\n",
    "    DirectPosterior,\n",
    "    prepare_for_sbi,\n",
    "    simulate_for_sbi,\n",
    ")\n",
    "from sbi.simulators.linear_gaussian import diagonal_linear_gaussian\n",
    "\n",
    "from sbi.utils.metrics import c2st\n",
    "\n",
    "from sbi.inference import (\n",
    "    SNLE,\n",
    "    SNPE,\n",
    "    DirectPosterior,\n",
    "    MCMCPosterior,\n",
    "    likelihood_estimator_based_potential,\n",
    "    prepare_for_sbi,\n",
    "    simulate_for_sbi,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/sbi/sbi/utils/user_input_checks.py:209: UserWarning: Casting 1D Uniform prior to BoxUniform to match sbi batch requirements.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "801c6bc1be314eb590d3950a01088e75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 100 simulations.:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/sbi3/lib/python3.11/site-packages/torch/nn/init.py:452: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Neural network successfully converged after 122 epochs."
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce0f2dcf45d94678bb7f3fec11fc869e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Drawing 100 posterior samples:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 100, 1])\n",
      "torch.Size([1, 100])\n",
      "torch.Size([68, 1])\n",
      "torch.Size([1, 100])\n",
      "torch.Size([1, 100, 1])\n",
      "torch.Size([1, 100])\n",
      "torch.Size([73, 1])\n",
      "torch.Size([1, 100])\n",
      "torch.Size([1, 10000, 1])\n",
      "torch.Size([1, 10000])\n",
      "torch.Size([7108, 1])\n",
      "torch.Size([1, 10000])\n",
      "torch.Size([1, 6102, 1])\n",
      "torch.Size([1, 6102])\n",
      "torch.Size([4313, 1])\n",
      "torch.Size([1, 6102])\n"
     ]
    }
   ],
   "source": [
    "from torch.distributions import MultivariateNormal, Uniform\n",
    "num_dim = 1\n",
    "x_o = torch.tensor([[1.0]])\n",
    "num_samples = 100\n",
    "\n",
    "# likelihood_mean will be likelihood_shift+theta\n",
    "likelihood_shift = -1.0 * ones(num_dim)\n",
    "likelihood_cov = 0.3 * eye(num_dim)\n",
    "\n",
    "prior = Uniform(low=torch.zeros(num_dim), high=torch.ones(num_dim))\n",
    "\n",
    "def simulator(theta: Tensor) -> Tensor:\n",
    "    return linear_gaussian(theta, likelihood_shift, likelihood_cov)\n",
    "\n",
    "simulator, prior = prepare_for_sbi(simulator, prior)\n",
    "inference = SNPE(density_estimator=\"mdn\")\n",
    "\n",
    "theta, x = simulate_for_sbi(simulator, prior, 100)\n",
    "posterior_estimator = inference.append_simulations(theta, x).train()\n",
    "posterior = DirectPosterior(posterior_estimator=posterior_estimator, prior=prior)\n",
    "samples = posterior.sample((num_samples,), x=x_o)\n",
    "log_probs = posterior.log_prob(samples, x=x_o)\n",
    "\n",
    "assert log_probs.shape == torch.Size([num_samples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([], size=(100, 0))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 32])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posterior.posterior_estimator.sample((10,), condition=xo).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15bfe30b17e944689124849428a21b1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Drawing 1 posterior samples:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posterior.sample((1,), x=x_o).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_o.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the inference procedure\n",
    "\n",
    "`sbi` provides a simple interface to run state-of-the-art algorithms for simulation-based inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For inference, you need to provide two ingredients:\n",
    "\n",
    "1) a prior distribution that allows to sample parameter sets.  \n",
    "2) a simulator that takes parameter sets and produces simulation outputs.\n",
    "\n",
    "For example, we can have a 3-dimensional parameter space with a uniform prior between [-1,1] and a simple simulator that for the sake of example adds 1.0 and some Gaussian noise to the parameter set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_dim = 3\n",
    "prior = utils.BoxUniform(low=-2 * torch.ones(num_dim), high=2 * torch.ones(num_dim))\n",
    "\n",
    "def simulator(parameter_set):\n",
    "    return 1.0 + parameter_set + torch.randn(parameter_set.shape) * 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sbi` can then run inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06a809bd82b74436ae8f42373fb70254",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 1000 simulations.:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Neural network successfully converged after 67 epochs."
     ]
    }
   ],
   "source": [
    "# Other methods are \"SNLE\" or \"SNRE\".\n",
    "posterior = infer(simulator, prior, method=\"SNPE\", num_simulations=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we have made some observation $x$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = torch.zeros(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Given this observation, we can then sample from the posterior $p(\\theta|x)$, evaluate its log-probability, or plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e382e3b345247018d7668ea40192143",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Only 0.010% proposal samples are\n",
      "                    accepted. It may take a long time to collect the remaining\n",
      "                    9999 samples. Consider interrupting (Ctrl-C) and switching to\n",
      "                    `build_posterior(..., sample_with='mcmc')`.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m samples \u001b[38;5;241m=\u001b[39m \u001b[43mposterior\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobservation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m log_probability \u001b[38;5;241m=\u001b[39m posterior\u001b[38;5;241m.\u001b[39mlog_prob(samples, x\u001b[38;5;241m=\u001b[39mobservation)\n\u001b[1;32m      3\u001b[0m _ \u001b[38;5;241m=\u001b[39m analysis\u001b[38;5;241m.\u001b[39mpairplot(samples, limits\u001b[38;5;241m=\u001b[39m[[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m], [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m], [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m]], figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n",
      "File \u001b[0;32m~/sbi/sbi/inference/posteriors/direct_posterior.py:118\u001b[0m, in \u001b[0;36mDirectPosterior.sample\u001b[0;34m(self, sample_shape, x, max_sampling_batch_size, sample_with, show_progress_bars)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_with \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    113\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou set `sample_with=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msample_with\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`. As of sbi v0.18.0, setting \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`sample_with` is no longer supported. You have to rerun \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`.build_posterior(sample_with=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msample_with\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m).`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    116\u001b[0m     )\n\u001b[0;32m--> 118\u001b[0m samples \u001b[38;5;241m=\u001b[39m \u001b[43maccept_reject_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproposal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mposterior_estimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_reject_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mwithin_support\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprior\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_progress_bars\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_sampling_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_sampling_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproposal_sampling_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcondition\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43malternative_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbuild_posterior(..., sample_with=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmcmc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m samples\n",
      "File \u001b[0;32m~/miniconda3/envs/sbi3/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/sbi/sbi/samplers/rejection/rejection.py:262\u001b[0m, in \u001b[0;36maccept_reject_sample\u001b[0;34m(proposal, accept_reject_fn, num_samples, show_progress_bars, warn_acceptance, sample_for_correction_factor, max_sampling_batch_size, proposal_sampling_kwargs, alternative_method, **kwargs)\u001b[0m\n\u001b[1;32m    259\u001b[0m sampling_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(num_samples, max_sampling_batch_size)\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m num_remaining \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;66;03m# Sample and reject.\u001b[39;00m\n\u001b[0;32m--> 262\u001b[0m     candidates \u001b[38;5;241m=\u001b[39m \u001b[43mproposal\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[43msampling_batch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mproposal_sampling_kwargs\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[1;32m    264\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;66;03m# SNPE-style rejection-sampling when the proposal is the neural net.\u001b[39;00m\n\u001b[1;32m    267\u001b[0m     are_accepted \u001b[38;5;241m=\u001b[39m accept_reject_fn(candidates)\n",
      "File \u001b[0;32m~/sbi/sbi/neural_nets/density_estimators/nflows_flow.py:110\u001b[0m, in \u001b[0;36mNFlowsFlow.sample\u001b[0;34m(self, sample_shape, condition)\u001b[0m\n\u001b[1;32m    108\u001b[0m     batch_shape \u001b[38;5;241m=\u001b[39m condition\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39mcondition_dims]\n\u001b[1;32m    109\u001b[0m     condition \u001b[38;5;241m=\u001b[39m condition\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition_shape)\n\u001b[0;32m--> 110\u001b[0m     samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcondition\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreshape(\n\u001b[1;32m    111\u001b[0m         (\u001b[38;5;241m*\u001b[39mbatch_shape, \u001b[38;5;241m*\u001b[39msample_shape, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    112\u001b[0m     )\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m samples\n",
      "File \u001b[0;32m~/miniconda3/envs/sbi3/lib/python3.11/site-packages/nflows/distributions/base.py:65\u001b[0m, in \u001b[0;36mDistribution.sample\u001b[0;34m(self, num_samples, context, batch_size)\u001b[0m\n\u001b[1;32m     62\u001b[0m     context \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mas_tensor(context)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m check\u001b[38;5;241m.\u001b[39mis_positive_int(batch_size):\n",
      "File \u001b[0;32m~/miniconda3/envs/sbi3/lib/python3.11/site-packages/nflows/flows/base.py:54\u001b[0m, in \u001b[0;36mFlow._sample\u001b[0;34m(self, num_samples, context)\u001b[0m\n\u001b[1;32m     49\u001b[0m     noise \u001b[38;5;241m=\u001b[39m torchutils\u001b[38;5;241m.\u001b[39mmerge_leading_dims(noise, num_dims\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     50\u001b[0m     embedded_context \u001b[38;5;241m=\u001b[39m torchutils\u001b[38;5;241m.\u001b[39mrepeat_rows(\n\u001b[1;32m     51\u001b[0m         embedded_context, num_reps\u001b[38;5;241m=\u001b[39mnum_samples\n\u001b[1;32m     52\u001b[0m     )\n\u001b[0;32m---> 54\u001b[0m samples, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minverse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnoise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedded_context\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m embedded_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;66;03m# Split the context dimension from sample dimension.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m     samples \u001b[38;5;241m=\u001b[39m torchutils\u001b[38;5;241m.\u001b[39msplit_leading_dim(samples, shape\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, num_samples])\n",
      "File \u001b[0;32m~/miniconda3/envs/sbi3/lib/python3.11/site-packages/nflows/transforms/base.py:60\u001b[0m, in \u001b[0;36mCompositeTransform.inverse\u001b[0;34m(self, inputs, context)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minverse\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, context\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     59\u001b[0m     funcs \u001b[38;5;241m=\u001b[39m (transform\u001b[38;5;241m.\u001b[39minverse \u001b[38;5;28;01mfor\u001b[39;00m transform \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transforms[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m---> 60\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cascade\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfuncs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sbi3/lib/python3.11/site-packages/nflows/transforms/base.py:50\u001b[0m, in \u001b[0;36mCompositeTransform._cascade\u001b[0;34m(inputs, funcs, context)\u001b[0m\n\u001b[1;32m     48\u001b[0m total_logabsdet \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mnew_zeros(batch_size)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m funcs:\n\u001b[0;32m---> 50\u001b[0m     outputs, logabsdet \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     total_logabsdet \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m logabsdet\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs, total_logabsdet\n",
      "File \u001b[0;32m~/miniconda3/envs/sbi3/lib/python3.11/site-packages/nflows/transforms/permutations.py:44\u001b[0m, in \u001b[0;36mPermutation.inverse\u001b[0;34m(self, inputs, context)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, context\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_permute(inputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_permutation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dim)\n\u001b[0;32m---> 44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minverse\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, context\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_permute(inputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inverse_permutation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dim)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "samples = posterior.sample((10000,), x=observation)\n",
    "log_probability = posterior.log_prob(samples, x=observation)\n",
    "_ = analysis.pairplot(samples, limits=[[-2, 2], [-2, 2], [-2, 2]], figsize=(6, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "The single-line interface described above provides an easy entry for using `sbi`. However, on almost any real-world problem that goes beyond a simple demonstration, we strongly recommend using the [flexible interface](https://sbi-dev.github.io/sbi/tutorial/02_flexible_interface/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
