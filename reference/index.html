
<!DOCTYPE html>

<html class="no-js" lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width,initial-scale=1" name="viewport"/>
<link href="https://mackelab.org/sbi/reference/" rel="canonical"/>
<link href="../assets/images/favicon.png" rel="shortcut icon"/>
<meta content="mkdocs-1.1.2, mkdocs-material-5.2.2" name="generator"/>
<title>API Reference - sbi</title>
<link href="../assets/stylesheets/main.a2408e81.min.css" rel="stylesheet"/>
<link href="../assets/stylesheets/palette.a46bcfb3.min.css" rel="stylesheet"/>
<meta content="#3f51b5" name="theme-color"/>
<link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect"/>
<link href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&amp;display=fallback" rel="stylesheet"/>
<style>body,input{font-family:"Roboto",-apple-system,BlinkMacSystemFont,Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono",SFMono-Regular,Consolas,Menlo,monospace}</style>
<link href="../static/global.css" rel="stylesheet"/>
</head>
<body data-md-color-accent="indigo" data-md-color-primary="indigo" data-md-color-scheme="" dir="ltr">
<input autocomplete="off" class="md-toggle" data-md-toggle="drawer" id="__drawer" type="checkbox"/>
<input autocomplete="off" class="md-toggle" data-md-toggle="search" id="__search" type="checkbox"/>
<label class="md-overlay" for="__drawer"></label>
<div data-md-component="skip">
<a class="md-skip" href="#api-reference">
          Skip to content
        </a>
</div>
<div data-md-component="announce">
</div>
<header class="md-header" data-md-component="header">
<nav aria-label="Header" class="md-header-nav md-grid">
<a aria-label="sbi" class="md-header-nav__button md-logo" href="https://mackelab.org/sbi/" title="sbi">
<img alt="logo" src="../static/logo.svg"/>
</a>
<label class="md-header-nav__button md-icon" for="__drawer">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"></path></svg>
</label>
<div class="md-header-nav__title" data-md-component="header-title">
<div class="md-header-nav__ellipsis">
<span class="md-header-nav__topic md-ellipsis">
            sbi
          </span>
<span class="md-header-nav__topic md-ellipsis">
            
              API Reference
            
          </span>
</div>
</div>
<label class="md-header-nav__button md-icon" for="__search">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M9.5 3A6.5 6.5 0 0116 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 019.5 16 6.5 6.5 0 013 9.5 6.5 6.5 0 019.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"></path></svg>
</label>
<div class="md-search" data-md-component="search" role="dialog">
<label class="md-search__overlay" for="__search"></label>
<div class="md-search__inner" role="search">
<form class="md-search__form" name="search">
<input aria-label="Search" autocapitalize="off" autocomplete="off" autocorrect="off" class="md-search__input" data-md-component="search-query" data-md-state="active" name="query" placeholder="Search" spellcheck="false" type="text"/>
<label class="md-search__icon md-icon" for="__search">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M9.5 3A6.5 6.5 0 0116 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 019.5 16 6.5 6.5 0 013 9.5 6.5 6.5 0 019.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"></path></svg>
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"></path></svg>
</label>
<button aria-label="Clear" class="md-search__icon md-icon" data-md-component="search-reset" tabindex="-1" type="reset">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"></path></svg>
</button>
</form>
<div class="md-search__output">
<div class="md-search__scrollwrap" data-md-scrollfix="">
<div class="md-search-result" data-md-component="search-result">
<div class="md-search-result__meta">
            Initializing search
          </div>
<ol class="md-search-result__list"></ol>
</div>
</div>
</div>
</div>
</div>
<div class="md-header-nav__source">
<a class="md-source" href="http://github.com/mackelab/sbi/" title="Go to repository">
<div class="md-source__icon md-icon">
<svg viewbox="0 0 448 512" xmlns="http://www.w3.org/2000/svg"><path d="M439.55 236.05L244 40.45a28.87 28.87 0 00-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 01-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 000 40.81l195.61 195.6a28.86 28.86 0 0040.8 0l194.69-194.69a28.86 28.86 0 000-40.81z"></path></svg>
</div>
<div class="md-source__repository">
    mackelab/sbi
  </div>
</a>
</div>
</nav>
</header>
<div class="md-container" data-md-component="container">
<main class="md-main" data-md-component="main">
<div class="md-main__inner md-grid">
<div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
<div class="md-sidebar__scrollwrap">
<div class="md-sidebar__inner">
<nav aria-label="Navigation" class="md-nav md-nav--primary" data-md-level="0">
<label class="md-nav__title" for="__drawer">
<a aria-label="sbi" class="md-nav__button md-logo" href="https://mackelab.org/sbi/" title="sbi">
<img alt="logo" src="../static/logo.svg"/>
</a>
    sbi
  </label>
<div class="md-nav__source">
<a class="md-source" href="http://github.com/mackelab/sbi/" title="Go to repository">
<div class="md-source__icon md-icon">
<svg viewbox="0 0 448 512" xmlns="http://www.w3.org/2000/svg"><path d="M439.55 236.05L244 40.45a28.87 28.87 0 00-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 01-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 000 40.81l195.61 195.6a28.86 28.86 0 0040.8 0l194.69-194.69a28.86 28.86 0 000-40.81z"></path></svg>
</div>
<div class="md-source__repository">
    mackelab/sbi
  </div>
</a>
</div>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href=".." title="Home">
      Home
    </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../install/" title="Installation">
      Installation
    </a>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle" data-md-toggle="nav-3" id="nav-3" type="checkbox"/>
<label class="md-nav__link" for="nav-3">
      Tutorials and Examples
      <span class="md-nav__icon md-icon">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"></path></svg>
</span>
</label>
<nav aria-label="Tutorials and Examples" class="md-nav" data-md-level="1">
<label class="md-nav__title" for="nav-3">
<span class="md-nav__icon md-icon">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"></path></svg>
</span>
        Tutorials and Examples
      </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle" data-md-toggle="nav-3-1" id="nav-3-1" type="checkbox"/>
<label class="md-nav__link" for="nav-3-1">
      Introduction
      <span class="md-nav__icon md-icon">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"></path></svg>
</span>
</label>
<nav aria-label="Introduction" class="md-nav" data-md-level="2">
<label class="md-nav__title" for="nav-3-1">
<span class="md-nav__icon md-icon">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"></path></svg>
</span>
        Introduction
      </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../tutorial/00_getting_started/" title="Getting started">
      Getting started
    </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../tutorial/01_gaussian_amortized/" title="Amortized inference">
      Amortized inference
    </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle" data-md-toggle="nav-3-2" id="nav-3-2" type="checkbox"/>
<label class="md-nav__link" for="nav-3-2">
      Advanced
      <span class="md-nav__icon md-icon">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"></path></svg>
</span>
</label>
<nav aria-label="Advanced" class="md-nav" data-md-level="2">
<label class="md-nav__title" for="nav-3-2">
<span class="md-nav__icon md-icon">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"></path></svg>
</span>
        Advanced
      </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../tutorial/02_flexible_interface/" title="Flexible interface">
      Flexible interface
    </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle" data-md-toggle="nav-3-3" id="nav-3-3" type="checkbox"/>
<label class="md-nav__link" for="nav-3-3">
      Examples
      <span class="md-nav__icon md-icon">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"></path></svg>
</span>
</label>
<nav aria-label="Examples" class="md-nav" data-md-level="2">
<label class="md-nav__title" for="nav-3-3">
<span class="md-nav__icon md-icon">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"></path></svg>
</span>
        Examples
      </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../examples/00_HH_simulator/" title="Hodgkin-Huxley example">
      Hodgkin-Huxley example
    </a>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../contribute/" title="Contribute">
      Contribute
    </a>
</li>
<li class="md-nav__item md-nav__item--active">
<input class="md-nav__toggle md-toggle" data-md-toggle="toc" id="__toc" type="checkbox"/>
<label class="md-nav__link md-nav__link--active" for="__toc">
        API Reference
        <span class="md-nav__icon md-icon">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M3 9h14V7H3v2m0 4h14v-2H3v2m0 4h14v-2H3v2m16 0h2v-2h-2v2m0-10v2h2V7h-2m0 6h2v-2h-2v2z"></path></svg>
</span>
</label>
<a class="md-nav__link md-nav__link--active" href="./" title="API Reference">
      API Reference
    </a>
<nav aria-label="Table of contents" class="md-nav md-nav--secondary">
<label class="md-nav__title" for="__toc">
<span class="md-nav__icon md-icon">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"></path></svg>
</span>
      Table of contents
    </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="#inference">
    Inference
  </a>
<nav aria-label="Inference" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#sbi.inference.base.infer">
    infer()
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#sbi.user_input.user_input_checks.prepare_for_sbi">
    prepare_for_sbi()
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#sbi.inference.snpe.snpe_c.SNPE_C">
    SNPE_C
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#sbi.inference.snle.snle_a.SNLE_A">
    SNLE_A
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#sbi.inference.snre.snre_a.SNRE_A">
    SNRE_A
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#sbi.inference.snre.snre_b.SNRE_B">
    SNRE_B
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#sbi.inference.abc.mcabc.MCABC">
    MCABC
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#sbi.inference.abc.smcabc.SMCABC">
    SMCABC
  </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#posteriors">
    Posteriors
  </a>
<nav aria-label="Posteriors" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#sbi.inference.posteriors.direct_posterior.DirectPosterior">
    DirectPosterior
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#sbi.inference.posteriors.likelihood_based_posterior.LikelihoodBasedPosterior">
    LikelihoodBasedPosterior
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#sbi.inference.posteriors.ratio_based_posterior.RatioBasedPosterior">
    RatioBasedPosterior
  </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#models">
    Models
  </a>
<nav aria-label="Models" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#sbi.utils.get_nn_models.posterior_nn">
    posterior_nn()
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#sbi.utils.get_nn_models.likelihood_nn">
    likelihood_nn()
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#sbi.utils.get_nn_models.classifier_nn">
    classifier_nn()
  </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#utils">
    Utils
  </a>
<nav aria-label="Utils" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#sbi.utils.plot.pairplot">
    pairplot()
  </a>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../faq/" title="FAQ">
      FAQ
    </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../credits/" title="Credits">
      Credits
    </a>
</li>
</ul>
</nav>
</div>
</div>
</div>
<div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
<div class="md-sidebar__scrollwrap">
<div class="md-sidebar__inner">
<nav aria-label="Table of contents" class="md-nav md-nav--secondary">
<label class="md-nav__title" for="__toc">
<span class="md-nav__icon md-icon">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"></path></svg>
</span>
      Table of contents
    </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="#inference">
    Inference
  </a>
<nav aria-label="Inference" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#sbi.inference.base.infer">
    infer()
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#sbi.user_input.user_input_checks.prepare_for_sbi">
    prepare_for_sbi()
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#sbi.inference.snpe.snpe_c.SNPE_C">
    SNPE_C
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#sbi.inference.snle.snle_a.SNLE_A">
    SNLE_A
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#sbi.inference.snre.snre_a.SNRE_A">
    SNRE_A
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#sbi.inference.snre.snre_b.SNRE_B">
    SNRE_B
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#sbi.inference.abc.mcabc.MCABC">
    MCABC
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#sbi.inference.abc.smcabc.SMCABC">
    SMCABC
  </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#posteriors">
    Posteriors
  </a>
<nav aria-label="Posteriors" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#sbi.inference.posteriors.direct_posterior.DirectPosterior">
    DirectPosterior
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#sbi.inference.posteriors.likelihood_based_posterior.LikelihoodBasedPosterior">
    LikelihoodBasedPosterior
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#sbi.inference.posteriors.ratio_based_posterior.RatioBasedPosterior">
    RatioBasedPosterior
  </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#models">
    Models
  </a>
<nav aria-label="Models" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#sbi.utils.get_nn_models.posterior_nn">
    posterior_nn()
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#sbi.utils.get_nn_models.likelihood_nn">
    likelihood_nn()
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#sbi.utils.get_nn_models.classifier_nn">
    classifier_nn()
  </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#utils">
    Utils
  </a>
<nav aria-label="Utils" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#sbi.utils.plot.pairplot">
    pairplot()
  </a>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</div>
</div>
</div>
<div class="md-content">
<article class="md-content__inner md-typeset">
<a class="md-content__button md-icon" href="http://github.com/mackelab/sbi/edit/master/docs/reference.md" title="Edit this page">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75L3 17.25z"></path></svg>
</a>
<h1 id="api-reference">API Reference<a class="headerlink" href="#api-reference" title="Permanent link">¶</a></h1>
<h2 id="inference">Inference<a class="headerlink" href="#inference" title="Permanent link">¶</a></h2>
<div class="doc doc-object doc-function">
<h3 class="doc doc-heading" id="sbi.inference.base.infer">
<code class="highlight language-python">
sbi.inference.base.infer<span class="p">(</span><span class="n">simulator</span><span class="p">,</span> <span class="n">prior</span><span class="p">,</span> <span class="n">method</span><span class="p">,</span> <span class="n">num_simulations</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> </code>
<a class="headerlink" href="#sbi.inference.base.infer" title="Permanent link">¶</a></h3>
<div class="doc doc-contents first">
<p>Return posterior distribution by running simulation-based inference.</p>
<p>This function provides a simple interface to run sbi. Inference is run for a single
round and hence the returned posterior <span><span class="MathJax_Preview">p(\theta|x)</span><script type="math/tex">p(\theta|x)</script></span> can be sampled and evaluated
for any <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> (i.e. it is amortized).</p>
<p>The scope of this function is limited to the most essential features of sbi. For
more flexibility (e.g. multi-round inference, different density estimators) please
use the flexible interface described here:
<a href="https://www.mackelab.org/sbi/tutorial/03_flexible_interface/">https://www.mackelab.org/sbi/tutorial/03_flexible_interface/</a></p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>simulator</code></td>
<td><code>Callable</code></td>
<td>
<p>A function that takes parameters <span><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span> and maps them to
simulations, or observations, <code>x</code>, <span><span class="MathJax_Preview">\mathrm{sim}(\theta)\to x</span><script type="math/tex">\mathrm{sim}(\theta)\to x</script></span>. Any
regular Python callable (i.e. function or class with <code>__call__</code> method)
can be used.</p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>prior</code></td>
<td><code></code></td>
<td>
<p>A probability distribution that expresses prior knowledge about the
parameters, e.g. which ranges are meaningful for them. Any
object with <code>.log_prob()</code>and <code>.sample()</code> (for example, a PyTorch
distribution) can be used.</p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>method</code></td>
<td><code>str</code></td>
<td>
<p>What inference method to use. Either of SNPE, SNLE or SNRE.</p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>num_simulations</code></td>
<td><code>int</code></td>
<td>
<p>Number of simulation calls. More simulations means a longer
runtime, but a better posterior estimate.</p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>num_workers</code></td>
<td><code>int</code></td>
<td>
<p>Number of parallel workers to use for simulations.</p>
</td>
<td><code>1</code></td>
</tr>
</tbody>
</table>
<p>Returns: Posterior over parameters conditional on observations (amortized).</p>
<details class="quote">
<summary>Source code in <code>sbi/inference/base.py</code></summary>
<table class="highlighttable">
<tr>
<td class="linenos">
<div class="linenodiv">
<pre><span></span>30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74</pre>
</div>
</td>
<td class="code">
<div class="highlight">
<pre><span></span><code><span class="k">def</span> <span class="nf">infer</span><span class="p">(</span>
    <span class="n">simulator</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">prior</span><span class="p">,</span> <span class="n">method</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">num_simulations</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">num_workers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">NeuralPosterior</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">"""</span>
<span class="sd">    Return posterior distribution by running simulation-based inference.</span>

<span class="sd">    This function provides a simple interface to run sbi. Inference is run for a single</span>
<span class="sd">    round and hence the returned posterior $p(\theta|x)$ can be sampled and evaluated</span>
<span class="sd">    for any $x$ (i.e. it is amortized).</span>

<span class="sd">    The scope of this function is limited to the most essential features of sbi. For</span>
<span class="sd">    more flexibility (e.g. multi-round inference, different density estimators) please</span>
<span class="sd">    use the flexible interface described here:</span>
<span class="sd">    https://www.mackelab.org/sbi/tutorial/03_flexible_interface/</span>

<span class="sd">    Args:</span>
<span class="sd">        simulator: A function that takes parameters $\theta$ and maps them to</span>
<span class="sd">            simulations, or observations, `x`, $\mathrm{sim}(\theta)\to x$. Any</span>
<span class="sd">            regular Python callable (i.e. function or class with `__call__` method)</span>
<span class="sd">            can be used.</span>
<span class="sd">        prior: A probability distribution that expresses prior knowledge about the</span>
<span class="sd">            parameters, e.g. which ranges are meaningful for them. Any</span>
<span class="sd">            object with `.log_prob()`and `.sample()` (for example, a PyTorch</span>
<span class="sd">            distribution) can be used.</span>
<span class="sd">        method: What inference method to use. Either of SNPE, SNLE or SNRE.</span>
<span class="sd">        num_simulations: Number of simulation calls. More simulations means a longer</span>
<span class="sd">            runtime, but a better posterior estimate.</span>
<span class="sd">        num_workers: Number of parallel workers to use for simulations.</span>

<span class="sd">    Returns: Posterior over parameters conditional on observations (amortized).</span>
<span class="sd">    """</span>

    <span class="k">try</span><span class="p">:</span>
        <span class="n">method_fun</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">sbi</span><span class="o">.</span><span class="n">inference</span><span class="p">,</span> <span class="n">method</span><span class="o">.</span><span class="n">upper</span><span class="p">())</span>
    <span class="k">except</span> <span class="ne">AttributeError</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NameError</span><span class="p">(</span>
            <span class="s2">"Method not available. `method` must be one of 'SNPE', 'SNLE', 'SNRE'."</span>
        <span class="p">)</span>

    <span class="n">simulator</span><span class="p">,</span> <span class="n">prior</span> <span class="o">=</span> <span class="n">prepare_for_sbi</span><span class="p">(</span><span class="n">simulator</span><span class="p">,</span> <span class="n">prior</span><span class="p">)</span>

    <span class="n">infer_</span> <span class="o">=</span> <span class="n">method_fun</span><span class="p">(</span><span class="n">simulator</span><span class="p">,</span> <span class="n">prior</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="n">num_workers</span><span class="p">)</span>
    <span class="n">posterior</span> <span class="o">=</span> <span class="n">infer_</span><span class="p">(</span><span class="n">num_simulations</span><span class="o">=</span><span class="n">num_simulations</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">posterior</span>
</code></pre>
</div>
</td>
</tr>
</table>
</details>
</div>
</div>
<div class="doc doc-object doc-function">
<h3 class="doc doc-heading" id="sbi.user_input.user_input_checks.prepare_for_sbi">
<code class="highlight language-python">
sbi.user_input.user_input_checks.prepare_for_sbi<span class="p">(</span><span class="n">simulator</span><span class="p">,</span> <span class="n">prior</span><span class="p">)</span> </code>
<a class="headerlink" href="#sbi.user_input.user_input_checks.prepare_for_sbi" title="Permanent link">¶</a></h3>
<div class="doc doc-contents first">
<p>Prepare simulator, prior and for usage in sbi.</p>
<p>One of the goals is to allow you to use sbi with inputs computed in numpy.</p>
<p>Attempts to meet the following requirements by reshaping and type-casting:
- the simulator function receives as input and returns a Tensor.
- the simulator can simulate batches of parameters and return batches of data.
- the prior does not produce batches and samples and evaluates to Tensor.
- the output shape is a <code>torch.Size((1,N))</code> (i.e, has a leading batch dimension 1).</p>
<p>If this is not possible, a suitable exception will be raised.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>simulator</code></td>
<td><code>Callable</code></td>
<td>
<p>Simulator as provided by the user.</p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>prior</code></td>
<td><code></code></td>
<td>
<p>Prior as provided by the user.</p>
</td>
<td><em>required</em></td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>Tuple[Callable, torch.distributions.distribution.Distribution]</code></td>
<td>
<p>Tuple (simulator, prior, x_shape) checked and matching the requirements of sbi.</p>
</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Source code in <code>sbi/user_input/user_input_checks.py</code></summary>
<table class="highlighttable">
<tr>
<td class="linenos">
<div class="linenodiv">
<pre><span></span>470
471
472
473
474
475
476
477
478
479
480
481
482
483
484
485
486
487
488
489
490
491
492
493
494
495
496
497
498
499
500</pre>
</div>
</td>
<td class="code">
<div class="highlight">
<pre><span></span><code><span class="k">def</span> <span class="nf">prepare_for_sbi</span><span class="p">(</span><span class="n">simulator</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">prior</span><span class="p">,)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Callable</span><span class="p">,</span> <span class="n">Distribution</span><span class="p">]:</span>
    <span class="sd">"""Prepare simulator, prior and for usage in sbi.</span>

<span class="sd">    One of the goals is to allow you to use sbi with inputs computed in numpy.</span>

<span class="sd">    Attempts to meet the following requirements by reshaping and type-casting:</span>
<span class="sd">    - the simulator function receives as input and returns a Tensor.</span>
<span class="sd">    - the simulator can simulate batches of parameters and return batches of data.</span>
<span class="sd">    - the prior does not produce batches and samples and evaluates to Tensor.</span>
<span class="sd">    - the output shape is a `torch.Size((1,N))` (i.e, has a leading batch dimension 1).</span>

<span class="sd">    If this is not possible, a suitable exception will be raised.</span>

<span class="sd">    Args:</span>
<span class="sd">        simulator: Simulator as provided by the user.</span>
<span class="sd">        prior: Prior as provided by the user.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tuple (simulator, prior, x_shape) checked and matching the requirements of sbi.</span>
<span class="sd">    """</span>

    <span class="c1"># Check prior, return PyTorch prior.</span>
    <span class="n">prior</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">prior_returns_numpy</span> <span class="o">=</span> <span class="n">process_prior</span><span class="p">(</span><span class="n">prior</span><span class="p">)</span>

    <span class="c1"># Check simulator, returns PyTorch simulator able to simulate batches.</span>
    <span class="n">simulator</span> <span class="o">=</span> <span class="n">process_simulator</span><span class="p">(</span><span class="n">simulator</span><span class="p">,</span> <span class="n">prior</span><span class="p">,</span> <span class="n">prior_returns_numpy</span><span class="p">)</span>

    <span class="c1"># Consistency check after making ready for sbi.</span>
    <span class="n">check_sbi_inputs</span><span class="p">(</span><span class="n">simulator</span><span class="p">,</span> <span class="n">prior</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">simulator</span><span class="p">,</span> <span class="n">prior</span>
</code></pre>
</div>
</td>
</tr>
</table>
</details>
</div>
</div>
<div class="doc doc-object doc-class">
<h3 class="doc doc-heading" id="sbi.inference.snpe.snpe_c.SNPE_C">
<code>sbi.inference.snpe.snpe_c.SNPE_C</code>
<a class="headerlink" href="#sbi.inference.snpe.snpe_c.SNPE_C" title="Permanent link">¶</a></h3>
<div class="doc doc-contents first">
<div class="doc doc-children">
<div class="doc doc-object doc-method">
<h4 class="doc doc-heading" data-toc-label="__call__()" id="sbi.inference.snpe.snpe_c.SNPE_C.__call__">
<code class="highlight language-python">
__call__<span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_simulations</span><span class="p">,</span> <span class="n">proposal</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">num_atoms</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">training_batch_size</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.0005</span><span class="p">,</span> <span class="n">validation_fraction</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">stop_after_epochs</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">max_num_epochs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">clip_max_norm</span><span class="o">=</span><span class="mf">5.0</span><span class="p">,</span> <span class="n">calibration_kernel</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">exclude_invalid_x</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">discard_prior_samples</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">retrain_from_scratch_each_round</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> </code>
<span class="doc doc-properties">
<small class="doc doc-property doc-property-special"><code>special</code></small>
</span>
</h4>
<div class="doc doc-contents">
<p>Run SNPE.</p>
<p>Return posterior <span><span class="MathJax_Preview">p(\theta|x)</span><script type="math/tex">p(\theta|x)</script></span> after inference.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>num_simulations</code></td>
<td><code>int</code></td>
<td>
<p>Number of simulator calls.</p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>proposal</code></td>
<td><code>Optional[Any]</code></td>
<td>
<p>Distribution that the parameters <span><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span> are drawn from.
<code>proposal=None</code> uses the prior. Setting the proposal to a distribution
targeted on a specific observation, e.g. a posterior <span><span class="MathJax_Preview">p(\theta|x_o)</span><script type="math/tex">p(\theta|x_o)</script></span>
obtained previously, can lead to less required simulations.</p>
</td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>num_atoms</code></td>
<td><code>int</code></td>
<td>
<p>Number of atoms to use for classification.</p>
</td>
<td><code>10</code></td>
</tr>
<tr>
<td><code>training_batch_size</code></td>
<td><code>int</code></td>
<td>
<p>Training batch size.</p>
</td>
<td><code>50</code></td>
</tr>
<tr>
<td><code>learning_rate</code></td>
<td><code>float</code></td>
<td>
<p>Learning rate for Adam optimizer.</p>
</td>
<td><code>0.0005</code></td>
</tr>
<tr>
<td><code>validation_fraction</code></td>
<td><code>float</code></td>
<td>
<p>The fraction of data to use for validation.</p>
</td>
<td><code>0.1</code></td>
</tr>
<tr>
<td><code>stop_after_epochs</code></td>
<td><code>int</code></td>
<td>
<p>The number of epochs to wait for improvement on the
validation set before terminating training.</p>
</td>
<td><code>20</code></td>
</tr>
<tr>
<td><code>max_num_epochs</code></td>
<td><code>Optional[int]</code></td>
<td>
<p>Maximum number of epochs to run. If reached, we stop
training even when the validation loss is still decreasing. If None, we
train until validation loss increases (see also <code>stop_after_epochs</code>).</p>
</td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>clip_max_norm</code></td>
<td><code>Optional[float]</code></td>
<td>
<p>Value at which to clip the total gradient norm in order to
prevent exploding gradients. Use None for no clipping.</p>
</td>
<td><code>5.0</code></td>
</tr>
<tr>
<td><code>calibration_kernel</code></td>
<td><code>Optional[Callable]</code></td>
<td>
<p>A function to calibrate the loss with respect to the
simulations <code>x</code>. See Lueckmann, Gonçalves et al., NeurIPS 2017.</p>
</td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>exclude_invalid_x</code></td>
<td><code>bool</code></td>
<td>
<p>Whether to exclude simulation outputs <code>x=NaN</code> or <code>x=±∞</code>
during training. Expect errors, silent or explicit, when <code>False</code>.</p>
</td>
<td><code>True</code></td>
</tr>
<tr>
<td><code>discard_prior_samples</code></td>
<td><code>bool</code></td>
<td>
<p>Whether to discard samples simulated in round 1, i.e.
from the prior. Training may be sped up by ignoring such less targeted
samples.</p>
</td>
<td><code>False</code></td>
</tr>
<tr>
<td><code>retrain_from_scratch_each_round</code></td>
<td><code>bool</code></td>
<td>
<p>Whether to retrain the conditional density
estimator for the posterior from scratch each round.</p>
</td>
<td><code>False</code></td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>DirectPosterior</code></td>
<td>
<p>Posterior <span><span class="MathJax_Preview">p(\theta|x)</span><script type="math/tex">p(\theta|x)</script></span> that can be sampled and evaluated.</p>
</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Source code in <code>sbi/inference/snpe/snpe_c.py</code></summary>
<table class="highlighttable">
<tr>
<td class="linenos">
<div class="linenodiv">
<pre><span></span>130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206</pre>
</div>
</td>
<td class="code">
<div class="highlight">
<pre><span></span><code><span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">num_simulations</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">proposal</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">num_atoms</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
    <span class="n">training_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">5e-4</span><span class="p">,</span>
    <span class="n">validation_fraction</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">stop_after_epochs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span>
    <span class="n">max_num_epochs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">clip_max_norm</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="mf">5.0</span><span class="p">,</span>
    <span class="n">calibration_kernel</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">exclude_invalid_x</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">discard_prior_samples</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">retrain_from_scratch_each_round</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">DirectPosterior</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">"""Run SNPE.</span>

<span class="sd">    Return posterior $p(\theta|x)$ after inference.</span>

<span class="sd">    Args:</span>
<span class="sd">        num_simulations: Number of simulator calls.</span>
<span class="sd">        proposal: Distribution that the parameters $\theta$ are drawn from.</span>
<span class="sd">            `proposal=None` uses the prior. Setting the proposal to a distribution</span>
<span class="sd">            targeted on a specific observation, e.g. a posterior $p(\theta|x_o)$</span>
<span class="sd">            obtained previously, can lead to less required simulations.</span>
<span class="sd">        num_atoms: Number of atoms to use for classification.</span>
<span class="sd">        training_batch_size: Training batch size.</span>
<span class="sd">        learning_rate: Learning rate for Adam optimizer.</span>
<span class="sd">        validation_fraction: The fraction of data to use for validation.</span>
<span class="sd">        stop_after_epochs: The number of epochs to wait for improvement on the</span>
<span class="sd">            validation set before terminating training.</span>
<span class="sd">        max_num_epochs: Maximum number of epochs to run. If reached, we stop</span>
<span class="sd">            training even when the validation loss is still decreasing. If None, we</span>
<span class="sd">            train until validation loss increases (see also `stop_after_epochs`).</span>
<span class="sd">        clip_max_norm: Value at which to clip the total gradient norm in order to</span>
<span class="sd">            prevent exploding gradients. Use None for no clipping.</span>
<span class="sd">        calibration_kernel: A function to calibrate the loss with respect to the</span>
<span class="sd">            simulations `x`. See Lueckmann, Gonçalves et al., NeurIPS 2017.</span>
<span class="sd">        exclude_invalid_x: Whether to exclude simulation outputs `x=NaN` or `x=±∞`</span>
<span class="sd">            during training. Expect errors, silent or explicit, when `False`.</span>
<span class="sd">        discard_prior_samples: Whether to discard samples simulated in round 1, i.e.</span>
<span class="sd">            from the prior. Training may be sped up by ignoring such less targeted</span>
<span class="sd">            samples.</span>
<span class="sd">        retrain_from_scratch_each_round: Whether to retrain the conditional density</span>
<span class="sd">            estimator for the posterior from scratch each round.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Posterior $p(\theta|x)$ that can be sampled and evaluated.</span>
<span class="sd">    """</span>

    <span class="c1"># WARNING: sneaky trick ahead. We proxy the parent's `__call__` here,</span>
    <span class="c1"># requiring the signature to have `num_atoms`, save it for use below, and</span>
    <span class="c1"># continue. It's sneaky because we are using the object (self) as a namespace</span>
    <span class="c1"># to pass arguments between functions, and that's implicit state management.</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_num_atoms</span> <span class="o">=</span> <span class="n">num_atoms</span>
    <span class="n">kwargs</span> <span class="o">=</span> <span class="n">del_entries</span><span class="p">(</span><span class="nb">locals</span><span class="p">(),</span> <span class="n">entries</span><span class="o">=</span><span class="p">(</span><span class="s2">"self"</span><span class="p">,</span> <span class="s2">"__class__"</span><span class="p">,</span> <span class="s2">"num_atoms"</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">proposal</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_non_atomic_loss</span> <span class="o">=</span> <span class="p">(</span>
            <span class="nb">isinstance</span><span class="p">(</span><span class="n">proposal</span><span class="o">.</span><span class="n">net</span><span class="o">.</span><span class="n">_distribution</span><span class="p">,</span> <span class="n">mdn</span><span class="p">)</span>
            <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_posterior</span><span class="o">.</span><span class="n">net</span><span class="o">.</span><span class="n">_distribution</span><span class="p">,</span> <span class="n">mdn</span><span class="p">)</span>
            <span class="ow">and</span> <span class="p">(</span>
                <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_prior</span><span class="p">,</span> <span class="n">utils</span><span class="o">.</span><span class="n">BoxUniform</span><span class="p">)</span>
                <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_prior</span><span class="p">,</span> <span class="n">MultivariateNormal</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="p">)</span>

        <span class="n">algorithm</span> <span class="o">=</span> <span class="s2">"non-atomic"</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_non_atomic_loss</span> <span class="k">else</span> <span class="s2">"atomic"</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Using SNPE-C with </span><span class="si">{</span><span class="n">algorithm</span><span class="si">}</span><span class="s2"> loss"</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_non_atomic_loss</span><span class="p">:</span>
            <span class="c1"># Take care of z-scoring, pre-compute and store prior terms.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_set_state_for_mog_proposal</span><span class="p">()</span>

    <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre>
</div>
</td>
</tr>
</table>
</details>
</div>
</div>
<div class="doc doc-object doc-method">
<h4 class="doc doc-heading" data-toc-label="__init__()" id="sbi.inference.snpe.snpe_c.SNPE_C.__init__">
<code class="highlight language-python">
__init__<span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">simulator</span><span class="p">,</span> <span class="n">prior</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">simulation_batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">density_estimator</span><span class="o">=</span><span class="s1">'maf'</span><span class="p">,</span> <span class="n">sample_with_mcmc</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">mcmc_method</span><span class="o">=</span><span class="s1">'slice_np'</span><span class="p">,</span> <span class="n">mcmc_parameters</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">use_combined_loss</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">'cpu'</span><span class="p">,</span> <span class="n">logging_level</span><span class="o">=</span><span class="s1">'WARNING'</span><span class="p">,</span> <span class="n">summary_writer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">show_progress_bars</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">show_round_summary</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> </code>
<span class="doc doc-properties">
<small class="doc doc-property doc-property-special"><code>special</code></small>
</span>
</h4>
<div class="doc doc-contents">
<p>SNPE-C / APT [1].</p>
<p>[1] <em>Automatic Posterior Transformation for Likelihood-free Inference</em>,
    Greenberg et al., ICML 2019, <a href="https://arxiv.org/abs/1905.07488">https://arxiv.org/abs/1905.07488</a>.</p>
<p>This class implements two loss variants of SNPE-C: the non-atomic and the atomic
version. The atomic loss of SNPE-C can be used for any density estimator,
i.e. also for normalizing flows. However, it suffers from leakage issues. On
the other hand, the non-atomic loss can only be used only if the proposal
distribution is a mixture of Gaussians, the density estimator is a mixture of
Gaussians, and the prior is either Gaussian or Uniform. It does not suffer from
leakage issues. At the beginning of each round, we print whether the non-atomic
or the atomic version is used.</p>
<p>In this codebase, we will automatically switch to the non-atomic loss if the
following criteria are fulfilled:
- proposal has is a <code>DirectPosterior</code> with density_estimator <code>mdn</code>, as built
    with <code>utils.sbi.posterior_nn()</code>.
- the density estimator is a <code>mdn</code>, as built with <code>utils.sbi.posterior_nn()</code>.
- <code>isinstance(prior, MultivariateNormal)</code> (from <code>torch.distributions</code>) or
    <code>isinstance(prior, sbi.utils.BoxUniform)</code></p>
<p>Note that custom implementations of any of these densities (or estimators) will
not trigger the non-atomic loss, and the algorithm will fall back onto using
the atomic loss.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>simulator</code></td>
<td><code>Callable</code></td>
<td>
<p>A function that takes parameters <span><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span> and maps them to
simulations, or observations, <code>x</code>, <span><span class="MathJax_Preview">\mathrm{sim}(\theta)\to x</span><script type="math/tex">\mathrm{sim}(\theta)\to x</script></span>. Any
regular Python callable (i.e. function or class with <code>__call__</code> method)
can be used.</p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>prior</code></td>
<td><code></code></td>
<td>
<p>A probability distribution that expresses prior knowledge about the
parameters, e.g. which ranges are meaningful for them. Any
object with <code>.log_prob()</code>and <code>.sample()</code> (for example, a PyTorch
distribution) can be used.</p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>num_workers</code></td>
<td><code>int</code></td>
<td>
<p>Number of parallel workers to use for simulations.</p>
</td>
<td><code>1</code></td>
</tr>
<tr>
<td><code>simulation_batch_size</code></td>
<td><code>int</code></td>
<td>
<p>Number of parameter sets that the simulator
maps to data x at once. If None, we simulate all parameter sets at the
same time. If &gt;= 1, the simulator has to process data of shape
(simulation_batch_size, parameter_dimension).</p>
</td>
<td><code>1</code></td>
</tr>
<tr>
<td><code>density_estimator</code></td>
<td><code>Union[str, Callable]</code></td>
<td>
<p>If it is a string, use a pre-configured network of the
provided type (one of nsf, maf, mdn, made). Alternatively, a function
that builds a custom neural network can be provided. The function will
be called with the first batch of simulations (theta, x), which can
thus be used for shape inference and potentially for z-scoring. It
needs to return a PyTorch <code>nn.Module</code> implementing the density
estimator. The density estimator needs to provide the methods
<code>.log_prob</code> and <code>.sample()</code>.</p>
</td>
<td><code>'maf'</code></td>
</tr>
<tr>
<td><code>sample_with_mcmc</code></td>
<td><code>bool</code></td>
<td>
<p>Whether to sample with MCMC. MCMC can be used to deal
with high leakage.</p>
</td>
<td><code>False</code></td>
</tr>
<tr>
<td><code>mcmc_method</code></td>
<td><code>str</code></td>
<td>
<p>Method used for MCMC sampling, one of <code>slice_np</code>, <code>slice</code>,
<code>hmc</code>, <code>nuts</code>. Currently defaults to <code>slice_np</code> for a custom numpy
implementation of slice sampling; select <code>hmc</code>, <code>nuts</code> or <code>slice</code> for
Pyro-based sampling.</p>
</td>
<td><code>'slice_np'</code></td>
</tr>
<tr>
<td><code>mcmc_parameters</code></td>
<td><code>Optional[Dict[str, Any]]</code></td>
<td>
<p>Dictionary overriding the default parameters for MCMC.
The following parameters are supported: <code>thin</code> to set the thinning
factor for the chain, <code>warmup_steps</code> to set the initial number of
samples to discard, <code>num_chains</code> for the number of chains,
<code>init_strategy</code> for the initialisation strategy for chains; <code>prior</code> will
draw init locations from prior, whereas <code>sir</code> will use
Sequential-Importance-Resampling using <code>init_strategy_num_candidates</code>
to find init locations.</p>
</td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>use_combined_loss</code></td>
<td><code>bool</code></td>
<td>
<p>Whether to train the neural net also on prior samples
using maximum likelihood in addition to training it on all samples using
atomic loss. The extra MLE loss helps prevent density leaking with
bounded priors.</p>
</td>
<td><code>False</code></td>
</tr>
<tr>
<td><code>device</code></td>
<td><code>str</code></td>
<td>
<p>torch device on which to compute, e.g. gpu, cpu.</p>
</td>
<td><code>'cpu'</code></td>
</tr>
<tr>
<td><code>logging_level</code></td>
<td><code>Union[int, str]</code></td>
<td>
<p>Minimum severity of messages to log. One of the strings
INFO, WARNING, DEBUG, ERROR and CRITICAL.</p>
</td>
<td><code>'WARNING'</code></td>
</tr>
<tr>
<td><code>summary_writer</code></td>
<td><code>Optional[Writer]</code></td>
<td>
<p>A tensorboard <code>SummaryWriter</code> to control, among others, log
file location (default is <code>&lt;current working directory&gt;/logs</code>.)</p>
</td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>show_progress_bars</code></td>
<td><code>bool</code></td>
<td>
<p>Whether to show a progressbar during simulation and
sampling.</p>
</td>
<td><code>True</code></td>
</tr>
<tr>
<td><code>show_round_summary</code></td>
<td><code>bool</code></td>
<td>
<p>Whether to show the validation loss and leakage after
each round.</p>
</td>
<td><code>False</code></td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Source code in <code>sbi/inference/snpe/snpe_c.py</code></summary>
<table class="highlighttable">
<tr>
<td class="linenos">
<div class="linenodiv">
<pre><span></span> 29
 30
 31
 32
 33
 34
 35
 36
 37
 38
 39
 40
 41
 42
 43
 44
 45
 46
 47
 48
 49
 50
 51
 52
 53
 54
 55
 56
 57
 58
 59
 60
 61
 62
 63
 64
 65
 66
 67
 68
 69
 70
 71
 72
 73
 74
 75
 76
 77
 78
 79
 80
 81
 82
 83
 84
 85
 86
 87
 88
 89
 90
 91
 92
 93
 94
 95
 96
 97
 98
 99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128</pre>
</div>
</td>
<td class="code">
<div class="highlight">
<pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">simulator</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
    <span class="n">prior</span><span class="p">,</span>
    <span class="n">num_workers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">simulation_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">density_estimator</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"maf"</span><span class="p">,</span>
    <span class="n">sample_with_mcmc</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">mcmc_method</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">"slice_np"</span><span class="p">,</span>
    <span class="n">mcmc_parameters</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">use_combined_loss</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">"cpu"</span><span class="p">,</span>
    <span class="n">logging_level</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"WARNING"</span><span class="p">,</span>
    <span class="n">summary_writer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TensorboardSummaryWriter</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">show_round_summary</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sa">r</span><span class="sd">"""SNPE-C / APT [1].</span>

<span class="sd">    [1] _Automatic Posterior Transformation for Likelihood-free Inference_,</span>
<span class="sd">        Greenberg et al., ICML 2019, https://arxiv.org/abs/1905.07488.</span>

<span class="sd">    This class implements two loss variants of SNPE-C: the non-atomic and the atomic</span>
<span class="sd">    version. The atomic loss of SNPE-C can be used for any density estimator,</span>
<span class="sd">    i.e. also for normalizing flows. However, it suffers from leakage issues. On</span>
<span class="sd">    the other hand, the non-atomic loss can only be used only if the proposal</span>
<span class="sd">    distribution is a mixture of Gaussians, the density estimator is a mixture of</span>
<span class="sd">    Gaussians, and the prior is either Gaussian or Uniform. It does not suffer from</span>
<span class="sd">    leakage issues. At the beginning of each round, we print whether the non-atomic</span>
<span class="sd">    or the atomic version is used.</span>

<span class="sd">    In this codebase, we will automatically switch to the non-atomic loss if the</span>
<span class="sd">    following criteria are fulfilled:</span>
<span class="sd">    - proposal has is a `DirectPosterior` with density_estimator `mdn`, as built</span>
<span class="sd">        with `utils.sbi.posterior_nn()`.</span>
<span class="sd">    - the density estimator is a `mdn`, as built with `utils.sbi.posterior_nn()`.</span>
<span class="sd">    - `isinstance(prior, MultivariateNormal)` (from `torch.distributions`) or</span>
<span class="sd">        `isinstance(prior, sbi.utils.BoxUniform)`</span>

<span class="sd">    Note that custom implementations of any of these densities (or estimators) will</span>
<span class="sd">    not trigger the non-atomic loss, and the algorithm will fall back onto using</span>
<span class="sd">    the atomic loss.</span>

<span class="sd">    Args:</span>
<span class="sd">        simulator: A function that takes parameters $\theta$ and maps them to</span>
<span class="sd">            simulations, or observations, `x`, $\mathrm{sim}(\theta)\to x$. Any</span>
<span class="sd">            regular Python callable (i.e. function or class with `__call__` method)</span>
<span class="sd">            can be used.</span>
<span class="sd">        prior: A probability distribution that expresses prior knowledge about the</span>
<span class="sd">            parameters, e.g. which ranges are meaningful for them. Any</span>
<span class="sd">            object with `.log_prob()`and `.sample()` (for example, a PyTorch</span>
<span class="sd">            distribution) can be used.</span>
<span class="sd">        num_workers: Number of parallel workers to use for simulations.</span>
<span class="sd">        simulation_batch_size: Number of parameter sets that the simulator</span>
<span class="sd">            maps to data x at once. If None, we simulate all parameter sets at the</span>
<span class="sd">            same time. If &gt;= 1, the simulator has to process data of shape</span>
<span class="sd">            (simulation_batch_size, parameter_dimension).</span>
<span class="sd">        density_estimator: If it is a string, use a pre-configured network of the</span>
<span class="sd">            provided type (one of nsf, maf, mdn, made). Alternatively, a function</span>
<span class="sd">            that builds a custom neural network can be provided. The function will</span>
<span class="sd">            be called with the first batch of simulations (theta, x), which can</span>
<span class="sd">            thus be used for shape inference and potentially for z-scoring. It</span>
<span class="sd">            needs to return a PyTorch `nn.Module` implementing the density</span>
<span class="sd">            estimator. The density estimator needs to provide the methods</span>
<span class="sd">            `.log_prob` and `.sample()`.</span>
<span class="sd">        sample_with_mcmc: Whether to sample with MCMC. MCMC can be used to deal</span>
<span class="sd">            with high leakage.</span>
<span class="sd">        mcmc_method: Method used for MCMC sampling, one of `slice_np`, `slice`,</span>
<span class="sd">            `hmc`, `nuts`. Currently defaults to `slice_np` for a custom numpy</span>
<span class="sd">            implementation of slice sampling; select `hmc`, `nuts` or `slice` for</span>
<span class="sd">            Pyro-based sampling.</span>
<span class="sd">        mcmc_parameters: Dictionary overriding the default parameters for MCMC.</span>
<span class="sd">            The following parameters are supported: `thin` to set the thinning</span>
<span class="sd">            factor for the chain, `warmup_steps` to set the initial number of</span>
<span class="sd">            samples to discard, `num_chains` for the number of chains,</span>
<span class="sd">            `init_strategy` for the initialisation strategy for chains; `prior` will</span>
<span class="sd">            draw init locations from prior, whereas `sir` will use</span>
<span class="sd">            Sequential-Importance-Resampling using `init_strategy_num_candidates`</span>
<span class="sd">            to find init locations.</span>
<span class="sd">        use_combined_loss: Whether to train the neural net also on prior samples</span>
<span class="sd">            using maximum likelihood in addition to training it on all samples using</span>
<span class="sd">            atomic loss. The extra MLE loss helps prevent density leaking with</span>
<span class="sd">            bounded priors.</span>
<span class="sd">        device: torch device on which to compute, e.g. gpu, cpu.</span>
<span class="sd">        logging_level: Minimum severity of messages to log. One of the strings</span>
<span class="sd">            INFO, WARNING, DEBUG, ERROR and CRITICAL.</span>
<span class="sd">        summary_writer: A tensorboard `SummaryWriter` to control, among others, log</span>
<span class="sd">            file location (default is `&lt;current working directory&gt;/logs`.)</span>
<span class="sd">        show_progress_bars: Whether to show a progressbar during simulation and</span>
<span class="sd">            sampling.</span>
<span class="sd">        show_round_summary: Whether to show the validation loss and leakage after</span>
<span class="sd">            each round.</span>
<span class="sd">    """</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_use_combined_loss</span> <span class="o">=</span> <span class="n">use_combined_loss</span>

    <span class="n">kwargs</span> <span class="o">=</span> <span class="n">del_entries</span><span class="p">(</span>
        <span class="nb">locals</span><span class="p">(),</span> <span class="n">entries</span><span class="o">=</span><span class="p">(</span><span class="s2">"self"</span><span class="p">,</span> <span class="s2">"__class__"</span><span class="p">,</span> <span class="s2">"use_combined_loss"</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre>
</div>
</td>
</tr>
</table>
</details>
</div>
</div>
<div class="doc doc-object doc-method">
<h4 class="doc doc-heading" data-toc-label="provide_presimulated()" id="sbi.inference.snpe.snpe_c.SNPE_C.provide_presimulated">
<code class="highlight language-python">
provide_presimulated<span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">from_round</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> </code>
<span class="doc doc-properties">
<small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
</span>
</h4>
<div class="doc doc-contents">
<p>Provide external <span><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span> and <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> to be used for training later on.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>theta</code></td>
<td><code>Tensor</code></td>
<td>
<p>Parameter sets used to generate presimulated data.</p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>x</code></td>
<td><code>Tensor</code></td>
<td>
<p>Simulation outputs of presimulated data.</p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>from_round</code></td>
<td><code>int</code></td>
<td>
<p>Which round the data was simulated from. <code>from_round=0</code> means
that the data came from the first round, i.e. the prior.</p>
</td>
<td><code>0</code></td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Source code in <code>sbi/inference/snpe/snpe_c.py</code></summary>
<table class="highlighttable">
<tr>
<td class="linenos">
<div class="linenodiv">
<pre><span></span>164
165
166
167
168
169
170
171
172
173
174
175
176</pre>
</div>
</td>
<td class="code">
<div class="highlight">
<pre><span></span><code><span class="k">def</span> <span class="nf">provide_presimulated</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">from_round</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">"""</span>
<span class="sd">    Provide external $\theta$ and $x$ to be used for training later on.</span>

<span class="sd">    Args:</span>
<span class="sd">        theta: Parameter sets used to generate presimulated data.</span>
<span class="sd">        x: Simulation outputs of presimulated data.</span>
<span class="sd">        from_round: Which round the data was simulated from. `from_round=0` means</span>
<span class="sd">            that the data came from the first round, i.e. the prior.</span>
<span class="sd">    """</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_append_to_data_bank</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">from_round</span><span class="p">)</span>
</code></pre>
</div>
</td>
</tr>
</table>
</details>
</div>
</div>
</div>
</div>
</div>
<div class="doc doc-object doc-class">
<h3 class="doc doc-heading" id="sbi.inference.snle.snle_a.SNLE_A">
<code>sbi.inference.snle.snle_a.SNLE_A</code>
<a class="headerlink" href="#sbi.inference.snle.snle_a.SNLE_A" title="Permanent link">¶</a></h3>
<div class="doc doc-contents first">
<div class="doc doc-children">
<div class="doc doc-object doc-method">
<h4 class="doc doc-heading" data-toc-label="__call__()" id="sbi.inference.snle.snle_a.SNLE_A.__call__">
<code class="highlight language-python">
__call__<span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_simulations</span><span class="p">,</span> <span class="n">proposal</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">training_batch_size</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.0005</span><span class="p">,</span> <span class="n">validation_fraction</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">stop_after_epochs</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">max_num_epochs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">clip_max_norm</span><span class="o">=</span><span class="mf">5.0</span><span class="p">,</span> <span class="n">exclude_invalid_x</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">discard_prior_samples</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">retrain_from_scratch_each_round</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> </code>
<span class="doc doc-properties">
<small class="doc doc-property doc-property-special"><code>special</code></small>
</span>
</h4>
<div class="doc doc-contents">
<p>Run SNLE.</p>
<p>Return posterior <span><span class="MathJax_Preview">p(\theta|x)</span><script type="math/tex">p(\theta|x)</script></span> after inference.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>num_simulations</code></td>
<td><code>int</code></td>
<td>
<p>Number of simulator calls.</p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>proposal</code></td>
<td><code>Optional[Any]</code></td>
<td>
<p>Distribution that the parameters <span><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span> are drawn from.
<code>proposal=None</code> uses the prior. Setting the proposal to a distribution
targeted on a specific observation, e.g. a posterior <span><span class="MathJax_Preview">p(\theta|x_o)</span><script type="math/tex">p(\theta|x_o)</script></span>
obtained previously, can lead to less required simulations.</p>
</td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>training_batch_size</code></td>
<td><code>int</code></td>
<td>
<p>Training batch size.</p>
</td>
<td><code>50</code></td>
</tr>
<tr>
<td><code>learning_rate</code></td>
<td><code>float</code></td>
<td>
<p>Learning rate for Adam optimizer.</p>
</td>
<td><code>0.0005</code></td>
</tr>
<tr>
<td><code>validation_fraction</code></td>
<td><code>float</code></td>
<td>
<p>The fraction of data to use for validation.</p>
</td>
<td><code>0.1</code></td>
</tr>
<tr>
<td><code>stop_after_epochs</code></td>
<td><code>int</code></td>
<td>
<p>The number of epochs to wait for improvement on the
validation set before terminating training.</p>
</td>
<td><code>20</code></td>
</tr>
<tr>
<td><code>max_num_epochs</code></td>
<td><code>Optional[int]</code></td>
<td>
<p>Maximum number of epochs to run. If reached, we stop
training even when the validation loss is still decreasing. If None, we
train until validation loss increases (see also <code>stop_after_epochs</code>).</p>
</td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>clip_max_norm</code></td>
<td><code>Optional[float]</code></td>
<td>
<p>Value at which to clip the total gradient norm in order to
prevent exploding gradients. Use None for no clipping.</p>
</td>
<td><code>5.0</code></td>
</tr>
<tr>
<td><code>exclude_invalid_x</code></td>
<td><code>bool</code></td>
<td>
<p>Whether to exclude simulation outputs <code>x=NaN</code> or <code>x=±∞</code>
during training. Expect errors, silent or explicit, when <code>False</code>.</p>
</td>
<td><code>True</code></td>
</tr>
<tr>
<td><code>discard_prior_samples</code></td>
<td><code>bool</code></td>
<td>
<p>Whether to discard samples simulated in round 1, i.e.
from the prior. Training may be sped up by ignoring such less targeted
samples.</p>
</td>
<td><code>False</code></td>
</tr>
<tr>
<td><code>retrain_from_scratch_each_round</code></td>
<td><code>bool</code></td>
<td>
<p>Whether to retrain the conditional density
estimator for the posterior from scratch each round.</p>
</td>
<td><code>False</code></td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>NeuralPosterior</code></td>
<td>
<p>Posterior <span><span class="MathJax_Preview">p(\theta|x_o)</span><script type="math/tex">p(\theta|x_o)</script></span> that can be sampled and evaluated.</p>
</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Source code in <code>sbi/inference/snle/snle_a.py</code></summary>
<table class="highlighttable">
<tr>
<td class="linenos">
<div class="linenodiv">
<pre><span></span> 82
 83
 84
 85
 86
 87
 88
 89
 90
 91
 92
 93
 94
 95
 96
 97
 98
 99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128</pre>
</div>
</td>
<td class="code">
<div class="highlight">
<pre><span></span><code><span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">num_simulations</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">proposal</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">training_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">5e-4</span><span class="p">,</span>
    <span class="n">validation_fraction</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">stop_after_epochs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span>
    <span class="n">max_num_epochs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">clip_max_norm</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="mf">5.0</span><span class="p">,</span>
    <span class="n">exclude_invalid_x</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">discard_prior_samples</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">retrain_from_scratch_each_round</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">NeuralPosterior</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">"""Run SNLE.</span>

<span class="sd">    Return posterior $p(\theta|x)$ after inference.</span>

<span class="sd">    Args:</span>
<span class="sd">        num_simulations: Number of simulator calls.</span>
<span class="sd">        proposal: Distribution that the parameters $\theta$ are drawn from.</span>
<span class="sd">            `proposal=None` uses the prior. Setting the proposal to a distribution</span>
<span class="sd">            targeted on a specific observation, e.g. a posterior $p(\theta|x_o)$</span>
<span class="sd">            obtained previously, can lead to less required simulations.</span>
<span class="sd">        training_batch_size: Training batch size.</span>
<span class="sd">        learning_rate: Learning rate for Adam optimizer.</span>
<span class="sd">        validation_fraction: The fraction of data to use for validation.</span>
<span class="sd">        stop_after_epochs: The number of epochs to wait for improvement on the</span>
<span class="sd">            validation set before terminating training.</span>
<span class="sd">        max_num_epochs: Maximum number of epochs to run. If reached, we stop</span>
<span class="sd">            training even when the validation loss is still decreasing. If None, we</span>
<span class="sd">            train until validation loss increases (see also `stop_after_epochs`).</span>
<span class="sd">        clip_max_norm: Value at which to clip the total gradient norm in order to</span>
<span class="sd">            prevent exploding gradients. Use None for no clipping.</span>
<span class="sd">        exclude_invalid_x: Whether to exclude simulation outputs `x=NaN` or `x=±∞`</span>
<span class="sd">            during training. Expect errors, silent or explicit, when `False`.</span>
<span class="sd">        discard_prior_samples: Whether to discard samples simulated in round 1, i.e.</span>
<span class="sd">            from the prior. Training may be sped up by ignoring such less targeted</span>
<span class="sd">            samples.</span>
<span class="sd">        retrain_from_scratch_each_round: Whether to retrain the conditional density</span>
<span class="sd">            estimator for the posterior from scratch each round.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Posterior $p(\theta|x_o)$ that can be sampled and evaluated.</span>
<span class="sd">    """</span>
    <span class="n">kwargs</span> <span class="o">=</span> <span class="n">del_entries</span><span class="p">(</span><span class="nb">locals</span><span class="p">(),</span> <span class="n">entries</span><span class="o">=</span><span class="p">(</span><span class="s2">"self"</span><span class="p">,</span> <span class="s2">"__class__"</span><span class="p">))</span>
    <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre>
</div>
</td>
</tr>
</table>
</details>
</div>
</div>
<div class="doc doc-object doc-method">
<h4 class="doc doc-heading" data-toc-label="__init__()" id="sbi.inference.snle.snle_a.SNLE_A.__init__">
<code class="highlight language-python">
__init__<span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">simulator</span><span class="p">,</span> <span class="n">prior</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">simulation_batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">density_estimator</span><span class="o">=</span><span class="s1">'maf'</span><span class="p">,</span> <span class="n">mcmc_method</span><span class="o">=</span><span class="s1">'slice_np'</span><span class="p">,</span> <span class="n">mcmc_parameters</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">'cpu'</span><span class="p">,</span> <span class="n">logging_level</span><span class="o">=</span><span class="s1">'WARNING'</span><span class="p">,</span> <span class="n">summary_writer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">show_progress_bars</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">show_round_summary</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> </code>
<span class="doc doc-properties">
<small class="doc doc-property doc-property-special"><code>special</code></small>
</span>
</h4>
<div class="doc doc-contents">
<p>Sequential Neural Likelihood [1].</p>
<p>[1] Sequential Neural Likelihood: Fast Likelihood-free Inference with
Autoregressive Flows_, Papamakarios et al., AISTATS 2019,
<a href="https://arxiv.org/abs/1805.07226">https://arxiv.org/abs/1805.07226</a></p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>simulator</code></td>
<td><code>Callable</code></td>
<td>
<p>A function that takes parameters <span><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span> and maps them to
simulations, or observations, <code>x</code>, <span><span class="MathJax_Preview">\text{sim}(\theta)\to x</span><script type="math/tex">\text{sim}(\theta)\to x</script></span>. Any
regular Python callable (i.e. function or class with <code>__call__</code> method)
can be used.</p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>prior</code></td>
<td><code></code></td>
<td>
<p>A probability distribution that expresses prior knowledge about the
parameters, e.g. which ranges are meaningful for them. Any
object with <code>.log_prob()</code>and <code>.sample()</code> (for example, a PyTorch
distribution) can be used.</p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>num_workers</code></td>
<td><code>int</code></td>
<td>
<p>Number of parallel workers to use for simulations.</p>
</td>
<td><code>1</code></td>
</tr>
<tr>
<td><code>simulation_batch_size</code></td>
<td><code>int</code></td>
<td>
<p>Number of parameter sets that the simulator
maps to data x at once. If None, we simulate all parameter sets at the
same time. If &gt;= 1, the simulator has to process data of shape
(simulation_batch_size, parameter_dimension).</p>
</td>
<td><code>1</code></td>
</tr>
<tr>
<td><code>density_estimator</code></td>
<td><code>Union[str, Callable]</code></td>
<td>
<p>If it is a string, use a pre-configured network of the
provided type (one of nsf, maf, mdn, made). Alternatively, a function
that builds a custom neural network can be provided. The function will
be called with the first batch of simulations (theta, x), which can
thus be used for shape inference and potentially for z-scoring. It
needs to return a PyTorch <code>nn.Module</code> implementing the density
estimator. The density estimator needs to provide the methods
<code>.log_prob</code> and <code>.sample()</code>.</p>
</td>
<td><code>'maf'</code></td>
</tr>
<tr>
<td><code>mcmc_method</code></td>
<td><code>str</code></td>
<td>
<p>Method used for MCMC sampling, one of <code>slice_np</code>, <code>slice</code>, <code>hmc</code>, <code>nuts</code>.
Currently defaults to <code>slice_np</code> for a custom numpy implementation of
slice sampling; select <code>hmc</code>, <code>nuts</code> or <code>slice</code> for Pyro-based sampling.</p>
</td>
<td><code>'slice_np'</code></td>
</tr>
<tr>
<td><code>mcmc_parameters</code></td>
<td><code>Optional[Dict[str, Any]]</code></td>
<td>
<p>Dictionary overriding the default parameters for MCMC.
The following parameters are supported: <code>thin</code> to set the thinning
factor for the chain, <code>warmup_steps</code> to set the initial number of
samples to discard, <code>num_chains</code> for the number of chains, <code>init_strategy</code>
for the initialisation strategy for chains; <code>prior</code> will draw init
locations from prior, whereas <code>sir</code> will use Sequential-Importance-
Resampling using <code>init_strategy_num_candidates</code> to find init
locations.</p>
</td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>device</code></td>
<td><code>str</code></td>
<td>
<p>torch device on which to compute, e.g. gpu, cpu.</p>
</td>
<td><code>'cpu'</code></td>
</tr>
<tr>
<td><code>logging_level</code></td>
<td><code>Union[int, str]</code></td>
<td>
<p>Minimum severity of messages to log. One of the strings
INFO, WARNING, DEBUG, ERROR and CRITICAL.</p>
</td>
<td><code>'WARNING'</code></td>
</tr>
<tr>
<td><code>summary_writer</code></td>
<td><code>Optional[Writer]</code></td>
<td>
<p>A tensorboard <code>SummaryWriter</code> to control, among others, log
file location (default is <code>&lt;current working directory&gt;/logs</code>.)</p>
</td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>show_progress_bars</code></td>
<td><code>bool</code></td>
<td>
<p>Whether to show a progressbar during simulation and
sampling.</p>
</td>
<td><code>True</code></td>
</tr>
<tr>
<td><code>show_round_summary</code></td>
<td><code>bool</code></td>
<td>
<p>Whether to show the validation loss and leakage after
each round.</p>
</td>
<td><code>False</code></td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Source code in <code>sbi/inference/snle/snle_a.py</code></summary>
<table class="highlighttable">
<tr>
<td class="linenos">
<div class="linenodiv">
<pre><span></span>14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80</pre>
</div>
</td>
<td class="code">
<div class="highlight">
<pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">simulator</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
    <span class="n">prior</span><span class="p">,</span>
    <span class="n">num_workers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">simulation_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">density_estimator</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"maf"</span><span class="p">,</span>
    <span class="n">mcmc_method</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">"slice_np"</span><span class="p">,</span>
    <span class="n">mcmc_parameters</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">"cpu"</span><span class="p">,</span>
    <span class="n">logging_level</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"WARNING"</span><span class="p">,</span>
    <span class="n">summary_writer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TensorboardSummaryWriter</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">show_round_summary</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sa">r</span><span class="sd">"""Sequential Neural Likelihood [1].</span>

<span class="sd">    [1] Sequential Neural Likelihood: Fast Likelihood-free Inference with</span>
<span class="sd">    Autoregressive Flows_, Papamakarios et al., AISTATS 2019,</span>
<span class="sd">    https://arxiv.org/abs/1805.07226</span>

<span class="sd">    Args:</span>
<span class="sd">        simulator: A function that takes parameters $\theta$ and maps them to</span>
<span class="sd">            simulations, or observations, `x`, $\text{sim}(\theta)\to x$. Any</span>
<span class="sd">            regular Python callable (i.e. function or class with `__call__` method)</span>
<span class="sd">            can be used.</span>
<span class="sd">        prior: A probability distribution that expresses prior knowledge about the</span>
<span class="sd">            parameters, e.g. which ranges are meaningful for them. Any</span>
<span class="sd">            object with `.log_prob()`and `.sample()` (for example, a PyTorch</span>
<span class="sd">            distribution) can be used.</span>
<span class="sd">        num_workers: Number of parallel workers to use for simulations.</span>
<span class="sd">        simulation_batch_size: Number of parameter sets that the simulator</span>
<span class="sd">            maps to data x at once. If None, we simulate all parameter sets at the</span>
<span class="sd">            same time. If &gt;= 1, the simulator has to process data of shape</span>
<span class="sd">            (simulation_batch_size, parameter_dimension).</span>
<span class="sd">        density_estimator: If it is a string, use a pre-configured network of the</span>
<span class="sd">            provided type (one of nsf, maf, mdn, made). Alternatively, a function</span>
<span class="sd">            that builds a custom neural network can be provided. The function will</span>
<span class="sd">            be called with the first batch of simulations (theta, x), which can</span>
<span class="sd">            thus be used for shape inference and potentially for z-scoring. It</span>
<span class="sd">            needs to return a PyTorch `nn.Module` implementing the density</span>
<span class="sd">            estimator. The density estimator needs to provide the methods</span>
<span class="sd">            `.log_prob` and `.sample()`.</span>
<span class="sd">        mcmc_method: Method used for MCMC sampling, one of `slice_np`, `slice`, `hmc`, `nuts`.</span>
<span class="sd">            Currently defaults to `slice_np` for a custom numpy implementation of</span>
<span class="sd">            slice sampling; select `hmc`, `nuts` or `slice` for Pyro-based sampling.</span>
<span class="sd">        mcmc_parameters: Dictionary overriding the default parameters for MCMC.</span>
<span class="sd">            The following parameters are supported: `thin` to set the thinning</span>
<span class="sd">            factor for the chain, `warmup_steps` to set the initial number of</span>
<span class="sd">            samples to discard, `num_chains` for the number of chains, `init_strategy`</span>
<span class="sd">            for the initialisation strategy for chains; `prior` will draw init</span>
<span class="sd">            locations from prior, whereas `sir` will use Sequential-Importance-</span>
<span class="sd">            Resampling using `init_strategy_num_candidates` to find init</span>
<span class="sd">            locations.</span>
<span class="sd">        device: torch device on which to compute, e.g. gpu, cpu.</span>
<span class="sd">        logging_level: Minimum severity of messages to log. One of the strings</span>
<span class="sd">            INFO, WARNING, DEBUG, ERROR and CRITICAL.</span>
<span class="sd">        summary_writer: A tensorboard `SummaryWriter` to control, among others, log</span>
<span class="sd">            file location (default is `&lt;current working directory&gt;/logs`.)</span>
<span class="sd">        show_progress_bars: Whether to show a progressbar during simulation and</span>
<span class="sd">            sampling.</span>
<span class="sd">        show_round_summary: Whether to show the validation loss and leakage after</span>
<span class="sd">            each round.</span>
<span class="sd">    """</span>

    <span class="n">kwargs</span> <span class="o">=</span> <span class="n">del_entries</span><span class="p">(</span><span class="nb">locals</span><span class="p">(),</span> <span class="n">entries</span><span class="o">=</span><span class="p">(</span><span class="s2">"self"</span><span class="p">,</span> <span class="s2">"__class__"</span><span class="p">))</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre>
</div>
</td>
</tr>
</table>
</details>
</div>
</div>
<div class="doc doc-object doc-method">
<h4 class="doc doc-heading" data-toc-label="provide_presimulated()" id="sbi.inference.snle.snle_a.SNLE_A.provide_presimulated">
<code class="highlight language-python">
provide_presimulated<span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">from_round</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> </code>
<span class="doc doc-properties">
<small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
</span>
</h4>
<div class="doc doc-contents">
<p>Provide external <span><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span> and <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> to be used for training later on.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>theta</code></td>
<td><code>Tensor</code></td>
<td>
<p>Parameter sets used to generate presimulated data.</p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>x</code></td>
<td><code>Tensor</code></td>
<td>
<p>Simulation outputs of presimulated data.</p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>from_round</code></td>
<td><code>int</code></td>
<td>
<p>Which round the data was simulated from. <code>from_round=0</code> means
that the data came from the first round, i.e. the prior.</p>
</td>
<td><code>0</code></td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Source code in <code>sbi/inference/snle/snle_a.py</code></summary>
<table class="highlighttable">
<tr>
<td class="linenos">
<div class="linenodiv">
<pre><span></span>164
165
166
167
168
169
170
171
172
173
174
175
176</pre>
</div>
</td>
<td class="code">
<div class="highlight">
<pre><span></span><code><span class="k">def</span> <span class="nf">provide_presimulated</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">from_round</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">"""</span>
<span class="sd">    Provide external $\theta$ and $x$ to be used for training later on.</span>

<span class="sd">    Args:</span>
<span class="sd">        theta: Parameter sets used to generate presimulated data.</span>
<span class="sd">        x: Simulation outputs of presimulated data.</span>
<span class="sd">        from_round: Which round the data was simulated from. `from_round=0` means</span>
<span class="sd">            that the data came from the first round, i.e. the prior.</span>
<span class="sd">    """</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_append_to_data_bank</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">from_round</span><span class="p">)</span>
</code></pre>
</div>
</td>
</tr>
</table>
</details>
</div>
</div>
</div>
</div>
</div>
<div class="doc doc-object doc-class">
<h3 class="doc doc-heading" id="sbi.inference.snre.snre_a.SNRE_A">
<code>sbi.inference.snre.snre_a.SNRE_A</code>
<a class="headerlink" href="#sbi.inference.snre.snre_a.SNRE_A" title="Permanent link">¶</a></h3>
<div class="doc doc-contents first">
<div class="doc doc-children">
<div class="doc doc-object doc-method">
<h4 class="doc doc-heading" data-toc-label="__call__()" id="sbi.inference.snre.snre_a.SNRE_A.__call__">
<code class="highlight language-python">
__call__<span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_simulations</span><span class="p">,</span> <span class="n">proposal</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">training_batch_size</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.0005</span><span class="p">,</span> <span class="n">validation_fraction</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">stop_after_epochs</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">max_num_epochs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">clip_max_norm</span><span class="o">=</span><span class="mf">5.0</span><span class="p">,</span> <span class="n">exclude_invalid_x</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">discard_prior_samples</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">retrain_from_scratch_each_round</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> </code>
<span class="doc doc-properties">
<small class="doc doc-property doc-property-special"><code>special</code></small>
</span>
</h4>
<div class="doc doc-contents">
<p>Run AALR / SNRE_A.</p>
<p>Return posterior <span><span class="MathJax_Preview">p(\theta|x)</span><script type="math/tex">p(\theta|x)</script></span> after inference.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>num_simulations</code></td>
<td><code>int</code></td>
<td>
<p>Number of simulator calls.</p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>proposal</code></td>
<td><code>Optional[Any]</code></td>
<td>
<p>Distribution that the parameters <span><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span> are drawn from.
<code>proposal=None</code> uses the prior. Setting the proposal to a distribution
targeted on a specific observation, e.g. a posterior <span><span class="MathJax_Preview">p(\theta|x_o)</span><script type="math/tex">p(\theta|x_o)</script></span>
obtained previously, can lead to less required simulations.</p>
</td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>training_batch_size</code></td>
<td><code>int</code></td>
<td>
<p>Training batch size.</p>
</td>
<td><code>50</code></td>
</tr>
<tr>
<td><code>learning_rate</code></td>
<td><code>float</code></td>
<td>
<p>Learning rate for Adam optimizer.</p>
</td>
<td><code>0.0005</code></td>
</tr>
<tr>
<td><code>validation_fraction</code></td>
<td><code>float</code></td>
<td>
<p>The fraction of data to use for validation.</p>
</td>
<td><code>0.1</code></td>
</tr>
<tr>
<td><code>stop_after_epochs</code></td>
<td><code>int</code></td>
<td>
<p>The number of epochs to wait for improvement on the
validation set before terminating training.</p>
</td>
<td><code>20</code></td>
</tr>
<tr>
<td><code>max_num_epochs</code></td>
<td><code>Optional[int]</code></td>
<td>
<p>Maximum number of epochs to run. If reached, we stop
training even when the validation loss is still decreasing. If None, we
train until validation loss increases (see also <code>stop_after_epochs</code>).</p>
</td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>clip_max_norm</code></td>
<td><code>Optional[float]</code></td>
<td>
<p>Value at which to clip the total gradient norm in order to
prevent exploding gradients. Use None for no clipping.</p>
</td>
<td><code>5.0</code></td>
</tr>
<tr>
<td><code>exclude_invalid_x</code></td>
<td><code>bool</code></td>
<td>
<p>Whether to exclude simulation outputs <code>x=NaN</code> or <code>x=±∞</code>
during training. Expect errors, silent or explicit, when <code>False</code>.</p>
</td>
<td><code>True</code></td>
</tr>
<tr>
<td><code>discard_prior_samples</code></td>
<td><code>bool</code></td>
<td>
<p>Whether to discard samples simulated in round 1, i.e.
from the prior. Training may be sped up by ignoring such less targeted
samples.</p>
</td>
<td><code>False</code></td>
</tr>
<tr>
<td><code>retrain_from_scratch_each_round</code></td>
<td><code>bool</code></td>
<td>
<p>Whether to retrain the conditional density
estimator for the posterior from scratch each round.</p>
</td>
<td><code>False</code></td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>NeuralPosterior</code></td>
<td>
<p>Posterior <span><span class="MathJax_Preview">p(\theta|x)</span><script type="math/tex">p(\theta|x)</script></span> that can be sampled and evaluated.</p>
</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Source code in <code>sbi/inference/snre/snre_a.py</code></summary>
<table class="highlighttable">
<tr>
<td class="linenos">
<div class="linenodiv">
<pre><span></span> 79
 80
 81
 82
 83
 84
 85
 86
 87
 88
 89
 90
 91
 92
 93
 94
 95
 96
 97
 98
 99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128</pre>
</div>
</td>
<td class="code">
<div class="highlight">
<pre><span></span><code><span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">num_simulations</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">proposal</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">training_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">5e-4</span><span class="p">,</span>
    <span class="n">validation_fraction</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">stop_after_epochs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span>
    <span class="n">max_num_epochs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">clip_max_norm</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="mf">5.0</span><span class="p">,</span>
    <span class="n">exclude_invalid_x</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">discard_prior_samples</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">retrain_from_scratch_each_round</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">NeuralPosterior</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">"""Run AALR / SNRE_A.</span>

<span class="sd">    Return posterior $p(\theta|x)$ after inference.</span>

<span class="sd">    Args:</span>
<span class="sd">        num_simulations: Number of simulator calls.</span>
<span class="sd">        proposal: Distribution that the parameters $\theta$ are drawn from.</span>
<span class="sd">            `proposal=None` uses the prior. Setting the proposal to a distribution</span>
<span class="sd">            targeted on a specific observation, e.g. a posterior $p(\theta|x_o)$</span>
<span class="sd">            obtained previously, can lead to less required simulations.</span>
<span class="sd">        training_batch_size: Training batch size.</span>
<span class="sd">        learning_rate: Learning rate for Adam optimizer.</span>
<span class="sd">        validation_fraction: The fraction of data to use for validation.</span>
<span class="sd">        stop_after_epochs: The number of epochs to wait for improvement on the</span>
<span class="sd">            validation set before terminating training.</span>
<span class="sd">        max_num_epochs: Maximum number of epochs to run. If reached, we stop</span>
<span class="sd">            training even when the validation loss is still decreasing. If None, we</span>
<span class="sd">            train until validation loss increases (see also `stop_after_epochs`).</span>
<span class="sd">        clip_max_norm: Value at which to clip the total gradient norm in order to</span>
<span class="sd">            prevent exploding gradients. Use None for no clipping.</span>
<span class="sd">        exclude_invalid_x: Whether to exclude simulation outputs `x=NaN` or `x=±∞`</span>
<span class="sd">            during training. Expect errors, silent or explicit, when `False`.</span>
<span class="sd">        discard_prior_samples: Whether to discard samples simulated in round 1, i.e.</span>
<span class="sd">            from the prior. Training may be sped up by ignoring such less targeted</span>
<span class="sd">            samples.</span>
<span class="sd">        retrain_from_scratch_each_round: Whether to retrain the conditional density</span>
<span class="sd">            estimator for the posterior from scratch each round.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Posterior $p(\theta|x)$ that can be sampled and evaluated.</span>
<span class="sd">    """</span>

    <span class="c1"># AALR is defined for `num_atoms=2`.</span>
    <span class="c1"># Proxy to `super().__call__` to ensure right parameter.</span>
    <span class="n">kwargs</span> <span class="o">=</span> <span class="n">del_entries</span><span class="p">(</span><span class="nb">locals</span><span class="p">(),</span> <span class="n">entries</span><span class="o">=</span><span class="p">(</span><span class="s2">"self"</span><span class="p">,</span> <span class="s2">"__class__"</span><span class="p">))</span>
    <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">,</span> <span class="n">num_atoms</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</code></pre>
</div>
</td>
</tr>
</table>
</details>
</div>
</div>
<div class="doc doc-object doc-method">
<h4 class="doc doc-heading" data-toc-label="__init__()" id="sbi.inference.snre.snre_a.SNRE_A.__init__">
<code class="highlight language-python">
__init__<span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">simulator</span><span class="p">,</span> <span class="n">prior</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">simulation_batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">classifier</span><span class="o">=</span><span class="s1">'resnet'</span><span class="p">,</span> <span class="n">mcmc_method</span><span class="o">=</span><span class="s1">'slice_np'</span><span class="p">,</span> <span class="n">mcmc_parameters</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">'cpu'</span><span class="p">,</span> <span class="n">logging_level</span><span class="o">=</span><span class="s1">'warning'</span><span class="p">,</span> <span class="n">summary_writer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">show_progress_bars</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">show_round_summary</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> </code>
<span class="doc doc-properties">
<small class="doc doc-property doc-property-special"><code>special</code></small>
</span>
</h4>
<div class="doc doc-contents">
<p>AALR[1], here known as SNRE_A.</p>
<p>[1] <em>Likelihood-free MCMC with Amortized Approximate Likelihood Ratios</em>, Hermans
    et al., ICML 2020, <a href="https://arxiv.org/abs/1903.04057">https://arxiv.org/abs/1903.04057</a></p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>simulator</code></td>
<td><code>Callable</code></td>
<td>
<p>A function that takes parameters <span><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span> and maps them to
simulations, or observations, <code>x</code>, <span><span class="MathJax_Preview">\mathrm{sim}(\theta)\to x</span><script type="math/tex">\mathrm{sim}(\theta)\to x</script></span>. Any
regular Python callable (i.e. function or class with <code>__call__</code> method)
can be used.</p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>prior</code></td>
<td><code></code></td>
<td>
<p>A probability distribution that expresses prior knowledge about the
parameters, e.g. which ranges are meaningful for them. Any
object with <code>.log_prob()</code>and <code>.sample()</code> (for example, a PyTorch
distribution) can be used.</p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>num_workers</code></td>
<td><code>int</code></td>
<td>
<p>Number of parallel workers to use for simulations.</p>
</td>
<td><code>1</code></td>
</tr>
<tr>
<td><code>simulation_batch_size</code></td>
<td><code>int</code></td>
<td>
<p>Number of parameter sets that the simulator
maps to data x at once. If None, we simulate all parameter sets at the
same time. If &gt;= 1, the simulator has to process data of shape
(simulation_batch_size, parameter_dimension).</p>
</td>
<td><code>1</code></td>
</tr>
<tr>
<td><code>classifier</code></td>
<td><code>Union[str, Callable]</code></td>
<td>
<p>Classifier trained to approximate likelihood ratios. If it is
a string, use a pre-configured network of the provided type (one of
linear, mlp, resnet). Alternatively, a function that builds a custom
neural network can be provided. The function will be called with the
first batch of simulations (theta, x), which can thus be used for shape
inference and potentially for z-scoring. It needs to return a PyTorch
<code>nn.Module</code> implementing the classifier.</p>
</td>
<td><code>'resnet'</code></td>
</tr>
<tr>
<td><code>mcmc_method</code></td>
<td><code>str</code></td>
<td>
<p>Method used for MCMC sampling, one of <code>slice_np</code>, <code>slice</code>, <code>hmc</code>, <code>nuts</code>.
Currently defaults to <code>slice_np</code> for a custom numpy implementation of
slice sampling; select <code>hmc</code>, <code>nuts</code> or <code>slice</code> for Pyro-based sampling.</p>
</td>
<td><code>'slice_np'</code></td>
</tr>
<tr>
<td><code>mcmc_parameters</code></td>
<td><code>Optional[Dict[str, Any]]</code></td>
<td>
<p>Dictionary overriding the default parameters for MCMC.
The following parameters are supported: <code>thin</code> to set the thinning
factor for the chain, <code>warmup_steps</code> to set the initial number of
samples to discard, <code>num_chains</code> for the number of chains, <code>init_strategy</code>
for the initialisation strategy for chains; <code>prior</code> will draw init
locations from prior, whereas <code>sir</code> will use Sequential-Importance-
Resampling using <code>init_strategy_num_candidates</code> to find init
locations.</p>
</td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>device</code></td>
<td><code>str</code></td>
<td>
<p>torch device on which to compute, e.g. gpu, cpu.</p>
</td>
<td><code>'cpu'</code></td>
</tr>
<tr>
<td><code>logging_level</code></td>
<td><code>Union[int, str]</code></td>
<td>
<p>Minimum severity of messages to log. One of the strings
INFO, WARNING, DEBUG, ERROR and CRITICAL.</p>
</td>
<td><code>'warning'</code></td>
</tr>
<tr>
<td><code>summary_writer</code></td>
<td><code>Optional[Writer]</code></td>
<td>
<p>A tensorboard <code>SummaryWriter</code> to control, among others, log
file location (default is <code>&lt;current working directory&gt;/logs</code>.)</p>
</td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>show_progress_bars</code></td>
<td><code>bool</code></td>
<td>
<p>Whether to show a progressbar during simulation and
sampling.</p>
</td>
<td><code>True</code></td>
</tr>
<tr>
<td><code>show_round_summary</code></td>
<td><code>bool</code></td>
<td>
<p>Whether to show the validation loss and leakage after
each round.</p>
</td>
<td><code>False</code></td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Source code in <code>sbi/inference/snre/snre_a.py</code></summary>
<table class="highlighttable">
<tr>
<td class="linenos">
<div class="linenodiv">
<pre><span></span>13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77</pre>
</div>
</td>
<td class="code">
<div class="highlight">
<pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">simulator</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
    <span class="n">prior</span><span class="p">,</span>
    <span class="n">num_workers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">simulation_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">classifier</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"resnet"</span><span class="p">,</span>
    <span class="n">mcmc_method</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">"slice_np"</span><span class="p">,</span>
    <span class="n">mcmc_parameters</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">"cpu"</span><span class="p">,</span>
    <span class="n">logging_level</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"warning"</span><span class="p">,</span>
    <span class="n">summary_writer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TensorboardSummaryWriter</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">show_round_summary</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sa">r</span><span class="sd">"""AALR[1], here known as SNRE_A.</span>

<span class="sd">    [1] _Likelihood-free MCMC with Amortized Approximate Likelihood Ratios_, Hermans</span>
<span class="sd">        et al., ICML 2020, https://arxiv.org/abs/1903.04057</span>

<span class="sd">    Args:</span>
<span class="sd">        simulator: A function that takes parameters $\theta$ and maps them to</span>
<span class="sd">            simulations, or observations, `x`, $\mathrm{sim}(\theta)\to x$. Any</span>
<span class="sd">            regular Python callable (i.e. function or class with `__call__` method)</span>
<span class="sd">            can be used.</span>
<span class="sd">        prior: A probability distribution that expresses prior knowledge about the</span>
<span class="sd">            parameters, e.g. which ranges are meaningful for them. Any</span>
<span class="sd">            object with `.log_prob()`and `.sample()` (for example, a PyTorch</span>
<span class="sd">            distribution) can be used.</span>
<span class="sd">        num_workers: Number of parallel workers to use for simulations.</span>
<span class="sd">        simulation_batch_size: Number of parameter sets that the simulator</span>
<span class="sd">            maps to data x at once. If None, we simulate all parameter sets at the</span>
<span class="sd">            same time. If &gt;= 1, the simulator has to process data of shape</span>
<span class="sd">            (simulation_batch_size, parameter_dimension).</span>
<span class="sd">        classifier: Classifier trained to approximate likelihood ratios. If it is</span>
<span class="sd">            a string, use a pre-configured network of the provided type (one of</span>
<span class="sd">            linear, mlp, resnet). Alternatively, a function that builds a custom</span>
<span class="sd">            neural network can be provided. The function will be called with the</span>
<span class="sd">            first batch of simulations (theta, x), which can thus be used for shape</span>
<span class="sd">            inference and potentially for z-scoring. It needs to return a PyTorch</span>
<span class="sd">            `nn.Module` implementing the classifier.</span>
<span class="sd">        mcmc_method: Method used for MCMC sampling, one of `slice_np`, `slice`, `hmc`, `nuts`.</span>
<span class="sd">            Currently defaults to `slice_np` for a custom numpy implementation of</span>
<span class="sd">            slice sampling; select `hmc`, `nuts` or `slice` for Pyro-based sampling.</span>
<span class="sd">        mcmc_parameters: Dictionary overriding the default parameters for MCMC.</span>
<span class="sd">            The following parameters are supported: `thin` to set the thinning</span>
<span class="sd">            factor for the chain, `warmup_steps` to set the initial number of</span>
<span class="sd">            samples to discard, `num_chains` for the number of chains, `init_strategy`</span>
<span class="sd">            for the initialisation strategy for chains; `prior` will draw init</span>
<span class="sd">            locations from prior, whereas `sir` will use Sequential-Importance-</span>
<span class="sd">            Resampling using `init_strategy_num_candidates` to find init</span>
<span class="sd">            locations.</span>
<span class="sd">        device: torch device on which to compute, e.g. gpu, cpu.</span>
<span class="sd">        logging_level: Minimum severity of messages to log. One of the strings</span>
<span class="sd">            INFO, WARNING, DEBUG, ERROR and CRITICAL.</span>
<span class="sd">        summary_writer: A tensorboard `SummaryWriter` to control, among others, log</span>
<span class="sd">            file location (default is `&lt;current working directory&gt;/logs`.)</span>
<span class="sd">        show_progress_bars: Whether to show a progressbar during simulation and</span>
<span class="sd">            sampling.</span>
<span class="sd">        show_round_summary: Whether to show the validation loss and leakage after</span>
<span class="sd">            each round.</span>
<span class="sd">    """</span>

    <span class="n">kwargs</span> <span class="o">=</span> <span class="n">del_entries</span><span class="p">(</span><span class="nb">locals</span><span class="p">(),</span> <span class="n">entries</span><span class="o">=</span><span class="p">(</span><span class="s2">"self"</span><span class="p">,</span> <span class="s2">"__class__"</span><span class="p">))</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre>
</div>
</td>
</tr>
</table>
</details>
</div>
</div>
<div class="doc doc-object doc-method">
<h4 class="doc doc-heading" data-toc-label="provide_presimulated()" id="sbi.inference.snre.snre_a.SNRE_A.provide_presimulated">
<code class="highlight language-python">
provide_presimulated<span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">from_round</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> </code>
<span class="doc doc-properties">
<small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
</span>
</h4>
<div class="doc doc-contents">
<p>Provide external <span><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span> and <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> to be used for training later on.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>theta</code></td>
<td><code>Tensor</code></td>
<td>
<p>Parameter sets used to generate presimulated data.</p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>x</code></td>
<td><code>Tensor</code></td>
<td>
<p>Simulation outputs of presimulated data.</p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>from_round</code></td>
<td><code>int</code></td>
<td>
<p>Which round the data was simulated from. <code>from_round=0</code> means
that the data came from the first round, i.e. the prior.</p>
</td>
<td><code>0</code></td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Source code in <code>sbi/inference/snre/snre_a.py</code></summary>
<table class="highlighttable">
<tr>
<td class="linenos">
<div class="linenodiv">
<pre><span></span>164
165
166
167
168
169
170
171
172
173
174
175
176</pre>
</div>
</td>
<td class="code">
<div class="highlight">
<pre><span></span><code><span class="k">def</span> <span class="nf">provide_presimulated</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">from_round</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">"""</span>
<span class="sd">    Provide external $\theta$ and $x$ to be used for training later on.</span>

<span class="sd">    Args:</span>
<span class="sd">        theta: Parameter sets used to generate presimulated data.</span>
<span class="sd">        x: Simulation outputs of presimulated data.</span>
<span class="sd">        from_round: Which round the data was simulated from. `from_round=0` means</span>
<span class="sd">            that the data came from the first round, i.e. the prior.</span>
<span class="sd">    """</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_append_to_data_bank</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">from_round</span><span class="p">)</span>
</code></pre>
</div>
</td>
</tr>
</table>
</details>
</div>
</div>
</div>
</div>
</div>
<div class="doc doc-object doc-class">
<h3 class="doc doc-heading" id="sbi.inference.snre.snre_b.SNRE_B">
<code>sbi.inference.snre.snre_b.SNRE_B</code>
<a class="headerlink" href="#sbi.inference.snre.snre_b.SNRE_B" title="Permanent link">¶</a></h3>
<div class="doc doc-contents first">
<div class="doc doc-children">
<div class="doc doc-object doc-method">
<h4 class="doc doc-heading" data-toc-label="__call__()" id="sbi.inference.snre.snre_b.SNRE_B.__call__">
<code class="highlight language-python">
__call__<span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_simulations</span><span class="p">,</span> <span class="n">proposal</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">num_atoms</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">training_batch_size</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.0005</span><span class="p">,</span> <span class="n">validation_fraction</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">stop_after_epochs</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">max_num_epochs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">clip_max_norm</span><span class="o">=</span><span class="mf">5.0</span><span class="p">,</span> <span class="n">exclude_invalid_x</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">discard_prior_samples</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">retrain_from_scratch_each_round</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> </code>
<span class="doc doc-properties">
<small class="doc doc-property doc-property-special"><code>special</code></small>
</span>
</h4>
<div class="doc doc-contents">
<p>Run SRE / SNRE_B.</p>
<p>Return posterior <span><span class="MathJax_Preview">p(\theta|x)</span><script type="math/tex">p(\theta|x)</script></span> after inference (possibly over several rounds).</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>num_simulations</code></td>
<td><code>int</code></td>
<td>
<p>Number of simulator calls.</p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>proposal</code></td>
<td><code>Optional[Any]</code></td>
<td>
<p>Distribution that the parameters <span><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span> are drawn from.
<code>proposal=None</code> uses the prior. Setting the proposal to a distribution
targeted on a specific observation, e.g. a posterior <span><span class="MathJax_Preview">p(\theta|x_o)</span><script type="math/tex">p(\theta|x_o)</script></span>
obtained previously, can lead to less required simulations.</p>
</td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>training_batch_size</code></td>
<td><code>int</code></td>
<td>
<p>Training batch size.</p>
</td>
<td><code>50</code></td>
</tr>
<tr>
<td><code>learning_rate</code></td>
<td><code>float</code></td>
<td>
<p>Learning rate for Adam optimizer.</p>
</td>
<td><code>0.0005</code></td>
</tr>
<tr>
<td><code>validation_fraction</code></td>
<td><code>float</code></td>
<td>
<p>The fraction of data to use for validation.</p>
</td>
<td><code>0.1</code></td>
</tr>
<tr>
<td><code>stop_after_epochs</code></td>
<td><code>int</code></td>
<td>
<p>The number of epochs to wait for improvement on the
validation set before terminating training.</p>
</td>
<td><code>20</code></td>
</tr>
<tr>
<td><code>max_num_epochs</code></td>
<td><code>Optional[int]</code></td>
<td>
<p>Maximum number of epochs to run. If reached, we stop
training even when the validation loss is still decreasing. If None, we
train until validation loss increases (see also <code>stop_after_epochs</code>).</p>
</td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>clip_max_norm</code></td>
<td><code>Optional[float]</code></td>
<td>
<p>Value at which to clip the total gradient norm in order to
prevent exploding gradients. Use None for no clipping.</p>
</td>
<td><code>5.0</code></td>
</tr>
<tr>
<td><code>exclude_invalid_x</code></td>
<td><code>bool</code></td>
<td>
<p>Whether to exclude simulation outputs <code>x=NaN</code> or <code>x=±∞</code>
during training. Expect errors, silent or explicit, when <code>False</code>.</p>
</td>
<td><code>True</code></td>
</tr>
<tr>
<td><code>discard_prior_samples</code></td>
<td><code>bool</code></td>
<td>
<p>Whether to discard samples simulated in round 1, i.e.
from the prior. Training may be sped up by ignoring such less targeted
samples.</p>
</td>
<td><code>False</code></td>
</tr>
<tr>
<td><code>retrain_from_scratch_each_round</code></td>
<td><code>bool</code></td>
<td>
<p>Whether to retrain the conditional density
estimator for the posterior from scratch each round.</p>
</td>
<td><code>False</code></td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>NeuralPosterior</code></td>
<td>
<p>Posterior <span><span class="MathJax_Preview">p(\theta|x)</span><script type="math/tex">p(\theta|x)</script></span> that can be sampled and evaluated.</p>
</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Source code in <code>sbi/inference/snre/snre_b.py</code></summary>
<table class="highlighttable">
<tr>
<td class="linenos">
<div class="linenodiv">
<pre><span></span> 79
 80
 81
 82
 83
 84
 85
 86
 87
 88
 89
 90
 91
 92
 93
 94
 95
 96
 97
 98
 99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126</pre>
</div>
</td>
<td class="code">
<div class="highlight">
<pre><span></span><code><span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">num_simulations</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">proposal</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">num_atoms</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
    <span class="n">training_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">5e-4</span><span class="p">,</span>
    <span class="n">validation_fraction</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">stop_after_epochs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span>
    <span class="n">max_num_epochs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">clip_max_norm</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="mf">5.0</span><span class="p">,</span>
    <span class="n">exclude_invalid_x</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">discard_prior_samples</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">retrain_from_scratch_each_round</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">NeuralPosterior</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">"""Run SRE / SNRE_B.</span>

<span class="sd">    Return posterior $p(\theta|x)$ after inference (possibly over several rounds).</span>

<span class="sd">    Args:</span>
<span class="sd">        num_simulations: Number of simulator calls.</span>
<span class="sd">        proposal: Distribution that the parameters $\theta$ are drawn from.</span>
<span class="sd">            `proposal=None` uses the prior. Setting the proposal to a distribution</span>
<span class="sd">            targeted on a specific observation, e.g. a posterior $p(\theta|x_o)$</span>
<span class="sd">            obtained previously, can lead to less required simulations.</span>
<span class="sd">        training_batch_size: Training batch size.</span>
<span class="sd">        learning_rate: Learning rate for Adam optimizer.</span>
<span class="sd">        validation_fraction: The fraction of data to use for validation.</span>
<span class="sd">        stop_after_epochs: The number of epochs to wait for improvement on the</span>
<span class="sd">            validation set before terminating training.</span>
<span class="sd">        max_num_epochs: Maximum number of epochs to run. If reached, we stop</span>
<span class="sd">            training even when the validation loss is still decreasing. If None, we</span>
<span class="sd">            train until validation loss increases (see also `stop_after_epochs`).</span>
<span class="sd">        clip_max_norm: Value at which to clip the total gradient norm in order to</span>
<span class="sd">            prevent exploding gradients. Use None for no clipping.</span>
<span class="sd">        exclude_invalid_x: Whether to exclude simulation outputs `x=NaN` or `x=±∞`</span>
<span class="sd">            during training. Expect errors, silent or explicit, when `False`.</span>
<span class="sd">        discard_prior_samples: Whether to discard samples simulated in round 1, i.e.</span>
<span class="sd">            from the prior. Training may be sped up by ignoring such less targeted</span>
<span class="sd">            samples.</span>
<span class="sd">        retrain_from_scratch_each_round: Whether to retrain the conditional density</span>
<span class="sd">            estimator for the posterior from scratch each round.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Posterior $p(\theta|x)$ that can be sampled and evaluated.</span>
<span class="sd">    """</span>
    <span class="n">kwargs</span> <span class="o">=</span> <span class="n">del_entries</span><span class="p">(</span><span class="nb">locals</span><span class="p">(),</span> <span class="n">entries</span><span class="o">=</span><span class="p">(</span><span class="s2">"self"</span><span class="p">,</span> <span class="s2">"__class__"</span><span class="p">))</span>
    <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre>
</div>
</td>
</tr>
</table>
</details>
</div>
</div>
<div class="doc doc-object doc-method">
<h4 class="doc doc-heading" data-toc-label="__init__()" id="sbi.inference.snre.snre_b.SNRE_B.__init__">
<code class="highlight language-python">
__init__<span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">simulator</span><span class="p">,</span> <span class="n">prior</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">simulation_batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">classifier</span><span class="o">=</span><span class="s1">'resnet'</span><span class="p">,</span> <span class="n">mcmc_method</span><span class="o">=</span><span class="s1">'slice_np'</span><span class="p">,</span> <span class="n">mcmc_parameters</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">'cpu'</span><span class="p">,</span> <span class="n">logging_level</span><span class="o">=</span><span class="s1">'warning'</span><span class="p">,</span> <span class="n">summary_writer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">show_progress_bars</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">show_round_summary</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> </code>
<span class="doc doc-properties">
<small class="doc doc-property doc-property-special"><code>special</code></small>
</span>
</h4>
<div class="doc doc-contents">
<p>SRE[1], here known as SNRE_B.</p>
<p>[1] <em>On Contrastive Learning for Likelihood-free Inference</em>, Durkan et al.,
    ICML 2020, <a href="https://arxiv.org/pdf/2002.03712">https://arxiv.org/pdf/2002.03712</a></p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>simulator</code></td>
<td><code>Callable</code></td>
<td>
<p>A function that takes parameters <span><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span> and maps them to
simulations, or observations, <code>x</code>, <span><span class="MathJax_Preview">\mathrm{sim}(\theta)\to x</span><script type="math/tex">\mathrm{sim}(\theta)\to x</script></span>. Any
regular Python callable (i.e. function or class with <code>__call__</code> method)
can be used.</p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>prior</code></td>
<td><code></code></td>
<td>
<p>A probability distribution that expresses prior knowledge about the
parameters, e.g. which ranges are meaningful for them. Any
object with <code>.log_prob()</code>and <code>.sample()</code> (for example, a PyTorch
distribution) can be used.</p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>num_workers</code></td>
<td><code>int</code></td>
<td>
<p>Number of parallel workers to use for simulations.</p>
</td>
<td><code>1</code></td>
</tr>
<tr>
<td><code>simulation_batch_size</code></td>
<td><code>int</code></td>
<td>
<p>Number of parameter sets that the simulator
maps to data x at once. If None, we simulate all parameter sets at the
same time. If &gt;= 1, the simulator has to process data of shape
(simulation_batch_size, parameter_dimension).</p>
</td>
<td><code>1</code></td>
</tr>
<tr>
<td><code>classifier</code></td>
<td><code>Union[str, Callable]</code></td>
<td>
<p>Classifier trained to approximate likelihood ratios. If it is
a string, use a pre-configured network of the provided type (one of
linear, mlp, resnet). Alternatively, a function that builds a custom
neural network can be provided. The function will be called with the
first batch of simulations (theta, x), which can thus be used for shape
inference and potentially for z-scoring. It needs to return a PyTorch
<code>nn.Module</code> implementing the classifier.</p>
</td>
<td><code>'resnet'</code></td>
</tr>
<tr>
<td><code>mcmc_method</code></td>
<td><code>str</code></td>
<td>
<p>Method used for MCMC sampling, one of <code>slice_np</code>, <code>slice</code>, <code>hmc</code>, <code>nuts</code>.
Currently defaults to <code>slice_np</code> for a custom numpy implementation of
slice sampling; select <code>hmc</code>, <code>nuts</code> or <code>slice</code> for Pyro-based sampling.</p>
</td>
<td><code>'slice_np'</code></td>
</tr>
<tr>
<td><code>mcmc_parameters</code></td>
<td><code>Optional[Dict[str, Any]]</code></td>
<td>
<p>Dictionary overriding the default parameters for MCMC.
The following parameters are supported: <code>thin</code> to set the thinning
factor for the chain, <code>warmup_steps</code> to set the initial number of
samples to discard, <code>num_chains</code> for the number of chains, <code>init_strategy</code>
for the initialisation strategy for chains; <code>prior</code> will draw init
locations from prior, whereas <code>sir</code> will use Sequential-Importance-
Resampling using <code>init_strategy_num_candidates</code> to find init
locations.</p>
</td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>device</code></td>
<td><code>str</code></td>
<td>
<p>torch device on which to compute, e.g. gpu, cpu.</p>
</td>
<td><code>'cpu'</code></td>
</tr>
<tr>
<td><code>logging_level</code></td>
<td><code>Union[int, str]</code></td>
<td>
<p>Minimum severity of messages to log. One of the strings
INFO, WARNING, DEBUG, ERROR and CRITICAL.</p>
</td>
<td><code>'warning'</code></td>
</tr>
<tr>
<td><code>summary_writer</code></td>
<td><code>Optional[Writer]</code></td>
<td>
<p>A tensorboard <code>SummaryWriter</code> to control, among others, log
file location (default is <code>&lt;current working directory&gt;/logs</code>.)</p>
</td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>show_progress_bars</code></td>
<td><code>bool</code></td>
<td>
<p>Whether to show a progressbar during simulation and
sampling.</p>
</td>
<td><code>True</code></td>
</tr>
<tr>
<td><code>show_round_summary</code></td>
<td><code>bool</code></td>
<td>
<p>Whether to show the validation loss and leakage after
each round.</p>
</td>
<td><code>False</code></td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Source code in <code>sbi/inference/snre/snre_b.py</code></summary>
<table class="highlighttable">
<tr>
<td class="linenos">
<div class="linenodiv">
<pre><span></span>13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77</pre>
</div>
</td>
<td class="code">
<div class="highlight">
<pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">simulator</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
    <span class="n">prior</span><span class="p">,</span>
    <span class="n">num_workers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">simulation_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">classifier</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"resnet"</span><span class="p">,</span>
    <span class="n">mcmc_method</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">"slice_np"</span><span class="p">,</span>
    <span class="n">mcmc_parameters</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">"cpu"</span><span class="p">,</span>
    <span class="n">logging_level</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"warning"</span><span class="p">,</span>
    <span class="n">summary_writer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TensorboardSummaryWriter</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">show_round_summary</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sa">r</span><span class="sd">"""SRE[1], here known as SNRE_B.</span>

<span class="sd">    [1] _On Contrastive Learning for Likelihood-free Inference_, Durkan et al.,</span>
<span class="sd">        ICML 2020, https://arxiv.org/pdf/2002.03712</span>

<span class="sd">    Args:</span>
<span class="sd">        simulator: A function that takes parameters $\theta$ and maps them to</span>
<span class="sd">            simulations, or observations, `x`, $\mathrm{sim}(\theta)\to x$. Any</span>
<span class="sd">            regular Python callable (i.e. function or class with `__call__` method)</span>
<span class="sd">            can be used.</span>
<span class="sd">        prior: A probability distribution that expresses prior knowledge about the</span>
<span class="sd">            parameters, e.g. which ranges are meaningful for them. Any</span>
<span class="sd">            object with `.log_prob()`and `.sample()` (for example, a PyTorch</span>
<span class="sd">            distribution) can be used.</span>
<span class="sd">        num_workers: Number of parallel workers to use for simulations.</span>
<span class="sd">        simulation_batch_size: Number of parameter sets that the simulator</span>
<span class="sd">            maps to data x at once. If None, we simulate all parameter sets at the</span>
<span class="sd">            same time. If &gt;= 1, the simulator has to process data of shape</span>
<span class="sd">            (simulation_batch_size, parameter_dimension).</span>
<span class="sd">        classifier: Classifier trained to approximate likelihood ratios. If it is</span>
<span class="sd">            a string, use a pre-configured network of the provided type (one of</span>
<span class="sd">            linear, mlp, resnet). Alternatively, a function that builds a custom</span>
<span class="sd">            neural network can be provided. The function will be called with the</span>
<span class="sd">            first batch of simulations (theta, x), which can thus be used for shape</span>
<span class="sd">            inference and potentially for z-scoring. It needs to return a PyTorch</span>
<span class="sd">            `nn.Module` implementing the classifier.</span>
<span class="sd">        mcmc_method: Method used for MCMC sampling, one of `slice_np`, `slice`, `hmc`, `nuts`.</span>
<span class="sd">            Currently defaults to `slice_np` for a custom numpy implementation of</span>
<span class="sd">            slice sampling; select `hmc`, `nuts` or `slice` for Pyro-based sampling.</span>
<span class="sd">        mcmc_parameters: Dictionary overriding the default parameters for MCMC.</span>
<span class="sd">            The following parameters are supported: `thin` to set the thinning</span>
<span class="sd">            factor for the chain, `warmup_steps` to set the initial number of</span>
<span class="sd">            samples to discard, `num_chains` for the number of chains, `init_strategy`</span>
<span class="sd">            for the initialisation strategy for chains; `prior` will draw init</span>
<span class="sd">            locations from prior, whereas `sir` will use Sequential-Importance-</span>
<span class="sd">            Resampling using `init_strategy_num_candidates` to find init</span>
<span class="sd">            locations.</span>
<span class="sd">        device: torch device on which to compute, e.g. gpu, cpu.</span>
<span class="sd">        logging_level: Minimum severity of messages to log. One of the strings</span>
<span class="sd">            INFO, WARNING, DEBUG, ERROR and CRITICAL.</span>
<span class="sd">        summary_writer: A tensorboard `SummaryWriter` to control, among others, log</span>
<span class="sd">            file location (default is `&lt;current working directory&gt;/logs`.)</span>
<span class="sd">        show_progress_bars: Whether to show a progressbar during simulation and</span>
<span class="sd">            sampling.</span>
<span class="sd">        show_round_summary: Whether to show the validation loss and leakage after</span>
<span class="sd">            each round.</span>
<span class="sd">    """</span>

    <span class="n">kwargs</span> <span class="o">=</span> <span class="n">del_entries</span><span class="p">(</span><span class="nb">locals</span><span class="p">(),</span> <span class="n">entries</span><span class="o">=</span><span class="p">(</span><span class="s2">"self"</span><span class="p">,</span> <span class="s2">"__class__"</span><span class="p">))</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre>
</div>
</td>
</tr>
</table>
</details>
</div>
</div>
<div class="doc doc-object doc-method">
<h4 class="doc doc-heading" data-toc-label="provide_presimulated()" id="sbi.inference.snre.snre_b.SNRE_B.provide_presimulated">
<code class="highlight language-python">
provide_presimulated<span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">from_round</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> </code>
<span class="doc doc-properties">
<small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
</span>
</h4>
<div class="doc doc-contents">
<p>Provide external <span><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span> and <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> to be used for training later on.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>theta</code></td>
<td><code>Tensor</code></td>
<td>
<p>Parameter sets used to generate presimulated data.</p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>x</code></td>
<td><code>Tensor</code></td>
<td>
<p>Simulation outputs of presimulated data.</p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>from_round</code></td>
<td><code>int</code></td>
<td>
<p>Which round the data was simulated from. <code>from_round=0</code> means
that the data came from the first round, i.e. the prior.</p>
</td>
<td><code>0</code></td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Source code in <code>sbi/inference/snre/snre_b.py</code></summary>
<table class="highlighttable">
<tr>
<td class="linenos">
<div class="linenodiv">
<pre><span></span>164
165
166
167
168
169
170
171
172
173
174
175
176</pre>
</div>
</td>
<td class="code">
<div class="highlight">
<pre><span></span><code><span class="k">def</span> <span class="nf">provide_presimulated</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">from_round</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">"""</span>
<span class="sd">    Provide external $\theta$ and $x$ to be used for training later on.</span>

<span class="sd">    Args:</span>
<span class="sd">        theta: Parameter sets used to generate presimulated data.</span>
<span class="sd">        x: Simulation outputs of presimulated data.</span>
<span class="sd">        from_round: Which round the data was simulated from. `from_round=0` means</span>
<span class="sd">            that the data came from the first round, i.e. the prior.</span>
<span class="sd">    """</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_append_to_data_bank</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">from_round</span><span class="p">)</span>
</code></pre>
</div>
</td>
</tr>
</table>
</details>
</div>
</div>
</div>
</div>
</div>
<div class="doc doc-object doc-class">
<h3 class="doc doc-heading" id="sbi.inference.abc.mcabc.MCABC">
<code>sbi.inference.abc.mcabc.MCABC</code>
<a class="headerlink" href="#sbi.inference.abc.mcabc.MCABC" title="Permanent link">¶</a></h3>
<div class="doc doc-contents first">
<div class="doc doc-children">
<div class="doc doc-object doc-method">
<h4 class="doc doc-heading" data-toc-label="__call__()" id="sbi.inference.abc.mcabc.MCABC.__call__">
<code class="highlight language-python">
__call__<span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_o</span><span class="p">,</span> <span class="n">num_simulations</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">quantile</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">return_distances</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> </code>
<span class="doc doc-properties">
<small class="doc doc-property doc-property-special"><code>special</code></small>
</span>
</h4>
<div class="doc doc-contents">
<p>Run MCABC.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>x_o</code></td>
<td><code>Union[torch.Tensor, numpy.ndarray]</code></td>
<td>
<p>Observed data.</p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>num_simulations</code></td>
<td><code>int</code></td>
<td>
<p>Number of simulations to run.</p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>eps</code></td>
<td><code>Optional[float]</code></td>
<td>
<p>Acceptance threshold <span><span class="MathJax_Preview">\epsilon</span><script type="math/tex">\epsilon</script></span> for distance between observed and
simulated data.</p>
</td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>quantile</code></td>
<td><code>Optional[float]</code></td>
<td>
<p>Upper quantile of smallest distances for which the corresponding
parameters are returned, e.g, q=0.01 will return the top 1%. Exactly
one of quantile or <code>eps</code> have to be passed.</p>
</td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>return_distances</code></td>
<td><code>bool</code></td>
<td>
<p>Whether to return the distances corresponding to the
selected parameters.</p>
</td>
<td><code>False</code></td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>Union[torch.distributions.distribution.Distribution, Tuple[torch.distributions.distribution.Distribution, torch.Tensor]]</code></td>
<td>
<p>posterior: Empirical distribution based on selected parameters.
distances: Tensor of distances of the selected parameters.</p>
</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Source code in <code>sbi/inference/abc/mcabc.py</code></summary>
<table class="highlighttable">
<tr>
<td class="linenos">
<div class="linenodiv">
<pre><span></span> 59
 60
 61
 62
 63
 64
 65
 66
 67
 68
 69
 70
 71
 72
 73
 74
 75
 76
 77
 78
 79
 80
 81
 82
 83
 84
 85
 86
 87
 88
 89
 90
 91
 92
 93
 94
 95
 96
 97
 98
 99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122</pre>
</div>
</td>
<td class="code">
<div class="highlight">
<pre><span></span><code><span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">x_o</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">ndarray</span><span class="p">],</span>
    <span class="n">num_simulations</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">eps</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">quantile</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">return_distances</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Distribution</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Distribution</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]]:</span>
    <span class="sa">r</span><span class="sd">"""Run MCABC.</span>

<span class="sd">    Args:</span>
<span class="sd">        x_o: Observed data.</span>
<span class="sd">        num_simulations: Number of simulations to run.</span>
<span class="sd">        eps: Acceptance threshold $\epsilon$ for distance between observed and</span>
<span class="sd">            simulated data.</span>
<span class="sd">        quantile: Upper quantile of smallest distances for which the corresponding</span>
<span class="sd">            parameters are returned, e.g, q=0.01 will return the top 1%. Exactly</span>
<span class="sd">            one of quantile or `eps` have to be passed.</span>
<span class="sd">        return_distances: Whether to return the distances corresponding to the</span>
<span class="sd">            selected parameters.</span>
<span class="sd">    Returns:</span>
<span class="sd">        posterior: Empirical distribution based on selected parameters.</span>
<span class="sd">        distances: Tensor of distances of the selected parameters.</span>
<span class="sd">    """</span>
    <span class="c1"># Exactly one of eps or quantile need to be passed.</span>
    <span class="k">assert</span> <span class="p">(</span><span class="n">eps</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span> <span class="o">^</span> <span class="p">(</span>
        <span class="n">quantile</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
    <span class="p">),</span> <span class="s2">"Eps or quantile must be passed, but not both."</span>

    <span class="c1"># Simulate and calculate distances.</span>
    <span class="n">theta</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prior</span><span class="o">.</span><span class="n">sample</span><span class="p">((</span><span class="n">num_simulations</span><span class="p">,))</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_batched_simulator</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>

    <span class="c1"># Infer shape of x to test and set x_o.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">x_shape</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">x_o</span> <span class="o">=</span> <span class="n">process_x</span><span class="p">(</span><span class="n">x_o</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">x_shape</span><span class="p">)</span>

    <span class="n">distances</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">distance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x_o</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>

    <span class="c1"># Select based on acceptance threshold epsilon.</span>
    <span class="k">if</span> <span class="n">eps</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">is_accepted</span> <span class="o">=</span> <span class="n">distances</span> <span class="o">&lt;</span> <span class="n">eps</span>
        <span class="n">num_accepted</span> <span class="o">=</span> <span class="n">is_accepted</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="k">assert</span> <span class="n">num_accepted</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="sa">f</span><span class="s2">"No parameters accepted, eps=</span><span class="si">{</span><span class="n">eps</span><span class="si">}</span><span class="s2"> too small"</span>

        <span class="n">theta_accepted</span> <span class="o">=</span> <span class="n">theta</span><span class="p">[</span><span class="n">is_accepted</span><span class="p">]</span>
        <span class="n">distances_accepted</span> <span class="o">=</span> <span class="n">distances</span><span class="p">[</span><span class="n">is_accepted</span><span class="p">]</span>

    <span class="c1"># Select based on quantile on sorted distances.</span>
    <span class="k">elif</span> <span class="n">quantile</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">num_top_samples</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">num_simulations</span> <span class="o">*</span> <span class="n">quantile</span><span class="p">)</span>
        <span class="n">sort_idx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">distances</span><span class="p">)</span>
        <span class="n">theta_accepted</span> <span class="o">=</span> <span class="n">theta</span><span class="p">[</span><span class="n">sort_idx</span><span class="p">][:</span><span class="n">num_top_samples</span><span class="p">]</span>
        <span class="n">distances_accepted</span> <span class="o">=</span> <span class="n">distances</span><span class="p">[</span><span class="n">sort_idx</span><span class="p">][:</span><span class="n">num_top_samples</span><span class="p">]</span>

    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">"One of epsilon or quantile has to be passed."</span><span class="p">)</span>

    <span class="n">posterior</span> <span class="o">=</span> <span class="n">Empirical</span><span class="p">(</span><span class="n">theta_accepted</span><span class="p">,</span> <span class="n">log_weights</span><span class="o">=</span><span class="n">ones</span><span class="p">(</span><span class="n">theta_accepted</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>

    <span class="k">if</span> <span class="n">return_distances</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">posterior</span><span class="p">,</span> <span class="n">distances_accepted</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">posterior</span>
</code></pre>
</div>
</td>
</tr>
</table>
</details>
</div>
</div>
<div class="doc doc-object doc-method">
<h4 class="doc doc-heading" data-toc-label="__init__()" id="sbi.inference.abc.mcabc.MCABC.__init__">
<code class="highlight language-python">
__init__<span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">simulator</span><span class="p">,</span> <span class="n">prior</span><span class="p">,</span> <span class="n">distance</span><span class="o">=</span><span class="s1">'l2'</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">simulation_batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">show_progress_bars</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> </code>
<span class="doc doc-properties">
<small class="doc doc-property doc-property-special"><code>special</code></small>
</span>
</h4>
<div class="doc doc-contents">
<p>Monte-Carlo Approximate Bayesian Computation (Rejection ABC) [1].</p>
<p>[1] Pritchard, J. K., Seielstad, M. T., Perez-Lezaun, A., &amp; Feldman, M. W.
(1999). Population growth of human Y chromosomes: a study of Y chromosome
microsatellites. Molecular biology and evolution, 16(12), 1791-1798.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>simulator</code></td>
<td><code>Callable</code></td>
<td>
<p>A function that takes parameters $       heta$ and maps them to
simulations, or observations, <code>x</code>, <span><span class="MathJax_Preview">\mathrm{sim}(       heta)   o x</span><script type="math/tex">\mathrm{sim}(       heta)   o x</script></span>. Any
regular Python callable (i.e. function or class with <code>__call__</code> method)
can be used.</p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>prior</code></td>
<td><code></code></td>
<td>
<p>A probability distribution that expresses prior knowledge about the
parameters, e.g. which ranges are meaningful for them. Any
object with <code>.log_prob()</code>and <code>.sample()</code> (for example, a PyTorch
distribution) can be used.</p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>distance</code></td>
<td><code>Union[str, Callable]</code></td>
<td>
<p>Distance function to compare observed and simulated data. Can be
a custom function or one of <code>l1</code>, <code>l2</code>, <code>mse</code>.</p>
</td>
<td><code>'l2'</code></td>
</tr>
<tr>
<td><code>num_workers</code></td>
<td><code>int</code></td>
<td>
<p>Number of parallel workers to use for simulations.</p>
</td>
<td><code>1</code></td>
</tr>
<tr>
<td><code>simulation_batch_size</code></td>
<td><code>int</code></td>
<td>
<p>Number of parameter sets that the simulator
maps to data x at once. If None, we simulate all parameter sets at the
same time. If &gt;= 1, the simulator has to process data of shape
(simulation_batch_size, parameter_dimension).</p>
</td>
<td><code>1</code></td>
</tr>
<tr>
<td><code>show_progress_bars</code></td>
<td><code>bool</code></td>
<td>
<p>Whether to show a progressbar during simulation and
sampling.</p>
</td>
<td><code>True</code></td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Source code in <code>sbi/inference/abc/mcabc.py</code></summary>
<table class="highlighttable">
<tr>
<td class="linenos">
<div class="linenodiv">
<pre><span></span>15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57</pre>
</div>
</td>
<td class="code">
<div class="highlight">
<pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">simulator</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
    <span class="n">prior</span><span class="p">,</span>
    <span class="n">distance</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"l2"</span><span class="p">,</span>
    <span class="n">num_workers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">simulation_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">"""Monte-Carlo Approximate Bayesian Computation (Rejection ABC) [1].</span>

<span class="sd">    [1] Pritchard, J. K., Seielstad, M. T., Perez-Lezaun, A., &amp; Feldman, M. W.</span>
<span class="sd">    (1999). Population growth of human Y chromosomes: a study of Y chromosome</span>
<span class="sd">    microsatellites. Molecular biology and evolution, 16(12), 1791-1798.</span>

<span class="sd">    Args:</span>
<span class="sd">        simulator: A function that takes parameters $\theta$ and maps them to</span>
<span class="sd">            simulations, or observations, `x`, $\mathrm{sim}(\theta)\to x$. Any</span>
<span class="sd">            regular Python callable (i.e. function or class with `__call__` method)</span>
<span class="sd">            can be used.</span>
<span class="sd">        prior: A probability distribution that expresses prior knowledge about the</span>
<span class="sd">            parameters, e.g. which ranges are meaningful for them. Any</span>
<span class="sd">            object with `.log_prob()`and `.sample()` (for example, a PyTorch</span>
<span class="sd">            distribution) can be used.</span>
<span class="sd">        distance: Distance function to compare observed and simulated data. Can be</span>
<span class="sd">            a custom function or one of `l1`, `l2`, `mse`.</span>
<span class="sd">        num_workers: Number of parallel workers to use for simulations.</span>
<span class="sd">        simulation_batch_size: Number of parameter sets that the simulator</span>
<span class="sd">            maps to data x at once. If None, we simulate all parameter sets at the</span>
<span class="sd">            same time. If &gt;= 1, the simulator has to process data of shape</span>
<span class="sd">            (simulation_batch_size, parameter_dimension).</span>
<span class="sd">        show_progress_bars: Whether to show a progressbar during simulation and</span>
<span class="sd">            sampling.</span>
<span class="sd">    """</span>

    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">simulator</span><span class="o">=</span><span class="n">simulator</span><span class="p">,</span>
        <span class="n">prior</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span>
        <span class="n">distance</span><span class="o">=</span><span class="n">distance</span><span class="p">,</span>
        <span class="n">num_workers</span><span class="o">=</span><span class="n">num_workers</span><span class="p">,</span>
        <span class="n">simulation_batch_size</span><span class="o">=</span><span class="n">simulation_batch_size</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="o">=</span><span class="n">show_progress_bars</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre>
</div>
</td>
</tr>
</table>
</details>
</div>
</div>
<div class="doc doc-object doc-method">
<h4 class="doc doc-heading" data-toc-label="choose_distance_function()" id="sbi.inference.abc.mcabc.MCABC.choose_distance_function">
<code class="highlight language-python">
choose_distance_function<span class="p">(</span><span class="n">distance_type</span><span class="o">=</span><span class="s1">'l2'</span><span class="p">)</span> </code>
<span class="doc doc-properties">
<small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
</span>
</h4>
<div class="doc doc-contents">
<p>Return distance function for given distance type.</p>
<details class="quote">
<summary>Source code in <code>sbi/inference/abc/mcabc.py</code></summary>
<table class="highlighttable">
<tr>
<td class="linenos">
<div class="linenodiv">
<pre><span></span>63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90</pre>
</div>
</td>
<td class="code">
<div class="highlight">
<pre><span></span><code><span class="nd">@staticmethod</span>
<span class="k">def</span> <span class="nf">choose_distance_function</span><span class="p">(</span><span class="n">distance_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">"l2"</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Callable</span><span class="p">:</span>
    <span class="sd">"""Return distance function for given distance type."""</span>

    <span class="k">if</span> <span class="n">distance_type</span> <span class="o">==</span> <span class="s2">"mse"</span><span class="p">:</span>
        <span class="n">distance</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">xo</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">xo</span> <span class="o">-</span> <span class="n">x</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">distance_type</span> <span class="o">==</span> <span class="s2">"l2"</span><span class="p">:</span>
        <span class="n">distance</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">xo</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">norm</span><span class="p">((</span><span class="n">xo</span> <span class="o">-</span> <span class="n">x</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">distance_type</span> <span class="o">==</span> <span class="s2">"l1"</span><span class="p">:</span>
        <span class="n">distance</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">xo</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="nb">abs</span><span class="p">(</span><span class="n">xo</span> <span class="o">-</span> <span class="n">x</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">r</span><span class="s2">"Distance </span><span class="si">{distance_type}</span><span class="s2"> not supported."</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">distance_fun</span><span class="p">(</span><span class="n">observed_data</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">simulated_data</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="sd">"""Return distance over batch dimension.</span>

<span class="sd">        Args:</span>
<span class="sd">            observed_data: Observed data, could be 1D.</span>
<span class="sd">            simulated_data: Batch of simulated data, has batch dimension.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Torch tensor with batch of distances.</span>
<span class="sd">        """</span>
        <span class="k">assert</span> <span class="n">simulated_data</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">,</span> <span class="s2">"simulated data needs batch dimension"</span>

        <span class="k">return</span> <span class="n">distance</span><span class="p">(</span><span class="n">observed_data</span><span class="p">,</span> <span class="n">simulated_data</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">distance_fun</span>
</code></pre>
</div>
</td>
</tr>
</table>
</details>
</div>
</div>
</div>
</div>
</div>
<div class="doc doc-object doc-class">
<h3 class="doc doc-heading" id="sbi.inference.abc.smcabc.SMCABC">
<code>sbi.inference.abc.smcabc.SMCABC</code>
<a class="headerlink" href="#sbi.inference.abc.smcabc.SMCABC" title="Permanent link">¶</a></h3>
<div class="doc doc-contents first">
<div class="doc doc-children">
<div class="doc doc-object doc-method">
<h4 class="doc doc-heading" data-toc-label="__call__()" id="sbi.inference.abc.smcabc.SMCABC.__call__">
<code class="highlight language-python">
__call__<span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_o</span><span class="p">,</span> <span class="n">num_particles</span><span class="p">,</span> <span class="n">num_initial_pop</span><span class="p">,</span> <span class="n">num_simulations</span><span class="p">,</span> <span class="n">epsilon_decay</span><span class="p">,</span> <span class="n">distance_based_decay</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ess_min</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">kernel_variance_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">use_last_pop_samples</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_summary</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> </code>
<span class="doc doc-properties">
<small class="doc doc-property doc-property-special"><code>special</code></small>
</span>
</h4>
<div class="doc doc-contents">
<p>Run SMCABC.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>x_o</code></td>
<td><code>Union[torch.Tensor, numpy.ndarray]</code></td>
<td>
<p>Observed data.</p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>num_particles</code></td>
<td><code>int</code></td>
<td>
<p>Number of particles in each population.</p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>num_initial_pop</code></td>
<td><code>int</code></td>
<td>
<p>Number of simulations used for initial population.</p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>num_simulations</code></td>
<td><code>int</code></td>
<td>
<p>Total number of possible simulations.</p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>epsilon_decay</code></td>
<td><code>float</code></td>
<td>
<p>Factor with which the acceptance threshold <span><span class="MathJax_Preview">\epsilon</span><script type="math/tex">\epsilon</script></span> decays.</p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>distance_based_decay</code></td>
<td><code>bool</code></td>
<td>
<p>Whether the <span><span class="MathJax_Preview">\epsilon</span><script type="math/tex">\epsilon</script></span> decay is constant over
populations or calculated from the previous populations distribution of
distances.</p>
</td>
<td><code>False</code></td>
</tr>
<tr>
<td><code>ess_min</code></td>
<td><code>float</code></td>
<td>
<p>Threshold of effective sampling size for resampling weights.</p>
</td>
<td><code>0.5</code></td>
</tr>
<tr>
<td><code>kernel_variance_scale</code></td>
<td><code>float</code></td>
<td>
<p>Factor for scaling the perturbation kernel variance.</p>
</td>
<td><code>1.0</code></td>
</tr>
<tr>
<td><code>use_last_pop_samples</code></td>
<td><code>bool</code></td>
<td>
<p>Whether to fill up the current population with
samples from the previous population when the budget is used up. If
False, the current population is discarded and the previous population
is returned.</p>
</td>
<td><code>True</code></td>
</tr>
<tr>
<td><code>return_summary</code></td>
<td><code>bool</code></td>
<td>
<p>Whether to return a dictionary with all accepted particles, 
weights, etc. at the end.</p>
</td>
<td><code>False</code></td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>Union[torch.distributions.distribution.Distribution, Tuple[torch.distributions.distribution.Distribution, dict]]</code></td>
<td>
<p>posterior: Empirical posterior distribution defined by the accepted
    particles and their weights.
summary (optional): A dictionary containing particles, weights, epsilons
    and distances of each population.</p>
</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Source code in <code>sbi/inference/abc/smcabc.py</code></summary>
<table class="highlighttable">
<tr>
<td class="linenos">
<div class="linenodiv">
<pre><span></span> 99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219</pre>
</div>
</td>
<td class="code">
<div class="highlight">
<pre><span></span><code><span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">x_o</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">ndarray</span><span class="p">],</span>
    <span class="n">num_particles</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">num_initial_pop</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">num_simulations</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">epsilon_decay</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
    <span class="n">distance_based_decay</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">ess_min</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">,</span>
    <span class="n">kernel_variance_scale</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
    <span class="n">use_last_pop_samples</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">return_summary</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Distribution</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Distribution</span><span class="p">,</span> <span class="nb">dict</span><span class="p">]]:</span>
    <span class="sa">r</span><span class="sd">"""Run SMCABC.</span>

<span class="sd">    Args:</span>
<span class="sd">        x_o: Observed data.</span>
<span class="sd">        num_particles: Number of particles in each population.</span>
<span class="sd">        num_initial_pop: Number of simulations used for initial population.</span>
<span class="sd">        num_simulations: Total number of possible simulations.</span>
<span class="sd">        epsilon_decay: Factor with which the acceptance threshold $\epsilon$ decays.</span>
<span class="sd">        distance_based_decay: Whether the $\epsilon$ decay is constant over</span>
<span class="sd">            populations or calculated from the previous populations distribution of</span>
<span class="sd">            distances.</span>
<span class="sd">        ess_min: Threshold of effective sampling size for resampling weights.</span>
<span class="sd">        kernel_variance_scale: Factor for scaling the perturbation kernel variance.</span>
<span class="sd">        use_last_pop_samples: Whether to fill up the current population with</span>
<span class="sd">            samples from the previous population when the budget is used up. If</span>
<span class="sd">            False, the current population is discarded and the previous population</span>
<span class="sd">            is returned.</span>
<span class="sd">        return_summary: Whether to return a dictionary with all accepted particles, </span>
<span class="sd">            weights, etc. at the end.</span>

<span class="sd">    Returns:</span>
<span class="sd">        posterior: Empirical posterior distribution defined by the accepted</span>
<span class="sd">            particles and their weights.</span>
<span class="sd">        summary (optional): A dictionary containing particles, weights, epsilons</span>
<span class="sd">            and distances of each population.</span>
<span class="sd">    """</span>

    <span class="n">pop_idx</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">num_simulations</span> <span class="o">=</span> <span class="n">num_simulations</span>

    <span class="c1"># run initial population</span>
    <span class="n">particles</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="n">distances</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set_xo_and_sample_initial_population</span><span class="p">(</span>
        <span class="n">x_o</span><span class="p">,</span> <span class="n">num_particles</span><span class="p">,</span> <span class="n">num_initial_pop</span>
    <span class="p">)</span>
    <span class="n">log_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">num_particles</span> <span class="o">*</span> <span class="n">ones</span><span class="p">(</span><span class="n">num_particles</span><span class="p">))</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
        <span class="p">(</span>
            <span class="sa">f</span><span class="s2">"population=</span><span class="si">{</span><span class="n">pop_idx</span><span class="si">}</span><span class="s2">, eps=</span><span class="si">{</span><span class="n">epsilon</span><span class="si">}</span><span class="s2">, ess=</span><span class="si">{</span><span class="mf">1.0</span><span class="si">}</span><span class="s2">, "</span>
            <span class="sa">f</span><span class="s2">"num_sims=</span><span class="si">{</span><span class="n">num_initial_pop</span><span class="si">}</span><span class="s2">"</span>
        <span class="p">)</span>
    <span class="p">)</span>

    <span class="n">all_particles</span> <span class="o">=</span> <span class="p">[</span><span class="n">particles</span><span class="p">]</span>
    <span class="n">all_log_weights</span> <span class="o">=</span> <span class="p">[</span><span class="n">log_weights</span><span class="p">]</span>
    <span class="n">all_distances</span> <span class="o">=</span> <span class="p">[</span><span class="n">distances</span><span class="p">]</span>
    <span class="n">all_epsilons</span> <span class="o">=</span> <span class="p">[</span><span class="n">epsilon</span><span class="p">]</span>

    <span class="k">while</span> <span class="bp">self</span><span class="o">.</span><span class="n">simulation_counter</span> <span class="o">&lt;</span> <span class="n">num_simulations</span><span class="p">:</span>

        <span class="n">pop_idx</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="c1"># Decay based on quantile of distances from previous pop.</span>
        <span class="k">if</span> <span class="n">distance_based_decay</span><span class="p">:</span>
            <span class="n">epsilon</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_next_epsilon</span><span class="p">(</span>
                <span class="n">all_distances</span><span class="p">[</span><span class="n">pop_idx</span> <span class="o">-</span> <span class="mi">1</span><span class="p">],</span> <span class="n">epsilon_decay</span>
            <span class="p">)</span>
        <span class="c1"># Constant decay.</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">epsilon</span> <span class="o">*=</span> <span class="n">epsilon_decay</span>

        <span class="c1"># Get kernel variance from previous pop.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kernel_variance</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_kernel_variance</span><span class="p">(</span>
            <span class="n">all_particles</span><span class="p">[</span><span class="n">pop_idx</span> <span class="o">-</span> <span class="mi">1</span><span class="p">],</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">all_log_weights</span><span class="p">[</span><span class="n">pop_idx</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]),</span>
            <span class="n">num_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
            <span class="n">kernel_variance_scale</span><span class="o">=</span><span class="n">kernel_variance_scale</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">particles</span><span class="p">,</span> <span class="n">log_weights</span><span class="p">,</span> <span class="n">distances</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sample_next_population</span><span class="p">(</span>
            <span class="n">particles</span><span class="o">=</span><span class="n">all_particles</span><span class="p">[</span><span class="n">pop_idx</span> <span class="o">-</span> <span class="mi">1</span><span class="p">],</span>
            <span class="n">log_weights</span><span class="o">=</span><span class="n">all_log_weights</span><span class="p">[</span><span class="n">pop_idx</span> <span class="o">-</span> <span class="mi">1</span><span class="p">],</span>
            <span class="n">distances</span><span class="o">=</span><span class="n">all_distances</span><span class="p">[</span><span class="n">pop_idx</span> <span class="o">-</span> <span class="mi">1</span><span class="p">],</span>
            <span class="n">epsilon</span><span class="o">=</span><span class="n">epsilon</span><span class="p">,</span>
            <span class="n">use_last_pop_samples</span><span class="o">=</span><span class="n">use_last_pop_samples</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># Resample population if effective sampling size is too small.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">algorithm_variant</span> <span class="o">==</span> <span class="s2">"B"</span><span class="p">:</span>
            <span class="n">particles</span><span class="p">,</span> <span class="n">log_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">resample_if_ess_too_small</span><span class="p">(</span>
                <span class="n">particles</span><span class="p">,</span> <span class="n">log_weights</span><span class="p">,</span> <span class="n">num_particles</span><span class="p">,</span> <span class="n">ess_min</span><span class="p">,</span> <span class="n">pop_idx</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="p">(</span>
                <span class="sa">f</span><span class="s2">"population=</span><span class="si">{</span><span class="n">pop_idx</span><span class="si">}</span><span class="s2"> done: eps=</span><span class="si">{</span><span class="n">epsilon</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">,"</span>
                <span class="sa">f</span><span class="s2">" num_sims=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">simulation_counter</span><span class="si">}</span><span class="s2">."</span>
            <span class="p">)</span>
        <span class="p">)</span>

        <span class="c1"># collect results</span>
        <span class="n">all_particles</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">particles</span><span class="p">)</span>
        <span class="n">all_log_weights</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">log_weights</span><span class="p">)</span>
        <span class="n">all_distances</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">distances</span><span class="p">)</span>
        <span class="n">all_epsilons</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">epsilon</span><span class="p">)</span>

    <span class="n">posterior</span> <span class="o">=</span> <span class="n">Empirical</span><span class="p">(</span><span class="n">all_particles</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">log_weights</span><span class="o">=</span><span class="n">all_log_weights</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

    <span class="k">if</span> <span class="n">return_summary</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="n">posterior</span><span class="p">,</span>
            <span class="nb">dict</span><span class="p">(</span>
                <span class="n">particles</span><span class="o">=</span><span class="n">all_particles</span><span class="p">,</span>
                <span class="n">weights</span><span class="o">=</span><span class="n">all_log_weights</span><span class="p">,</span>
                <span class="n">epsilons</span><span class="o">=</span><span class="n">all_epsilons</span><span class="p">,</span>
                <span class="n">distances</span><span class="o">=</span><span class="n">all_distances</span><span class="p">,</span>
            <span class="p">),</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">posterior</span>
</code></pre>
</div>
</td>
</tr>
</table>
</details>
</div>
</div>
<div class="doc doc-object doc-method">
<h4 class="doc doc-heading" data-toc-label="__init__()" id="sbi.inference.abc.smcabc.SMCABC.__init__">
<code class="highlight language-python">
__init__<span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">simulator</span><span class="p">,</span> <span class="n">prior</span><span class="p">,</span> <span class="n">distance</span><span class="o">=</span><span class="s1">'l2'</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">simulation_batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">show_progress_bars</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">kernel</span><span class="o">=</span><span class="s1">'gaussian'</span><span class="p">,</span> <span class="n">algorithm_variant</span><span class="o">=</span><span class="s1">'C'</span><span class="p">)</span> </code>
<span class="doc doc-properties">
<small class="doc doc-property doc-property-special"><code>special</code></small>
</span>
</h4>
<div class="doc doc-contents">
<p>Sequential Monte Carlo Approximate Bayesian Computation.</p>
<p>We distinguish between three different SMC methods here:
    - A: Toni et al. 2010 (Phd Thesis)
    - B: Sisson et al. 2007 (with correction from 2009)
    - C: Beaumont et al. 2009</p>
<p>In Toni et al. 2010 we find an overview of the differences on page 34:
    - B: same as A except for resampling of weights if the effective sampling
        size is too small.
    - C: same as A except for calculation of the covariance of the perturbation
        kernel: the kernel covariance is a scaled version of the covariance of
        the previous population.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>simulator</code></td>
<td><code>Callable</code></td>
<td>
<p>A function that takes parameters <span><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span> and maps them to
simulations, or observations, <code>x</code>, <span><span class="MathJax_Preview">\mathrm{sim}(\theta)\to x</span><script type="math/tex">\mathrm{sim}(\theta)\to x</script></span>. Any
regular Python callable (i.e. function or class with <code>__call__</code> method)
can be used.</p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>prior</code></td>
<td><code>Distribution</code></td>
<td>
<p>A probability distribution that expresses prior knowledge about the
parameters, e.g. which ranges are meaningful for them. Any
object with <code>.log_prob()</code>and <code>.sample()</code> (for example, a PyTorch
distribution) can be used.</p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>distance</code></td>
<td><code>Union[str, Callable]</code></td>
<td>
<p>Distance function to compare observed and simulated data. Can be
a custom function or one of <code>l1</code>, <code>l2</code>, <code>mse</code>.</p>
</td>
<td><code>'l2'</code></td>
</tr>
<tr>
<td><code>num_workers</code></td>
<td><code>int</code></td>
<td>
<p>Number of parallel workers to use for simulations.</p>
</td>
<td><code>1</code></td>
</tr>
<tr>
<td><code>simulation_batch_size</code></td>
<td><code>int</code></td>
<td>
<p>Number of parameter sets that the simulator
maps to data x at once. If None, we simulate all parameter sets at the
same time. If &gt;= 1, the simulator has to process data of shape
(simulation_batch_size, parameter_dimension).</p>
</td>
<td><code>1</code></td>
</tr>
<tr>
<td><code>show_progress_bars</code></td>
<td><code>bool</code></td>
<td>
<p>Whether to show a progressbar during simulation and
sampling.</p>
</td>
<td><code>True</code></td>
</tr>
<tr>
<td><code>kernel</code></td>
<td><code>Optional[str]</code></td>
<td>
<p>Perturbation kernel.</p>
</td>
<td><code>'gaussian'</code></td>
</tr>
<tr>
<td><code>algorithm_variant</code></td>
<td><code>str</code></td>
<td>
<p>Indicating the choice of algorithm variant, A, B, or C.</p>
</td>
<td><code>'C'</code></td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Source code in <code>sbi/inference/abc/smcabc.py</code></summary>
<table class="highlighttable">
<tr>
<td class="linenos">
<div class="linenodiv">
<pre><span></span>17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97</pre>
</div>
</td>
<td class="code">
<div class="highlight">
<pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">simulator</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
    <span class="n">prior</span><span class="p">:</span> <span class="n">Distribution</span><span class="p">,</span>
    <span class="n">distance</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"l2"</span><span class="p">,</span>
    <span class="n">num_workers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">simulation_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">kernel</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"gaussian"</span><span class="p">,</span>
    <span class="n">algorithm_variant</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">"C"</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sa">r</span><span class="sd">"""Sequential Monte Carlo Approximate Bayesian Computation.</span>

<span class="sd">    We distinguish between three different SMC methods here:</span>
<span class="sd">        - A: Toni et al. 2010 (Phd Thesis)</span>
<span class="sd">        - B: Sisson et al. 2007 (with correction from 2009)</span>
<span class="sd">        - C: Beaumont et al. 2009</span>

<span class="sd">    In Toni et al. 2010 we find an overview of the differences on page 34:</span>
<span class="sd">        - B: same as A except for resampling of weights if the effective sampling</span>
<span class="sd">            size is too small.</span>
<span class="sd">        - C: same as A except for calculation of the covariance of the perturbation</span>
<span class="sd">            kernel: the kernel covariance is a scaled version of the covariance of</span>
<span class="sd">            the previous population.</span>

<span class="sd">    Args:</span>
<span class="sd">        simulator: A function that takes parameters $\theta$ and maps them to</span>
<span class="sd">            simulations, or observations, `x`, $\mathrm{sim}(\theta)\to x$. Any</span>
<span class="sd">            regular Python callable (i.e. function or class with `__call__` method)</span>
<span class="sd">            can be used.</span>
<span class="sd">        prior: A probability distribution that expresses prior knowledge about the</span>
<span class="sd">            parameters, e.g. which ranges are meaningful for them. Any</span>
<span class="sd">            object with `.log_prob()`and `.sample()` (for example, a PyTorch</span>
<span class="sd">            distribution) can be used.</span>
<span class="sd">        distance: Distance function to compare observed and simulated data. Can be</span>
<span class="sd">            a custom function or one of `l1`, `l2`, `mse`.</span>
<span class="sd">        num_workers: Number of parallel workers to use for simulations.</span>
<span class="sd">        simulation_batch_size: Number of parameter sets that the simulator</span>
<span class="sd">            maps to data x at once. If None, we simulate all parameter sets at the</span>
<span class="sd">            same time. If &gt;= 1, the simulator has to process data of shape</span>
<span class="sd">            (simulation_batch_size, parameter_dimension).</span>
<span class="sd">        show_progress_bars: Whether to show a progressbar during simulation and</span>
<span class="sd">            sampling.</span>
<span class="sd">        kernel: Perturbation kernel.</span>
<span class="sd">        algorithm_variant: Indicating the choice of algorithm variant, A, B, or C.</span>

<span class="sd">    """</span>

    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">simulator</span><span class="o">=</span><span class="n">simulator</span><span class="p">,</span>
        <span class="n">prior</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span>
        <span class="n">distance</span><span class="o">=</span><span class="n">distance</span><span class="p">,</span>
        <span class="n">num_workers</span><span class="o">=</span><span class="n">num_workers</span><span class="p">,</span>
        <span class="n">simulation_batch_size</span><span class="o">=</span><span class="n">simulation_batch_size</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="o">=</span><span class="n">show_progress_bars</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">kernels</span> <span class="o">=</span> <span class="p">(</span><span class="s2">"gaussian"</span><span class="p">,</span> <span class="s2">"uniform"</span><span class="p">)</span>
    <span class="k">assert</span> <span class="p">(</span>
        <span class="n">kernel</span> <span class="ow">in</span> <span class="n">kernels</span>
    <span class="p">),</span> <span class="sa">f</span><span class="s2">"Kernel '</span><span class="si">{</span><span class="n">kernel</span><span class="si">}</span><span class="s2">' not supported. Choose one from </span><span class="si">{</span><span class="n">kernels</span><span class="si">}</span><span class="s2">."</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span> <span class="o">=</span> <span class="n">kernel</span>

    <span class="n">algorithm_variants</span> <span class="o">=</span> <span class="p">(</span><span class="s2">"A"</span><span class="p">,</span> <span class="s2">"B"</span><span class="p">,</span> <span class="s2">"C"</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">algorithm_variant</span> <span class="ow">in</span> <span class="n">algorithm_variants</span><span class="p">,</span> <span class="p">(</span>
        <span class="sa">f</span><span class="s2">"SMCABC variant '</span><span class="si">{</span><span class="n">algorithm_variant</span><span class="si">}</span><span class="s2">' not supported, choose one from"</span>
        <span class="s2">" </span><span class="si">{algorithm_variants}</span><span class="s2">."</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">algorithm_variant</span> <span class="o">=</span> <span class="n">algorithm_variant</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">distance_to_x0</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">simulation_counter</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">num_simulations</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>

    <span class="c1"># Define simulator that keeps track of budget.</span>
    <span class="k">def</span> <span class="nf">simulate_with_budget</span><span class="p">(</span><span class="n">theta</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">simulation_counter</span> <span class="o">+=</span> <span class="n">theta</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_batched_simulator</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_simulate_with_budget</span> <span class="o">=</span> <span class="n">simulate_with_budget</span>
</code></pre>
</div>
</td>
</tr>
</table>
</details>
</div>
</div>
<div class="doc doc-object doc-method">
<h4 class="doc doc-heading" data-toc-label="choose_distance_function()" id="sbi.inference.abc.smcabc.SMCABC.choose_distance_function">
<code class="highlight language-python">
choose_distance_function<span class="p">(</span><span class="n">distance_type</span><span class="o">=</span><span class="s1">'l2'</span><span class="p">)</span> </code>
<span class="doc doc-properties">
<small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
</span>
</h4>
<div class="doc doc-contents">
<p>Return distance function for given distance type.</p>
<details class="quote">
<summary>Source code in <code>sbi/inference/abc/smcabc.py</code></summary>
<table class="highlighttable">
<tr>
<td class="linenos">
<div class="linenodiv">
<pre><span></span>63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90</pre>
</div>
</td>
<td class="code">
<div class="highlight">
<pre><span></span><code><span class="nd">@staticmethod</span>
<span class="k">def</span> <span class="nf">choose_distance_function</span><span class="p">(</span><span class="n">distance_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">"l2"</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Callable</span><span class="p">:</span>
    <span class="sd">"""Return distance function for given distance type."""</span>

    <span class="k">if</span> <span class="n">distance_type</span> <span class="o">==</span> <span class="s2">"mse"</span><span class="p">:</span>
        <span class="n">distance</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">xo</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">xo</span> <span class="o">-</span> <span class="n">x</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">distance_type</span> <span class="o">==</span> <span class="s2">"l2"</span><span class="p">:</span>
        <span class="n">distance</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">xo</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">norm</span><span class="p">((</span><span class="n">xo</span> <span class="o">-</span> <span class="n">x</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">distance_type</span> <span class="o">==</span> <span class="s2">"l1"</span><span class="p">:</span>
        <span class="n">distance</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">xo</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="nb">abs</span><span class="p">(</span><span class="n">xo</span> <span class="o">-</span> <span class="n">x</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">r</span><span class="s2">"Distance </span><span class="si">{distance_type}</span><span class="s2"> not supported."</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">distance_fun</span><span class="p">(</span><span class="n">observed_data</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">simulated_data</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="sd">"""Return distance over batch dimension.</span>

<span class="sd">        Args:</span>
<span class="sd">            observed_data: Observed data, could be 1D.</span>
<span class="sd">            simulated_data: Batch of simulated data, has batch dimension.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Torch tensor with batch of distances.</span>
<span class="sd">        """</span>
        <span class="k">assert</span> <span class="n">simulated_data</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">,</span> <span class="s2">"simulated data needs batch dimension"</span>

        <span class="k">return</span> <span class="n">distance</span><span class="p">(</span><span class="n">observed_data</span><span class="p">,</span> <span class="n">simulated_data</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">distance_fun</span>
</code></pre>
</div>
</td>
</tr>
</table>
</details>
</div>
</div>
<div class="doc doc-object doc-method">
<h4 class="doc doc-heading" data-toc-label="get_new_kernel()" id="sbi.inference.abc.smcabc.SMCABC.get_new_kernel">
<code class="highlight language-python">
get_new_kernel<span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">thetas</span><span class="p">)</span> </code>
</h4>
<div class="doc doc-contents">
<p>Return new kernel distribution for a given set of paramters.</p>
<details class="quote">
<summary>Source code in <code>sbi/inference/abc/smcabc.py</code></summary>
<table class="highlighttable">
<tr>
<td class="linenos">
<div class="linenodiv">
<pre><span></span>480
481
482
483
484
485
486
487
488
489
490
491
492
493
494
495
496</pre>
</div>
</td>
<td class="code">
<div class="highlight">
<pre><span></span><code><span class="k">def</span> <span class="nf">get_new_kernel</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">thetas</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Distribution</span><span class="p">:</span>
    <span class="sd">"""Return new kernel distribution for a given set of paramters."""</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span> <span class="o">==</span> <span class="s2">"gaussian"</span><span class="p">:</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_variance</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span>
        <span class="k">return</span> <span class="n">MultivariateNormal</span><span class="p">(</span>
            <span class="n">loc</span><span class="o">=</span><span class="n">thetas</span><span class="p">,</span> <span class="n">covariance_matrix</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_variance</span>
        <span class="p">)</span>

    <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span> <span class="o">==</span> <span class="s2">"uniform"</span><span class="p">:</span>
        <span class="n">low</span> <span class="o">=</span> <span class="n">thetas</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_variance</span>
        <span class="n">high</span> <span class="o">=</span> <span class="n">thetas</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_variance</span>
        <span class="c1"># Move batch shape to event shape to get Uniform that is multivariate in</span>
        <span class="c1"># parameter dimension.</span>
        <span class="k">return</span> <span class="n">Uniform</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="n">low</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="n">high</span><span class="p">)</span><span class="o">.</span><span class="n">to_event</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Kernel, '</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="si">}</span><span class="s2">' not supported."</span><span class="p">)</span>
</code></pre>
</div>
</td>
</tr>
</table>
</details>
</div>
</div>
<div class="doc doc-object doc-method">
<h4 class="doc doc-heading" data-toc-label="resample_if_ess_too_small()" id="sbi.inference.abc.smcabc.SMCABC.resample_if_ess_too_small">
<code class="highlight language-python">
resample_if_ess_too_small<span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">particles</span><span class="p">,</span> <span class="n">log_weights</span><span class="p">,</span> <span class="n">num_particles</span><span class="p">,</span> <span class="n">ess_min</span><span class="p">,</span> <span class="n">pop_idx</span><span class="p">)</span> </code>
</h4>
<div class="doc doc-contents">
<p>Return resampled particles and uniform weights if effectice sampling size is
too small.</p>
<details class="quote">
<summary>Source code in <code>sbi/inference/abc/smcabc.py</code></summary>
<table class="highlighttable">
<tr>
<td class="linenos">
<div class="linenodiv">
<pre><span></span>498
499
500
501
502
503
504
505
506
507
508
509
510
511
512
513
514
515
516
517
518
519
520</pre>
</div>
</td>
<td class="code">
<div class="highlight">
<pre><span></span><code><span class="k">def</span> <span class="nf">resample_if_ess_too_small</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">particles</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">log_weights</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">num_particles</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">ess_min</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
    <span class="n">pop_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
    <span class="sd">"""Return resampled particles and uniform weights if effectice sampling size is</span>
<span class="sd">    too small.</span>
<span class="sd">    """</span>

    <span class="n">ess</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="mf">2.0</span> <span class="o">*</span> <span class="n">log_weights</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span> <span class="o">/</span> <span class="n">num_particles</span>
    <span class="c1"># Resampling of weights for low ESS only for Sisson et al. 2007.</span>
    <span class="k">if</span> <span class="n">ess</span> <span class="o">&lt;</span> <span class="n">ess_min</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">"ESS=</span><span class="si">{</span><span class="n">ess</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> too low, resampling pop </span><span class="si">{</span><span class="n">pop_idx</span><span class="si">}</span><span class="s2">..."</span><span class="p">)</span>
        <span class="c1"># First resample, then set to uniform weights in in Sisson et al. 2007.</span>
        <span class="n">particles</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sample_from_population_with_weights</span><span class="p">(</span>
            <span class="n">particles</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_weights</span><span class="p">),</span> <span class="n">num_samples</span><span class="o">=</span><span class="n">num_particles</span>
        <span class="p">)</span>
        <span class="n">log_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">num_particles</span> <span class="o">*</span> <span class="n">ones</span><span class="p">(</span><span class="n">num_particles</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">particles</span><span class="p">,</span> <span class="n">log_weights</span>
</code></pre>
</div>
</td>
</tr>
</table>
</details>
</div>
</div>
<div class="doc doc-object doc-method">
<h4 class="doc doc-heading" data-toc-label="sample_from_population_with_weights()" id="sbi.inference.abc.smcabc.SMCABC.sample_from_population_with_weights">
<code class="highlight language-python">
sample_from_population_with_weights<span class="p">(</span><span class="n">particles</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> </code>
<span class="doc doc-properties">
<small class="doc doc-property doc-property-staticmethod"><code>staticmethod</code></small>
</span>
</h4>
<div class="doc doc-contents">
<p>Return samples from particles sampled with weights.</p>
<details class="quote">
<summary>Source code in <code>sbi/inference/abc/smcabc.py</code></summary>
<table class="highlighttable">
<tr>
<td class="linenos">
<div class="linenodiv">
<pre><span></span>392
393
394
395
396
397
398
399
400
401
402
403
404
405</pre>
</div>
</td>
<td class="code">
<div class="highlight">
<pre><span></span><code><span class="nd">@staticmethod</span>
<span class="k">def</span> <span class="nf">sample_from_population_with_weights</span><span class="p">(</span>
    <span class="n">particles</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">weights</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">num_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
    <span class="sd">"""Return samples from particles sampled with weights."""</span>

    <span class="c1"># define multinomial with weights as probs</span>
    <span class="n">multi</span> <span class="o">=</span> <span class="n">Multinomial</span><span class="p">(</span><span class="n">probs</span><span class="o">=</span><span class="n">weights</span><span class="p">)</span>
    <span class="c1"># sample num samples, with replacement</span>
    <span class="n">samples</span> <span class="o">=</span> <span class="n">multi</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">sample_shape</span><span class="o">=</span><span class="p">(</span><span class="n">num_samples</span><span class="p">,))</span>
    <span class="c1"># get indices of success trials</span>
    <span class="n">indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">samples</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
    <span class="c1"># return those indices from trace</span>
    <span class="k">return</span> <span class="n">particles</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span>
</code></pre>
</div>
</td>
</tr>
</table>
</details>
</div>
</div>
</div>
</div>
</div>
<h2 id="posteriors">Posteriors<a class="headerlink" href="#posteriors" title="Permanent link">¶</a></h2>
<div class="doc doc-object doc-class">
<h3 class="doc doc-heading" id="sbi.inference.posteriors.direct_posterior.DirectPosterior">
<code>sbi.inference.posteriors.direct_posterior.DirectPosterior</code>
<a class="headerlink" href="#sbi.inference.posteriors.direct_posterior.DirectPosterior" title="Permanent link">¶</a></h3>
<div class="doc doc-contents first">
<p>Posterior <span><span class="MathJax_Preview">p(\theta|x)</span><script type="math/tex">p(\theta|x)</script></span> with <code>log_prob()</code> and <code>sample()</code> methods, obtained with
    SNPE.<br/>
<br/>

    SNPE trains a neural network to directly approximate the posterior distribution.
    However, for bounded priors, the neural network can have leakage: it puts non-zero
    mass in regions where the prior is zero. The <code>SnpePosterior</code> class wraps the trained
    network to deal with these cases.<br/>
<br/>

    Specifically, this class offers the following functionality:<br/>

    - correct the calculation of the log probability such that it compensates for the
      leakage.<br/>

    - reject samples that lie outside of the prior bounds.<br/>

    - alternatively, if leakage is very high (which can happen for multi-round SNPE),
      sample from the posterior with MCMC.<br/>
<br/>

    The neural network itself can be accessed via the <code>.net</code> attribute.</p>
<div class="doc doc-children">
<div class="doc doc-object doc-attribute">
<h4 class="doc doc-heading" data-toc-label="default_x" id="sbi.inference.posteriors.direct_posterior.DirectPosterior.default_x">
<code class="highlight">
default_x: <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> </code>
<span class="doc doc-properties">
<small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
<small class="doc doc-property doc-property-property"><code>property</code></small>
<small class="doc doc-property doc-property-writable"><code>writable</code></small>
</span>
</h4>
<div class="doc doc-contents">
<p>Return default x used by <code>.sample(), .log_prob</code> as conditioning context.</p>
</div>
</div>
<div class="doc doc-object doc-attribute">
<h4 class="doc doc-heading" data-toc-label="mcmc_method" id="sbi.inference.posteriors.direct_posterior.DirectPosterior.mcmc_method">
<code class="highlight">
mcmc_method: <span class="nb">str</span> </code>
<span class="doc doc-properties">
<small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
<small class="doc doc-property doc-property-property"><code>property</code></small>
<small class="doc doc-property doc-property-writable"><code>writable</code></small>
</span>
</h4>
<div class="doc doc-contents">
<p>Returns MCMC method.</p>
</div>
</div>
<div class="doc doc-object doc-attribute">
<h4 class="doc doc-heading" data-toc-label="mcmc_parameters" id="sbi.inference.posteriors.direct_posterior.DirectPosterior.mcmc_parameters">
<code class="highlight">
mcmc_parameters: <span class="nb">dict</span> </code>
<span class="doc doc-properties">
<small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
<small class="doc doc-property doc-property-property"><code>property</code></small>
<small class="doc doc-property doc-property-writable"><code>writable</code></small>
</span>
</h4>
<div class="doc doc-contents">
<p>Returns MCMC parameters.</p>
</div>
</div>
<div class="doc doc-object doc-attribute">
<h4 class="doc doc-heading" data-toc-label="sample_with_mcmc" id="sbi.inference.posteriors.direct_posterior.DirectPosterior.sample_with_mcmc">
<code class="highlight">
sample_with_mcmc: <span class="nb">bool</span> </code>
<span class="doc doc-properties">
<small class="doc doc-property doc-property-property"><code>property</code></small>
<small class="doc doc-property doc-property-writable"><code>writable</code></small>
</span>
</h4>
<div class="doc doc-contents">
<p>Return <code>True</code> if NeuralPosterior instance should use MCMC in <code>.sample()</code>.</p>
</div>
</div>
<div class="doc doc-object doc-method">
<h4 class="doc doc-heading" data-toc-label="__init__()" id="sbi.inference.posteriors.direct_posterior.DirectPosterior.__init__">
<code class="highlight language-python">
__init__<span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">method_family</span><span class="p">,</span> <span class="n">neural_net</span><span class="p">,</span> <span class="n">prior</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">,</span> <span class="n">sample_with_mcmc</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">mcmc_method</span><span class="o">=</span><span class="s1">'slice_np'</span><span class="p">,</span> <span class="n">mcmc_parameters</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">get_potential_function</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span> </code>
<span class="doc doc-properties">
<small class="doc doc-property doc-property-special"><code>special</code></small>
</span>
</h4>
<div class="doc doc-contents">
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>method_family</code></td>
<td><code>str</code></td>
<td>
<p>One of snpe, snl, snre_a or snre_b.</p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>neural_net</code></td>
<td><code>Module</code></td>
<td>
<p>A classifier for SNRE, a density estimator for SNPE and SNL.</p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>prior</code></td>
<td><code></code></td>
<td>
<p>Prior distribution with <code>.log_prob()</code> and <code>.sample()</code>.</p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>x_shape</code></td>
<td><code>Size</code></td>
<td>
<p>Shape of a single simulator output.</p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>sample_with_mcmc</code></td>
<td><code>bool</code></td>
<td>
<p>Whether to sample with MCMC. Will always be <code>True</code> for SRE
and SNL, but can also be set to <code>True</code> for SNPE if MCMC is preferred to
deal with leakage over rejection sampling.</p>
</td>
<td><code>True</code></td>
</tr>
<tr>
<td><code>mcmc_method</code></td>
<td><code>str</code></td>
<td>
<p>Method used for MCMC sampling, one of <code>slice_np</code>, <code>slice</code>,
<code>hmc</code>, <code>nuts</code>. Currently defaults to <code>slice_np</code> for a custom numpy
implementation of slice sampling; select <code>hmc</code>, <code>nuts</code> or <code>slice</code> for
Pyro-based sampling.</p>
</td>
<td><code>'slice_np'</code></td>
</tr>
<tr>
<td><code>mcmc_parameters</code></td>
<td><code>Optional[Dict[str, Any]]</code></td>
<td>
<p>Dictionary overriding the default parameters for MCMC.
The following parameters are supported: <code>thin</code> to set the thinning
factor for the chain, <code>warmup_steps</code> to set the initial number of
samples to discard, <code>num_chains</code> for the number of chains,
<code>init_strategy</code> for the initialisation strategy for chains; <code>prior</code>
will draw init locations from prior, whereas <code>sir</code> will use Sequential-
Importance-Resampling using <code>init_strategy_num_candidates</code> to find init
locations.</p>
</td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>get_potential_function</code></td>
<td><code>Optional[Callable]</code></td>
<td>
<p>Callable that returns the potential function used
for MCMC sampling.</p>
</td>
<td><code>None</code></td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Source code in <code>sbi/inference/posteriors/direct_posterior.py</code></summary>
<table class="highlighttable">
<tr>
<td class="linenos">
<div class="linenodiv">
<pre><span></span>43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88</pre>
</div>
</td>
<td class="code">
<div class="highlight">
<pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">method_family</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">neural_net</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">prior</span><span class="p">,</span>
    <span class="n">x_shape</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">,</span>
    <span class="n">sample_with_mcmc</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">mcmc_method</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">"slice_np"</span><span class="p">,</span>
    <span class="n">mcmc_parameters</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">get_potential_function</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Args:</span>
<span class="sd">        method_family: One of snpe, snl, snre_a or snre_b.</span>
<span class="sd">        neural_net: A classifier for SNRE, a density estimator for SNPE and SNL.</span>
<span class="sd">        prior: Prior distribution with `.log_prob()` and `.sample()`.</span>
<span class="sd">        x_shape: Shape of a single simulator output.</span>
<span class="sd">        sample_with_mcmc: Whether to sample with MCMC. Will always be `True` for SRE</span>
<span class="sd">            and SNL, but can also be set to `True` for SNPE if MCMC is preferred to</span>
<span class="sd">            deal with leakage over rejection sampling.</span>
<span class="sd">        mcmc_method: Method used for MCMC sampling, one of `slice_np`, `slice`,</span>
<span class="sd">            `hmc`, `nuts`. Currently defaults to `slice_np` for a custom numpy</span>
<span class="sd">            implementation of slice sampling; select `hmc`, `nuts` or `slice` for</span>
<span class="sd">            Pyro-based sampling.</span>
<span class="sd">        mcmc_parameters: Dictionary overriding the default parameters for MCMC.</span>
<span class="sd">            The following parameters are supported: `thin` to set the thinning</span>
<span class="sd">            factor for the chain, `warmup_steps` to set the initial number of</span>
<span class="sd">            samples to discard, `num_chains` for the number of chains,</span>
<span class="sd">            `init_strategy` for the initialisation strategy for chains; `prior`</span>
<span class="sd">            will draw init locations from prior, whereas `sir` will use Sequential-</span>
<span class="sd">            Importance-Resampling using `init_strategy_num_candidates` to find init</span>
<span class="sd">            locations.</span>
<span class="sd">        get_potential_function: Callable that returns the potential function used</span>
<span class="sd">            for MCMC sampling.</span>
<span class="sd">    """</span>

    <span class="n">kwargs</span> <span class="o">=</span> <span class="n">del_entries</span><span class="p">(</span>
        <span class="nb">locals</span><span class="p">(),</span> <span class="n">entries</span><span class="o">=</span><span class="p">(</span><span class="s2">"self"</span><span class="p">,</span> <span class="s2">"__class__"</span><span class="p">,</span> <span class="s2">"sample_with_mcmc"</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">set_sample_with_mcmc</span><span class="p">(</span><span class="n">sample_with_mcmc</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_purpose</span> <span class="o">=</span> <span class="p">(</span>
        <span class="s2">"It allows to .sample() and .log_prob() the posterior and wraps the "</span>
        <span class="s2">"output of the .net to avoid leakage into regions with 0 prior probability."</span>
    <span class="p">)</span>
</code></pre>
</div>
</td>
</tr>
</table>
</details>
</div>
</div>
<div class="doc doc-object doc-method">
<h4 class="doc doc-heading" data-toc-label="leakage_correction()" id="sbi.inference.posteriors.direct_posterior.DirectPosterior.leakage_correction">
<code class="highlight language-python">
leakage_correction<span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">num_rejection_samples</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">force_update</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">show_progress_bars</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> </code>
</h4>
<div class="doc doc-contents">
<p>Return leakage correction factor for a leaky posterior density estimate.</p>
<p>The factor is estimated from the acceptance probability during rejection
sampling from the posterior.</p>
<p>This is to avoid re-estimating the acceptance probability from scratch
whenever <code>log_prob</code> is called and <code>norm_posterior=True</code>. Here, it
is estimated only once for <code>self.default_x</code> and saved for later. We
re-evaluate only whenever a new <code>x</code> is passed.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>x</code></td>
<td><code>Tensor</code></td>
<td>
<p>Conditioning context for posterior <span><span class="MathJax_Preview">p(\theta|x)</span><script type="math/tex">p(\theta|x)</script></span>.</p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>num_rejection_samples</code></td>
<td><code>int</code></td>
<td>
<p>Number of samples used to estimate correction factor.</p>
</td>
<td><code>10000</code></td>
</tr>
<tr>
<td><code>force_update</code></td>
<td><code>bool</code></td>
<td>
<p>Whether to force a reevaluation of the leakage correction even
if the context <code>x</code> is the same as <code>self.default_x</code>. This is useful to
enforce a new leakage estimate for rounds after the first (2, 3,..).</p>
</td>
<td><code>False</code></td>
</tr>
<tr>
<td><code>show_progress_bars</code></td>
<td><code>bool</code></td>
<td>
<p>Whether to show a progress bar during sampling.</p>
</td>
<td><code>False</code></td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>Tensor</code></td>
<td>
<p>Saved or newly-estimated correction factor (as a scalar <code>Tensor</code>).</p>
</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Source code in <code>sbi/inference/posteriors/direct_posterior.py</code></summary>
<table class="highlighttable">
<tr>
<td class="linenos">
<div class="linenodiv">
<pre><span></span>175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227</pre>
</div>
</td>
<td class="code">
<div class="highlight">
<pre><span></span><code><span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
<span class="k">def</span> <span class="nf">leakage_correction</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">num_rejection_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10_000</span><span class="p">,</span>
    <span class="n">force_update</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">"""Return leakage correction factor for a leaky posterior density estimate.</span>

<span class="sd">    The factor is estimated from the acceptance probability during rejection</span>
<span class="sd">    sampling from the posterior.</span>

<span class="sd">    This is to avoid re-estimating the acceptance probability from scratch</span>
<span class="sd">    whenever `log_prob` is called and `norm_posterior=True`. Here, it</span>
<span class="sd">    is estimated only once for `self.default_x` and saved for later. We</span>
<span class="sd">    re-evaluate only whenever a new `x` is passed.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        x: Conditioning context for posterior $p(\theta|x)$.</span>
<span class="sd">        num_rejection_samples: Number of samples used to estimate correction factor.</span>
<span class="sd">        force_update: Whether to force a reevaluation of the leakage correction even</span>
<span class="sd">            if the context `x` is the same as `self.default_x`. This is useful to</span>
<span class="sd">            enforce a new leakage estimate for rounds after the first (2, 3,..).</span>
<span class="sd">        show_progress_bars: Whether to show a progress bar during sampling.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Saved or newly-estimated correction factor (as a scalar `Tensor`).</span>
<span class="sd">    """</span>

    <span class="k">def</span> <span class="nf">acceptance_at</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">utils</span><span class="o">.</span><span class="n">sample_posterior_within_prior</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span><span class="p">,</span>
            <span class="n">x</span><span class="p">,</span>
            <span class="n">num_rejection_samples</span><span class="p">,</span>
            <span class="n">show_progress_bars</span><span class="p">,</span>
            <span class="n">sample_for_correction_factor</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="p">)[</span><span class="mi">1</span><span class="p">]</span>

    <span class="c1"># Check if the provided x matches the default x (short-circuit on identity).</span>
    <span class="n">is_new_x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">default_x</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="p">(</span>
        <span class="n">x</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">default_x</span> <span class="ow">and</span> <span class="p">(</span><span class="n">x</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">default_x</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">()</span>
    <span class="p">)</span>

    <span class="n">not_saved_at_default_x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_leakage_density_correction_factor</span> <span class="ow">is</span> <span class="kc">None</span>

    <span class="k">if</span> <span class="n">is_new_x</span><span class="p">:</span>  <span class="c1"># Calculate at x; don't save.</span>
        <span class="k">return</span> <span class="n">acceptance_at</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">not_saved_at_default_x</span> <span class="ow">or</span> <span class="n">force_update</span><span class="p">:</span>  <span class="c1"># Calculate at default_x; save.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_leakage_density_correction_factor</span> <span class="o">=</span> <span class="n">acceptance_at</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">default_x</span><span class="p">)</span>

    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_leakage_density_correction_factor</span>  <span class="c1"># type:ignore</span>
</code></pre>
</div>
</td>
</tr>
</table>
</details>
</div>
</div>
<div class="doc doc-object doc-method">
<h4 class="doc doc-heading" data-toc-label="log_prob()" id="sbi.inference.posteriors.direct_posterior.DirectPosterior.log_prob">
<code class="highlight language-python">
log_prob<span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">norm_posterior</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">track_gradients</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> </code>
</h4>
<div class="doc doc-contents">
<p>Returns the log-probability of the posterior <span><span class="MathJax_Preview">p(\theta|x).</span><script type="math/tex">p(\theta|x).</script></span></p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>theta</code></td>
<td><code>Tensor</code></td>
<td>
<p>Parameters <span><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span>.</p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>x</code></td>
<td><code>Optional[torch.Tensor]</code></td>
<td>
<p>Conditioning context for posterior <span><span class="MathJax_Preview">p(\theta|x)</span><script type="math/tex">p(\theta|x)</script></span>. If not provided, fall
back onto an <code>x_o</code> if previously provided for multi-round training, or
to another default if set later for convenience, see <code>.set_default_x()</code>.</p>
</td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>norm_posterior</code></td>
<td><code>bool</code></td>
<td>
<p>Whether to enforce a normalized posterior density.
Renormalization of the posterior is useful when some
probability falls out or leaks out of the prescribed prior support.
The normalizing factor is calculated via rejection sampling, so if you
need speedier but unnormalized log posterior estimates set here
<code>norm_posterior=False</code>. The returned log posterior is set to
-∞ outside of the prior support regardless of this setting.</p>
</td>
<td><code>True</code></td>
</tr>
<tr>
<td><code>track_gradients</code></td>
<td><code>bool</code></td>
<td>
<p>Whether the returned tensor supports tracking gradients.
This can be helpful for e.g. sensitivity analysis, but increases memory
consumption.</p>
</td>
<td><code>False</code></td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>Tensor</code></td>
<td>
<p><code>(len(θ),)</code>-shaped log posterior probability <span><span class="MathJax_Preview">\log p(\theta|x)</span><script type="math/tex">\log p(\theta|x)</script></span> for θ in the
support of the prior, -∞ (corresponding to 0 probability) outside.</p>
</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Source code in <code>sbi/inference/posteriors/direct_posterior.py</code></summary>
<table class="highlighttable">
<tr>
<td class="linenos">
<div class="linenodiv">
<pre><span></span>118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173</pre>
</div>
</td>
<td class="code">
<div class="highlight">
<pre><span></span><code><span class="k">def</span> <span class="nf">log_prob</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">theta</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">norm_posterior</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">track_gradients</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">"""</span>
<span class="sd">    Returns the log-probability of the posterior $p(\theta|x).$</span>

<span class="sd">    Args:</span>
<span class="sd">        theta: Parameters $\theta$.</span>
<span class="sd">        x: Conditioning context for posterior $p(\theta|x)$. If not provided, fall</span>
<span class="sd">            back onto an `x_o` if previously provided for multi-round training, or</span>
<span class="sd">            to another default if set later for convenience, see `.set_default_x()`.</span>
<span class="sd">        norm_posterior: Whether to enforce a normalized posterior density.</span>
<span class="sd">            Renormalization of the posterior is useful when some</span>
<span class="sd">            probability falls out or leaks out of the prescribed prior support.</span>
<span class="sd">            The normalizing factor is calculated via rejection sampling, so if you</span>
<span class="sd">            need speedier but unnormalized log posterior estimates set here</span>
<span class="sd">            `norm_posterior=False`. The returned log posterior is set to</span>
<span class="sd">            -∞ outside of the prior support regardless of this setting.</span>
<span class="sd">        track_gradients: Whether the returned tensor supports tracking gradients.</span>
<span class="sd">            This can be helpful for e.g. sensitivity analysis, but increases memory</span>
<span class="sd">            consumption.</span>

<span class="sd">    Returns:</span>
<span class="sd">        `(len(θ),)`-shaped log posterior probability $\log p(\theta|x)$ for θ in the</span>
<span class="sd">        support of the prior, -∞ (corresponding to 0 probability) outside.</span>

<span class="sd">    """</span>

    <span class="c1"># TODO Train exited here, entered after sampling?</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

    <span class="n">theta</span><span class="p">,</span> <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_theta_and_x_for_log_prob_</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>

    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">set_grad_enabled</span><span class="p">(</span><span class="n">track_gradients</span><span class="p">):</span>
        <span class="n">unnorm_log_prob</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>

        <span class="c1"># Force probability to be zero outside prior support.</span>
        <span class="n">is_prior_finite</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">isfinite</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_prior</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">theta</span><span class="p">))</span>

        <span class="n">masked_log_prob</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span>
            <span class="n">is_prior_finite</span><span class="p">,</span>
            <span class="n">unnorm_log_prob</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="s2">"-inf"</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span>
        <span class="p">)</span>

        <span class="n">log_factor</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">log</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">leakage_correction</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">batched_first_of_batch</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
            <span class="k">if</span> <span class="n">norm_posterior</span>
            <span class="k">else</span> <span class="mi">0</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">masked_log_prob</span> <span class="o">-</span> <span class="n">log_factor</span>
</code></pre>
</div>
</td>
</tr>
</table>
</details>
</div>
</div>
<div class="doc doc-object doc-method">
<h4 class="doc doc-heading" data-toc-label="sample()" id="sbi.inference.posteriors.direct_posterior.DirectPosterior.sample">
<code class="highlight language-python">
sample<span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sample_shape</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([]),</span> <span class="n">x</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">show_progress_bars</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sample_with_mcmc</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">mcmc_method</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">mcmc_parameters</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span> </code>
</h4>
<div class="doc doc-contents">
<p>Return samples from posterior distribution <span><span class="MathJax_Preview">p(\theta|x)</span><script type="math/tex">p(\theta|x)</script></span>.</p>
<p>Samples are obtained either with rejection sampling or MCMC. Rejection sampling
will be a lot faster if leakage is rather low. If leakage is high (e.g. above
99%, which can happen in multi-round SNPE), MCMC can be faster than rejection
sampling.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>sample_shape</code></td>
<td><code>Union[torch.Size, Tuple[int, ...]]</code></td>
<td>
<p>Desired shape of samples that are drawn from posterior. If
sample_shape is multidimensional we simply draw <code>sample_shape.numel()</code>
samples and then reshape into the desired shape.</p>
</td>
<td><code>torch.Size([])</code></td>
</tr>
<tr>
<td><code>x</code></td>
<td><code>Optional[torch.Tensor]</code></td>
<td>
<p>Conditioning context for posterior <span><span class="MathJax_Preview">p(\theta|x)</span><script type="math/tex">p(\theta|x)</script></span>. If not provided,
fall back onto <code>x_o</code> if previously provided for multiround training, or
to a set default (see <code>set_default_x()</code> method).</p>
</td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>show_progress_bars</code></td>
<td><code>bool</code></td>
<td>
<p>Whether to show sampling progress monitor.</p>
</td>
<td><code>True</code></td>
</tr>
<tr>
<td><code>sample_with_mcmc</code></td>
<td><code>Optional[bool]</code></td>
<td>
<p>Optional parameter to override <code>self.sample_with_mcmc</code>.</p>
</td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>mcmc_method</code></td>
<td><code>Optional[str]</code></td>
<td>
<p>Optional parameter to override <code>self.mcmc_method</code>.</p>
</td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>mcmc_parameters</code></td>
<td><code>Optional[Dict[str, Any]]</code></td>
<td>
<p>Dictionary overriding the default parameters for MCMC.
The following parameters are supported: <code>thin</code> to set the thinning
factor for the chain, <code>warmup_steps</code> to set the initial number of
samples to discard, <code>num_chains</code> for the number of chains,
<code>init_strategy</code> for the initialisation strategy for chains; <code>prior</code>
will draw init locations from prior, whereas <code>sir</code> will use Sequential-
Importance-Resampling using <code>init_strategy_num_candidates</code> to find init
locations.</p>
</td>
<td><code>None</code></td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>Tensor</code></td>
<td>
<p>Samples from posterior.</p>
</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Source code in <code>sbi/inference/posteriors/direct_posterior.py</code></summary>
<table class="highlighttable">
<tr>
<td class="linenos">
<div class="linenodiv">
<pre><span></span>229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295</pre>
</div>
</td>
<td class="code">
<div class="highlight">
<pre><span></span><code><span class="k">def</span> <span class="nf">sample</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">sample_shape</span><span class="p">:</span> <span class="n">Shape</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(),</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">sample_with_mcmc</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">mcmc_method</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">mcmc_parameters</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">"""</span>
<span class="sd">    Return samples from posterior distribution $p(\theta|x)$.</span>

<span class="sd">    Samples are obtained either with rejection sampling or MCMC. Rejection sampling</span>
<span class="sd">    will be a lot faster if leakage is rather low. If leakage is high (e.g. above</span>
<span class="sd">    99%, which can happen in multi-round SNPE), MCMC can be faster than rejection</span>
<span class="sd">    sampling.</span>

<span class="sd">    Args:</span>
<span class="sd">        sample_shape: Desired shape of samples that are drawn from posterior. If</span>
<span class="sd">            sample_shape is multidimensional we simply draw `sample_shape.numel()`</span>
<span class="sd">            samples and then reshape into the desired shape.</span>
<span class="sd">        x: Conditioning context for posterior $p(\theta|x)$. If not provided,</span>
<span class="sd">            fall back onto `x_o` if previously provided for multiround training, or</span>
<span class="sd">            to a set default (see `set_default_x()` method).</span>
<span class="sd">        show_progress_bars: Whether to show sampling progress monitor.</span>
<span class="sd">        sample_with_mcmc: Optional parameter to override `self.sample_with_mcmc`.</span>
<span class="sd">        mcmc_method: Optional parameter to override `self.mcmc_method`.</span>
<span class="sd">        mcmc_parameters: Dictionary overriding the default parameters for MCMC.</span>
<span class="sd">            The following parameters are supported: `thin` to set the thinning</span>
<span class="sd">            factor for the chain, `warmup_steps` to set the initial number of</span>
<span class="sd">            samples to discard, `num_chains` for the number of chains,</span>
<span class="sd">            `init_strategy` for the initialisation strategy for chains; `prior`</span>
<span class="sd">            will draw init locations from prior, whereas `sir` will use Sequential-</span>
<span class="sd">            Importance-Resampling using `init_strategy_num_candidates` to find init</span>
<span class="sd">            locations.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Samples from posterior.</span>
<span class="sd">    """</span>

    <span class="n">x</span><span class="p">,</span> <span class="n">num_samples</span><span class="p">,</span> <span class="n">mcmc_method</span><span class="p">,</span> <span class="n">mcmc_parameters</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_for_sample</span><span class="p">(</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">sample_shape</span><span class="p">,</span> <span class="n">mcmc_method</span><span class="p">,</span> <span class="n">mcmc_parameters</span>
    <span class="p">)</span>

    <span class="n">sample_with_mcmc</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sample_with_mcmc</span> <span class="k">if</span> <span class="n">sample_with_mcmc</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">sample_with_mcmc</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="n">sample_with_mcmc</span><span class="p">:</span>
        <span class="n">samples</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sample_posterior_mcmc</span><span class="p">(</span>
            <span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span>
            <span class="n">num_samples</span><span class="o">=</span><span class="n">num_samples</span><span class="p">,</span>
            <span class="n">show_progress_bars</span><span class="o">=</span><span class="n">show_progress_bars</span><span class="p">,</span>
            <span class="n">mcmc_method</span><span class="o">=</span><span class="n">mcmc_method</span><span class="p">,</span>
            <span class="o">**</span><span class="n">mcmc_parameters</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Rejection sampling.</span>
        <span class="n">samples</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">sample_posterior_within_prior</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span><span class="p">,</span>
            <span class="n">x</span><span class="p">,</span>
            <span class="n">num_samples</span><span class="o">=</span><span class="n">num_samples</span><span class="p">,</span>
            <span class="n">show_progress_bars</span><span class="o">=</span><span class="n">show_progress_bars</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">return</span> <span class="n">samples</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">*</span><span class="n">sample_shape</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
</code></pre>
</div>
</td>
</tr>
</table>
</details>
</div>
</div>
<div class="doc doc-object doc-method">
<h4 class="doc doc-heading" data-toc-label="set_default_x()" id="sbi.inference.posteriors.direct_posterior.DirectPosterior.set_default_x">
<code class="highlight language-python">
set_default_x<span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> </code>
<span class="doc doc-properties">
<small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
</span>
</h4>
<div class="doc doc-contents">
<p>Set new default x for <code>.sample(), .log_prob</code> to use as conditioning context.</p>
<p>This is a pure convenience to avoid having to repeatedly specify <code>x</code> in calls to
<code>.sample()</code> and <code>.log_prob()</code> - only θ needs to be passed.</p>
<p>This convenience is particularly useful when the posterior is focused, i.e.
has been trained over multiple rounds to be accurate in the vicinity of a
particular <code>x=x_o</code> (you can check if your posterior object is focused by
printing it).</p>
<p>NOTE: this method is chainable, i.e. will return the NeuralPosterior object so
that calls like <code>posterior.set_default_x(my_x).sample(mytheta)</code> are possible.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>x</code></td>
<td><code>Tensor</code></td>
<td>
<p>The default observation to set for the posterior <span><span class="MathJax_Preview">p(theta|x)</span><script type="math/tex">p(theta|x)</script></span>.</p>
</td>
<td><em>required</em></td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>NeuralPosterior</code></td>
<td>
<p><code>NeuralPosterior</code> that will use a default <code>x</code> when not explicitly passed.</p>
</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Source code in <code>sbi/inference/posteriors/direct_posterior.py</code></summary>
<table class="highlighttable">
<tr>
<td class="linenos">
<div class="linenodiv">
<pre><span></span>101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124</pre>
</div>
</td>
<td class="code">
<div class="highlight">
<pre><span></span><code><span class="k">def</span> <span class="nf">set_default_x</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">"NeuralPosterior"</span><span class="p">:</span>
    <span class="sd">"""Set new default x for `.sample(), .log_prob` to use as conditioning context.</span>

<span class="sd">    This is a pure convenience to avoid having to repeatedly specify `x` in calls to</span>
<span class="sd">    `.sample()` and `.log_prob()` - only θ needs to be passed.</span>

<span class="sd">    This convenience is particularly useful when the posterior is focused, i.e.</span>
<span class="sd">    has been trained over multiple rounds to be accurate in the vicinity of a</span>
<span class="sd">    particular `x=x_o` (you can check if your posterior object is focused by</span>
<span class="sd">    printing it).</span>

<span class="sd">    NOTE: this method is chainable, i.e. will return the NeuralPosterior object so</span>
<span class="sd">    that calls like `posterior.set_default_x(my_x).sample(mytheta)` are possible.</span>

<span class="sd">    Args:</span>
<span class="sd">        x: The default observation to set for the posterior $p(theta|x)$.</span>

<span class="sd">    Returns:</span>
<span class="sd">        `NeuralPosterior` that will use a default `x` when not explicitly passed.</span>
<span class="sd">    """</span>
    <span class="n">processed_x</span> <span class="o">=</span> <span class="n">process_x</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_x_shape</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_x</span> <span class="o">=</span> <span class="n">processed_x</span>

    <span class="k">return</span> <span class="bp">self</span>
</code></pre>
</div>
</td>
</tr>
</table>
</details>
</div>
</div>
<div class="doc doc-object doc-method">
<h4 class="doc doc-heading" data-toc-label="set_mcmc_method()" id="sbi.inference.posteriors.direct_posterior.DirectPosterior.set_mcmc_method">
<code class="highlight language-python">
set_mcmc_method<span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">method</span><span class="p">)</span> </code>
<span class="doc doc-properties">
<small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
</span>
</h4>
<div class="doc doc-contents">
<p>Sets sampling method to for MCMC and returns <code>NeuralPosterior</code>.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>method</code></td>
<td><code>str</code></td>
<td>
<p>Method to use.</p>
</td>
<td><em>required</em></td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>NeuralPosterior</code></td>
<td>
<p><code>NeuralPosterior</code> for chainable calls.</p>
</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Source code in <code>sbi/inference/posteriors/direct_posterior.py</code></summary>
<table class="highlighttable">
<tr>
<td class="linenos">
<div class="linenodiv">
<pre><span></span>136
137
138
139
140
141
142
143
144
145
146</pre>
</div>
</td>
<td class="code">
<div class="highlight">
<pre><span></span><code><span class="k">def</span> <span class="nf">set_mcmc_method</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">method</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">"NeuralPosterior"</span><span class="p">:</span>
    <span class="sd">"""Sets sampling method to for MCMC and returns `NeuralPosterior`.</span>

<span class="sd">    Args:</span>
<span class="sd">        method: Method to use.</span>

<span class="sd">    Returns:</span>
<span class="sd">        `NeuralPosterior` for chainable calls.</span>
<span class="sd">    """</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_mcmc_method</span> <span class="o">=</span> <span class="n">method</span>
    <span class="k">return</span> <span class="bp">self</span>
</code></pre>
</div>
</td>
</tr>
</table>
</details>
</div>
</div>
<div class="doc doc-object doc-method">
<h4 class="doc doc-heading" data-toc-label="set_mcmc_parameters()" id="sbi.inference.posteriors.direct_posterior.DirectPosterior.set_mcmc_parameters">
<code class="highlight language-python">
set_mcmc_parameters<span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">parameters</span><span class="p">)</span> </code>
<span class="doc doc-properties">
<small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
</span>
</h4>
<div class="doc doc-contents">
<p>Sets parameters for MCMC and returns <code>NeuralPosterior</code>.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>parameters</code></td>
<td><code>Dict[str, Any]</code></td>
<td>
<p>Dictionary overriding the default parameters for MCMC.
The following parameters are supported: <code>thin</code> to set the thinning
factor for the chain, <code>warmup_steps</code> to set the initial number of
samples to discard, <code>num_chains</code> for the number of chains,
<code>init_strategy</code> for the initialisation strategy for chains; <code>prior</code>
will draw init locations from prior, whereas <code>sir</code> will use Sequential-
Importance-Resampling using <code>init_strategy_num_candidates</code> to find init
locations.</p>
</td>
<td><em>required</em></td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>NeuralPosterior</code></td>
<td>
<p><code>NeuralPosterior</code> for chainable calls.</p>
</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Source code in <code>sbi/inference/posteriors/direct_posterior.py</code></summary>
<table class="highlighttable">
<tr>
<td class="linenos">
<div class="linenodiv">
<pre><span></span>161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178</pre>
</div>
</td>
<td class="code">
<div class="highlight">
<pre><span></span><code><span class="k">def</span> <span class="nf">set_mcmc_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">parameters</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="s2">"NeuralPosterior"</span><span class="p">:</span>
    <span class="sd">"""Sets parameters for MCMC and returns `NeuralPosterior`.</span>

<span class="sd">    Args:</span>
<span class="sd">        parameters: Dictionary overriding the default parameters for MCMC.</span>
<span class="sd">            The following parameters are supported: `thin` to set the thinning</span>
<span class="sd">            factor for the chain, `warmup_steps` to set the initial number of</span>
<span class="sd">            samples to discard, `num_chains` for the number of chains,</span>
<span class="sd">            `init_strategy` for the initialisation strategy for chains; `prior`</span>
<span class="sd">            will draw init locations from prior, whereas `sir` will use Sequential-</span>
<span class="sd">            Importance-Resampling using `init_strategy_num_candidates` to find init</span>
<span class="sd">            locations.</span>

<span class="sd">    Returns:</span>
<span class="sd">        `NeuralPosterior` for chainable calls.</span>
<span class="sd">    """</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_mcmc_parameters</span> <span class="o">=</span> <span class="n">parameters</span>
    <span class="k">return</span> <span class="bp">self</span>
</code></pre>
</div>
</td>
</tr>
</table>
</details>
</div>
</div>
<div class="doc doc-object doc-method">
<h4 class="doc doc-heading" data-toc-label="set_sample_with_mcmc()" id="sbi.inference.posteriors.direct_posterior.DirectPosterior.set_sample_with_mcmc">
<code class="highlight language-python">
set_sample_with_mcmc<span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">use_mcmc</span><span class="p">)</span> </code>
</h4>
<div class="doc doc-contents">
<p>Turns MCMC sampling on or off and returns <code>NeuralPosterior</code>.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>use_mcmc</code></td>
<td><code>bool</code></td>
<td>
<p>Flag to set whether or not MCMC sampling is used.</p>
</td>
<td><em>required</em></td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>NeuralPosterior</code></td>
<td>
<p><code>NeuralPosterior</code> for chainable calls.</p>
</td>
</tr>
</tbody>
</table>
<p><strong>Exceptions:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>ValueError</code></td>
<td>
<p>on attempt to turn off MCMC sampling for family of methods that
do not support rejection sampling.</p>
</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Source code in <code>sbi/inference/posteriors/direct_posterior.py</code></summary>
<table class="highlighttable">
<tr>
<td class="linenos">
<div class="linenodiv">
<pre><span></span>102
103
104
105
106
107
108
109
110
111
112
113
114
115
116</pre>
</div>
</td>
<td class="code">
<div class="highlight">
<pre><span></span><code><span class="k">def</span> <span class="nf">set_sample_with_mcmc</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">use_mcmc</span><span class="p">:</span> <span class="nb">bool</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">"NeuralPosterior"</span><span class="p">:</span>
    <span class="sd">"""Turns MCMC sampling on or off and returns `NeuralPosterior`.</span>

<span class="sd">    Args:</span>
<span class="sd">        use_mcmc: Flag to set whether or not MCMC sampling is used.</span>

<span class="sd">    Returns:</span>
<span class="sd">        `NeuralPosterior` for chainable calls.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: on attempt to turn off MCMC sampling for family of methods that</span>
<span class="sd">            do not support rejection sampling.</span>
<span class="sd">    """</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_sample_with_mcmc</span> <span class="o">=</span> <span class="n">use_mcmc</span>
    <span class="k">return</span> <span class="bp">self</span>
</code></pre>
</div>
</td>
</tr>
</table>
</details>
</div>
</div>
</div>
</div>
</div>
<div class="doc doc-object doc-class">
<h3 class="doc doc-heading" id="sbi.inference.posteriors.likelihood_based_posterior.LikelihoodBasedPosterior">
<code>sbi.inference.posteriors.likelihood_based_posterior.LikelihoodBasedPosterior</code>
<a class="headerlink" href="#sbi.inference.posteriors.likelihood_based_posterior.LikelihoodBasedPosterior" title="Permanent link">¶</a></h3>
<div class="doc doc-contents first">
<p>Posterior <span><span class="MathJax_Preview">p(\theta|x)</span><script type="math/tex">p(\theta|x)</script></span> with <code>log_prob()</code> and <code>sample()</code> methods, obtained with
    SNLE.<br/>
<br/>

    SNLE trains a neural network to approximate the likelihood <span><span class="MathJax_Preview">p(x|\theta)</span><script type="math/tex">p(x|\theta)</script></span>. The
    <code>SNLE_Posterior</code> class wraps the trained network such that one can directly evaluate
    the unnormalized posterior log probability <span><span class="MathJax_Preview">p(\theta|x) \propto p(x|\theta) \cdot
    p(\theta)</span><script type="math/tex">p(\theta|x) \propto p(x|\theta) \cdot
    p(\theta)</script></span> and draw samples from the posterior with MCMC.<br/>
<br/>

    The neural network itself can be accessed via the <code>.net</code> attribute.</p>
<div class="doc doc-children">
<div class="doc doc-object doc-attribute">
<h4 class="doc doc-heading" data-toc-label="default_x" id="sbi.inference.posteriors.likelihood_based_posterior.LikelihoodBasedPosterior.default_x">
<code class="highlight">
default_x: <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> </code>
<span class="doc doc-properties">
<small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
<small class="doc doc-property doc-property-property"><code>property</code></small>
<small class="doc doc-property doc-property-writable"><code>writable</code></small>
</span>
</h4>
<div class="doc doc-contents">
<p>Return default x used by <code>.sample(), .log_prob</code> as conditioning context.</p>
</div>
</div>
<div class="doc doc-object doc-attribute">
<h4 class="doc doc-heading" data-toc-label="mcmc_method" id="sbi.inference.posteriors.likelihood_based_posterior.LikelihoodBasedPosterior.mcmc_method">
<code class="highlight">
mcmc_method: <span class="nb">str</span> </code>
<span class="doc doc-properties">
<small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
<small class="doc doc-property doc-property-property"><code>property</code></small>
<small class="doc doc-property doc-property-writable"><code>writable</code></small>
</span>
</h4>
<div class="doc doc-contents">
<p>Returns MCMC method.</p>
</div>
</div>
<div class="doc doc-object doc-attribute">
<h4 class="doc doc-heading" data-toc-label="mcmc_parameters" id="sbi.inference.posteriors.likelihood_based_posterior.LikelihoodBasedPosterior.mcmc_parameters">
<code class="highlight">
mcmc_parameters: <span class="nb">dict</span> </code>
<span class="doc doc-properties">
<small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
<small class="doc doc-property doc-property-property"><code>property</code></small>
<small class="doc doc-property doc-property-writable"><code>writable</code></small>
</span>
</h4>
<div class="doc doc-contents">
<p>Returns MCMC parameters.</p>
</div>
</div>
<div class="doc doc-object doc-method">
<h4 class="doc doc-heading" data-toc-label="log_prob()" id="sbi.inference.posteriors.likelihood_based_posterior.LikelihoodBasedPosterior.log_prob">
<code class="highlight language-python">
log_prob<span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">track_gradients</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> </code>
</h4>
<div class="doc doc-contents">
<p>Returns the log-probability of <span><span class="MathJax_Preview">p(x|\theta) \cdot p(\theta).</span><script type="math/tex">p(x|\theta) \cdot p(\theta).</script></span></p>
<p>This corresponds to an <strong>unnormalized</strong> posterior log-probability.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>theta</code></td>
<td><code>Tensor</code></td>
<td>
<p>Parameters <span><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span>.</p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>x</code></td>
<td><code>Optional[torch.Tensor]</code></td>
<td>
<p>Conditioning context for posterior <span><span class="MathJax_Preview">p(\theta|x)</span><script type="math/tex">p(\theta|x)</script></span>. If not provided, fall
back onto an <code>x_o</code> if previously provided for multi-round training, or
to another default if set later for convenience, see <code>.set_default_x()</code>.</p>
</td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>track_gradients</code></td>
<td><code>bool</code></td>
<td>
<p>Whether the returned tensor supports tracking gradients.
This can be helpful for e.g. sensitivity analysis, but increases memory
consumption.</p>
</td>
<td><code>False</code></td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>Tensor</code></td>
<td>
<p><code>(len(θ),)</code>-shaped log-probability <span><span class="MathJax_Preview">\log(p(x|\theta) \cdot p(\theta))</span><script type="math/tex">\log(p(x|\theta) \cdot p(\theta))</script></span>.</p>
</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Source code in <code>sbi/inference/posteriors/likelihood_based_posterior.py</code></summary>
<table class="highlighttable">
<tr>
<td class="linenos">
<div class="linenodiv">
<pre><span></span> 77
 78
 79
 80
 81
 82
 83
 84
 85
 86
 87
 88
 89
 90
 91
 92
 93
 94
 95
 96
 97
 98
 99
100
101
102
103
104
105
106
107
108
109</pre>
</div>
</td>
<td class="code">
<div class="highlight">
<pre><span></span><code><span class="k">def</span> <span class="nf">log_prob</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">track_gradients</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">"""</span>
<span class="sd">    Returns the log-probability of $p(x|\theta) \cdot p(\theta).$</span>

<span class="sd">    This corresponds to an **unnormalized** posterior log-probability.</span>

<span class="sd">    Args:</span>
<span class="sd">        theta: Parameters $\theta$.</span>
<span class="sd">        x: Conditioning context for posterior $p(\theta|x)$. If not provided, fall</span>
<span class="sd">            back onto an `x_o` if previously provided for multi-round training, or</span>
<span class="sd">            to another default if set later for convenience, see `.set_default_x()`.</span>
<span class="sd">        track_gradients: Whether the returned tensor supports tracking gradients.</span>
<span class="sd">            This can be helpful for e.g. sensitivity analysis, but increases memory</span>
<span class="sd">            consumption.</span>

<span class="sd">    Returns:</span>
<span class="sd">        `(len(θ),)`-shaped log-probability $\log(p(x|\theta) \cdot p(\theta))$.</span>

<span class="sd">    """</span>

    <span class="c1"># TODO Train exited here, entered after sampling?</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

    <span class="n">theta</span><span class="p">,</span> <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_theta_and_x_for_log_prob_</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>

    <span class="n">warn</span><span class="p">(</span>
        <span class="s2">"The log probability from SNL is only correct up to a normalizing constant."</span>
    <span class="p">)</span>

    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">set_grad_enabled</span><span class="p">(</span><span class="n">track_gradients</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">theta</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>
</code></pre>
</div>
</td>
</tr>
</table>
</details>
</div>
</div>
<div class="doc doc-object doc-method">
<h4 class="doc doc-heading" data-toc-label="sample()" id="sbi.inference.posteriors.likelihood_based_posterior.LikelihoodBasedPosterior.sample">
<code class="highlight language-python">
sample<span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sample_shape</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([]),</span> <span class="n">x</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">show_progress_bars</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sample_with_mcmc</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">mcmc_method</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">mcmc_parameters</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span> </code>
</h4>
<div class="doc doc-contents">
<p>Return samples from posterior distribution <span><span class="MathJax_Preview">p(\theta|x)</span><script type="math/tex">p(\theta|x)</script></span> with MCMC.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>sample_shape</code></td>
<td><code>Union[torch.Size, Tuple[int, ...]]</code></td>
<td>
<p>Desired shape of samples that are drawn from posterior. If
sample_shape is multidimensional we simply draw <code>sample_shape.numel()</code>
samples and then reshape into the desired shape.</p>
</td>
<td><code>torch.Size([])</code></td>
</tr>
<tr>
<td><code>x</code></td>
<td><code>Optional[torch.Tensor]</code></td>
<td>
<p>Conditioning context for posterior <span><span class="MathJax_Preview">p(\theta|x)</span><script type="math/tex">p(\theta|x)</script></span>. If not provided,
fall back onto <code>x_o</code> if previously provided for multiround training, or
to a set default (see <code>set_default_x()</code> method).</p>
</td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>show_progress_bars</code></td>
<td><code>bool</code></td>
<td>
<p>Whether to show sampling progress monitor.</p>
</td>
<td><code>True</code></td>
</tr>
<tr>
<td><code>sample_with_mcmc</code></td>
<td><code>Optional[bool]</code></td>
<td>
<p>Optional parameter to override <code>self.sample_with_mcmc</code>.</p>
</td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>mcmc_method</code></td>
<td><code>Optional[str]</code></td>
<td>
<p>Optional parameter to override <code>self.mcmc_method</code>.</p>
</td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>mcmc_parameters</code></td>
<td><code>Optional[Dict[str, Any]]</code></td>
<td>
<p>Dictionary overriding the default parameters for MCMC.
The following parameters are supported: <code>thin</code> to set the thinning
factor for the chain, <code>warmup_steps</code> to set the initial number of
samples to discard, <code>num_chains</code> for the number of chains,
<code>init_strategy</code> for the initialisation strategy for chains; <code>prior</code>
will draw init locations from prior, whereas <code>sir</code> will use Sequential-
Importance-Resampling using <code>init_strategy_num_candidates</code> to find init
locations.</p>
</td>
<td><code>None</code></td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>Tensor</code></td>
<td>
<p>Samples from posterior.</p>
</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Source code in <code>sbi/inference/posteriors/likelihood_based_posterior.py</code></summary>
<table class="highlighttable">
<tr>
<td class="linenos">
<div class="linenodiv">
<pre><span></span>111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158</pre>
</div>
</td>
<td class="code">
<div class="highlight">
<pre><span></span><code><span class="k">def</span> <span class="nf">sample</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">sample_shape</span><span class="p">:</span> <span class="n">Shape</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(),</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">sample_with_mcmc</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">mcmc_method</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">mcmc_parameters</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">"""</span>
<span class="sd">    Return samples from posterior distribution $p(\theta|x)$ with MCMC.</span>

<span class="sd">    Args:</span>
<span class="sd">        sample_shape: Desired shape of samples that are drawn from posterior. If</span>
<span class="sd">            sample_shape is multidimensional we simply draw `sample_shape.numel()`</span>
<span class="sd">            samples and then reshape into the desired shape.</span>
<span class="sd">        x: Conditioning context for posterior $p(\theta|x)$. If not provided,</span>
<span class="sd">            fall back onto `x_o` if previously provided for multiround training, or</span>
<span class="sd">            to a set default (see `set_default_x()` method).</span>
<span class="sd">        show_progress_bars: Whether to show sampling progress monitor.</span>
<span class="sd">        sample_with_mcmc: Optional parameter to override `self.sample_with_mcmc`.</span>
<span class="sd">        mcmc_method: Optional parameter to override `self.mcmc_method`.</span>
<span class="sd">        mcmc_parameters: Dictionary overriding the default parameters for MCMC.</span>
<span class="sd">            The following parameters are supported: `thin` to set the thinning</span>
<span class="sd">            factor for the chain, `warmup_steps` to set the initial number of</span>
<span class="sd">            samples to discard, `num_chains` for the number of chains,</span>
<span class="sd">            `init_strategy` for the initialisation strategy for chains; `prior`</span>
<span class="sd">            will draw init locations from prior, whereas `sir` will use Sequential-</span>
<span class="sd">            Importance-Resampling using `init_strategy_num_candidates` to find init</span>
<span class="sd">            locations.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Samples from posterior.</span>
<span class="sd">    """</span>

    <span class="n">x</span><span class="p">,</span> <span class="n">num_samples</span><span class="p">,</span> <span class="n">mcmc_method</span><span class="p">,</span> <span class="n">mcmc_parameters</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_for_sample</span><span class="p">(</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">sample_shape</span><span class="p">,</span> <span class="n">mcmc_method</span><span class="p">,</span> <span class="n">mcmc_parameters</span>
    <span class="p">)</span>

    <span class="n">samples</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sample_posterior_mcmc</span><span class="p">(</span>
        <span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span>
        <span class="n">num_samples</span><span class="o">=</span><span class="n">num_samples</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="o">=</span><span class="n">show_progress_bars</span><span class="p">,</span>
        <span class="n">mcmc_method</span><span class="o">=</span><span class="n">mcmc_method</span><span class="p">,</span>
        <span class="o">**</span><span class="n">mcmc_parameters</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">samples</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">*</span><span class="n">sample_shape</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
</code></pre>
</div>
</td>
</tr>
</table>
</details>
</div>
</div>
<div class="doc doc-object doc-method">
<h4 class="doc doc-heading" data-toc-label="set_default_x()" id="sbi.inference.posteriors.likelihood_based_posterior.LikelihoodBasedPosterior.set_default_x">
<code class="highlight language-python">
set_default_x<span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> </code>
<span class="doc doc-properties">
<small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
</span>
</h4>
<div class="doc doc-contents">
<p>Set new default x for <code>.sample(), .log_prob</code> to use as conditioning context.</p>
<p>This is a pure convenience to avoid having to repeatedly specify <code>x</code> in calls to
<code>.sample()</code> and <code>.log_prob()</code> - only θ needs to be passed.</p>
<p>This convenience is particularly useful when the posterior is focused, i.e.
has been trained over multiple rounds to be accurate in the vicinity of a
particular <code>x=x_o</code> (you can check if your posterior object is focused by
printing it).</p>
<p>NOTE: this method is chainable, i.e. will return the NeuralPosterior object so
that calls like <code>posterior.set_default_x(my_x).sample(mytheta)</code> are possible.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>x</code></td>
<td><code>Tensor</code></td>
<td>
<p>The default observation to set for the posterior <span><span class="MathJax_Preview">p(theta|x)</span><script type="math/tex">p(theta|x)</script></span>.</p>
</td>
<td><em>required</em></td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>NeuralPosterior</code></td>
<td>
<p><code>NeuralPosterior</code> that will use a default <code>x</code> when not explicitly passed.</p>
</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Source code in <code>sbi/inference/posteriors/likelihood_based_posterior.py</code></summary>
<table class="highlighttable">
<tr>
<td class="linenos">
<div class="linenodiv">
<pre><span></span>101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124</pre>
</div>
</td>
<td class="code">
<div class="highlight">
<pre><span></span><code><span class="k">def</span> <span class="nf">set_default_x</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">"NeuralPosterior"</span><span class="p">:</span>
    <span class="sd">"""Set new default x for `.sample(), .log_prob` to use as conditioning context.</span>

<span class="sd">    This is a pure convenience to avoid having to repeatedly specify `x` in calls to</span>
<span class="sd">    `.sample()` and `.log_prob()` - only θ needs to be passed.</span>

<span class="sd">    This convenience is particularly useful when the posterior is focused, i.e.</span>
<span class="sd">    has been trained over multiple rounds to be accurate in the vicinity of a</span>
<span class="sd">    particular `x=x_o` (you can check if your posterior object is focused by</span>
<span class="sd">    printing it).</span>

<span class="sd">    NOTE: this method is chainable, i.e. will return the NeuralPosterior object so</span>
<span class="sd">    that calls like `posterior.set_default_x(my_x).sample(mytheta)` are possible.</span>

<span class="sd">    Args:</span>
<span class="sd">        x: The default observation to set for the posterior $p(theta|x)$.</span>

<span class="sd">    Returns:</span>
<span class="sd">        `NeuralPosterior` that will use a default `x` when not explicitly passed.</span>
<span class="sd">    """</span>
    <span class="n">processed_x</span> <span class="o">=</span> <span class="n">process_x</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_x_shape</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_x</span> <span class="o">=</span> <span class="n">processed_x</span>

    <span class="k">return</span> <span class="bp">self</span>
</code></pre>
</div>
</td>
</tr>
</table>
</details>
</div>
</div>
<div class="doc doc-object doc-method">
<h4 class="doc doc-heading" data-toc-label="set_mcmc_method()" id="sbi.inference.posteriors.likelihood_based_posterior.LikelihoodBasedPosterior.set_mcmc_method">
<code class="highlight language-python">
set_mcmc_method<span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">method</span><span class="p">)</span> </code>
<span class="doc doc-properties">
<small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
</span>
</h4>
<div class="doc doc-contents">
<p>Sets sampling method to for MCMC and returns <code>NeuralPosterior</code>.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>method</code></td>
<td><code>str</code></td>
<td>
<p>Method to use.</p>
</td>
<td><em>required</em></td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>NeuralPosterior</code></td>
<td>
<p><code>NeuralPosterior</code> for chainable calls.</p>
</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Source code in <code>sbi/inference/posteriors/likelihood_based_posterior.py</code></summary>
<table class="highlighttable">
<tr>
<td class="linenos">
<div class="linenodiv">
<pre><span></span>136
137
138
139
140
141
142
143
144
145
146</pre>
</div>
</td>
<td class="code">
<div class="highlight">
<pre><span></span><code><span class="k">def</span> <span class="nf">set_mcmc_method</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">method</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">"NeuralPosterior"</span><span class="p">:</span>
    <span class="sd">"""Sets sampling method to for MCMC and returns `NeuralPosterior`.</span>

<span class="sd">    Args:</span>
<span class="sd">        method: Method to use.</span>

<span class="sd">    Returns:</span>
<span class="sd">        `NeuralPosterior` for chainable calls.</span>
<span class="sd">    """</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_mcmc_method</span> <span class="o">=</span> <span class="n">method</span>
    <span class="k">return</span> <span class="bp">self</span>
</code></pre>
</div>
</td>
</tr>
</table>
</details>
</div>
</div>
<div class="doc doc-object doc-method">
<h4 class="doc doc-heading" data-toc-label="set_mcmc_parameters()" id="sbi.inference.posteriors.likelihood_based_posterior.LikelihoodBasedPosterior.set_mcmc_parameters">
<code class="highlight language-python">
set_mcmc_parameters<span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">parameters</span><span class="p">)</span> </code>
<span class="doc doc-properties">
<small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
</span>
</h4>
<div class="doc doc-contents">
<p>Sets parameters for MCMC and returns <code>NeuralPosterior</code>.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>parameters</code></td>
<td><code>Dict[str, Any]</code></td>
<td>
<p>Dictionary overriding the default parameters for MCMC.
The following parameters are supported: <code>thin</code> to set the thinning
factor for the chain, <code>warmup_steps</code> to set the initial number of
samples to discard, <code>num_chains</code> for the number of chains,
<code>init_strategy</code> for the initialisation strategy for chains; <code>prior</code>
will draw init locations from prior, whereas <code>sir</code> will use Sequential-
Importance-Resampling using <code>init_strategy_num_candidates</code> to find init
locations.</p>
</td>
<td><em>required</em></td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>NeuralPosterior</code></td>
<td>
<p><code>NeuralPosterior</code> for chainable calls.</p>
</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Source code in <code>sbi/inference/posteriors/likelihood_based_posterior.py</code></summary>
<table class="highlighttable">
<tr>
<td class="linenos">
<div class="linenodiv">
<pre><span></span>161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178</pre>
</div>
</td>
<td class="code">
<div class="highlight">
<pre><span></span><code><span class="k">def</span> <span class="nf">set_mcmc_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">parameters</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="s2">"NeuralPosterior"</span><span class="p">:</span>
    <span class="sd">"""Sets parameters for MCMC and returns `NeuralPosterior`.</span>

<span class="sd">    Args:</span>
<span class="sd">        parameters: Dictionary overriding the default parameters for MCMC.</span>
<span class="sd">            The following parameters are supported: `thin` to set the thinning</span>
<span class="sd">            factor for the chain, `warmup_steps` to set the initial number of</span>
<span class="sd">            samples to discard, `num_chains` for the number of chains,</span>
<span class="sd">            `init_strategy` for the initialisation strategy for chains; `prior`</span>
<span class="sd">            will draw init locations from prior, whereas `sir` will use Sequential-</span>
<span class="sd">            Importance-Resampling using `init_strategy_num_candidates` to find init</span>
<span class="sd">            locations.</span>

<span class="sd">    Returns:</span>
<span class="sd">        `NeuralPosterior` for chainable calls.</span>
<span class="sd">    """</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_mcmc_parameters</span> <span class="o">=</span> <span class="n">parameters</span>
    <span class="k">return</span> <span class="bp">self</span>
</code></pre>
</div>
</td>
</tr>
</table>
</details>
</div>
</div>
</div>
</div>
</div>
<div class="doc doc-object doc-class">
<h3 class="doc doc-heading" id="sbi.inference.posteriors.ratio_based_posterior.RatioBasedPosterior">
<code>sbi.inference.posteriors.ratio_based_posterior.RatioBasedPosterior</code>
<a class="headerlink" href="#sbi.inference.posteriors.ratio_based_posterior.RatioBasedPosterior" title="Permanent link">¶</a></h3>
<div class="doc doc-contents first">
<p>Posterior <span><span class="MathJax_Preview">p(\theta|x)</span><script type="math/tex">p(\theta|x)</script></span> with <code>log_prob()</code> and <code>sample()</code> methods, obtained with
    SNRE.<br/>
<br/>

    SNRE trains a neural network to approximate likelihood ratios, which in turn can be
    used obtain an unnormalized posterior <span><span class="MathJax_Preview">p(\theta|x) \propto p(x|\theta) \cdot
    p(\theta)</span><script type="math/tex">p(\theta|x) \propto p(x|\theta) \cdot
    p(\theta)</script></span>. The <code>SNRE_Posterior</code> class wraps the trained network such that one can
    directly evaluate the unnormalized posterior log-probability <span><span class="MathJax_Preview">p(\theta|x) \propto
    p(x|\theta) \cdot p(\theta)</span><script type="math/tex">p(\theta|x) \propto
    p(x|\theta) \cdot p(\theta)</script></span> and draw samples from the posterior with
    MCMC. Note that, in the case of single-round SNRE_A / AALR, it is possible to
    evaluate the log-probability of the <strong>normalized</strong> posterior, but sampling still
    requires MCMC.<br/>
<br/>

    The neural network itself can be accessed via the <code>.net</code> attribute.</p>
<div class="doc doc-children">
<div class="doc doc-object doc-attribute">
<h4 class="doc doc-heading" data-toc-label="default_x" id="sbi.inference.posteriors.ratio_based_posterior.RatioBasedPosterior.default_x">
<code class="highlight">
default_x: <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> </code>
<span class="doc doc-properties">
<small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
<small class="doc doc-property doc-property-property"><code>property</code></small>
<small class="doc doc-property doc-property-writable"><code>writable</code></small>
</span>
</h4>
<div class="doc doc-contents">
<p>Return default x used by <code>.sample(), .log_prob</code> as conditioning context.</p>
</div>
</div>
<div class="doc doc-object doc-attribute">
<h4 class="doc doc-heading" data-toc-label="mcmc_method" id="sbi.inference.posteriors.ratio_based_posterior.RatioBasedPosterior.mcmc_method">
<code class="highlight">
mcmc_method: <span class="nb">str</span> </code>
<span class="doc doc-properties">
<small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
<small class="doc doc-property doc-property-property"><code>property</code></small>
<small class="doc doc-property doc-property-writable"><code>writable</code></small>
</span>
</h4>
<div class="doc doc-contents">
<p>Returns MCMC method.</p>
</div>
</div>
<div class="doc doc-object doc-attribute">
<h4 class="doc doc-heading" data-toc-label="mcmc_parameters" id="sbi.inference.posteriors.ratio_based_posterior.RatioBasedPosterior.mcmc_parameters">
<code class="highlight">
mcmc_parameters: <span class="nb">dict</span> </code>
<span class="doc doc-properties">
<small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
<small class="doc doc-property doc-property-property"><code>property</code></small>
<small class="doc doc-property doc-property-writable"><code>writable</code></small>
</span>
</h4>
<div class="doc doc-contents">
<p>Returns MCMC parameters.</p>
</div>
</div>
<div class="doc doc-object doc-method">
<h4 class="doc doc-heading" data-toc-label="log_prob()" id="sbi.inference.posteriors.ratio_based_posterior.RatioBasedPosterior.log_prob">
<code class="highlight language-python">
log_prob<span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">track_gradients</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> </code>
</h4>
<div class="doc doc-contents">
<p>Returns the log-probability of <span><span class="MathJax_Preview">p(x|\theta) \cdot p(\theta).</span><script type="math/tex">p(x|\theta) \cdot p(\theta).</script></span></p>
<p>This corresponds to an <strong>unnormalized</strong> posterior log-probability. Only for
single-round SNRE_A / AALR, the returned log-probability will correspond to the
<strong>normalized</strong> log-probability.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>theta</code></td>
<td><code>Tensor</code></td>
<td>
<p>Parameters <span><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span>.</p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>x</code></td>
<td><code>Optional[torch.Tensor]</code></td>
<td>
<p>Conditioning context for posterior <span><span class="MathJax_Preview">p(\theta|x)</span><script type="math/tex">p(\theta|x)</script></span>. If not provided, fall
back onto an <code>x_o</code> if previously provided for multi-round training, or
to another default if set later for convenience, see <code>.set_default_x()</code>.</p>
</td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>track_gradients</code></td>
<td><code>bool</code></td>
<td>
<p>Whether the returned tensor supports tracking gradients.
This can be helpful for e.g. sensitivity analysis, but increases memory
consumption.</p>
</td>
<td><code>False</code></td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>Tensor</code></td>
<td>
<p><code>(len(θ),)</code>-shaped log-probability <span><span class="MathJax_Preview">\log(p(x|\theta) \cdot p(\theta))</span><script type="math/tex">\log(p(x|\theta) \cdot p(\theta))</script></span>.</p>
</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Source code in <code>sbi/inference/posteriors/ratio_based_posterior.py</code></summary>
<table class="highlighttable">
<tr>
<td class="linenos">
<div class="linenodiv">
<pre><span></span> 75
 76
 77
 78
 79
 80
 81
 82
 83
 84
 85
 86
 87
 88
 89
 90
 91
 92
 93
 94
 95
 96
 97
 98
 99
100
101
102
103
104
105
106
107
108</pre>
</div>
</td>
<td class="code">
<div class="highlight">
<pre><span></span><code><span class="k">def</span> <span class="nf">log_prob</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">track_gradients</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">"""</span>
<span class="sd">    Returns the log-probability of $p(x|\theta) \cdot p(\theta).$</span>

<span class="sd">    This corresponds to an **unnormalized** posterior log-probability. Only for</span>
<span class="sd">    single-round SNRE_A / AALR, the returned log-probability will correspond to the</span>
<span class="sd">    **normalized** log-probability.</span>

<span class="sd">    Args:</span>
<span class="sd">        theta: Parameters $\theta$.</span>
<span class="sd">        x: Conditioning context for posterior $p(\theta|x)$. If not provided, fall</span>
<span class="sd">            back onto an `x_o` if previously provided for multi-round training, or</span>
<span class="sd">            to another default if set later for convenience, see `.set_default_x()`.</span>
<span class="sd">        track_gradients: Whether the returned tensor supports tracking gradients.</span>
<span class="sd">            This can be helpful for e.g. sensitivity analysis, but increases memory</span>
<span class="sd">            consumption.</span>

<span class="sd">    Returns:</span>
<span class="sd">        `(len(θ),)`-shaped log-probability $\log(p(x|\theta) \cdot p(\theta))$.</span>

<span class="sd">    """</span>

    <span class="c1"># TODO Train exited here, entered after sampling?</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

    <span class="n">theta</span><span class="p">,</span> <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_theta_and_x_for_log_prob_</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_warn_log_prob_snre</span><span class="p">()</span>

    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">set_grad_enabled</span><span class="p">(</span><span class="n">track_gradients</span><span class="p">):</span>
        <span class="n">log_ratio</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">log_ratio</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>
</code></pre>
</div>
</td>
</tr>
</table>
</details>
</div>
</div>
<div class="doc doc-object doc-method">
<h4 class="doc doc-heading" data-toc-label="sample()" id="sbi.inference.posteriors.ratio_based_posterior.RatioBasedPosterior.sample">
<code class="highlight language-python">
sample<span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sample_shape</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([]),</span> <span class="n">x</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">show_progress_bars</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sample_with_mcmc</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">mcmc_method</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">mcmc_parameters</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span> </code>
</h4>
<div class="doc doc-contents">
<p>Return samples from posterior distribution <span><span class="MathJax_Preview">p(\theta|x)</span><script type="math/tex">p(\theta|x)</script></span> with MCMC.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>sample_shape</code></td>
<td><code>Union[torch.Size, Tuple[int, ...]]</code></td>
<td>
<p>Desired shape of samples that are drawn from posterior. If
sample_shape is multidimensional we simply draw <code>sample_shape.numel()</code>
samples and then reshape into the desired shape.</p>
</td>
<td><code>torch.Size([])</code></td>
</tr>
<tr>
<td><code>x</code></td>
<td><code>Optional[torch.Tensor]</code></td>
<td>
<p>Conditioning context for posterior <span><span class="MathJax_Preview">p(\theta|x)</span><script type="math/tex">p(\theta|x)</script></span>. If not provided,
fall back onto <code>x_o</code> if previously provided for multiround training, or
to a set default (see <code>set_default_x()</code> method).</p>
</td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>show_progress_bars</code></td>
<td><code>bool</code></td>
<td>
<p>Whether to show sampling progress monitor.</p>
</td>
<td><code>True</code></td>
</tr>
<tr>
<td><code>sample_with_mcmc</code></td>
<td><code>Optional[bool]</code></td>
<td>
<p>Optional parameter to override <code>self.sample_with_mcmc</code>.</p>
</td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>mcmc_method</code></td>
<td><code>Optional[str]</code></td>
<td>
<p>Optional parameter to override <code>self.mcmc_method</code>.</p>
</td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>mcmc_parameters</code></td>
<td><code>Optional[Dict[str, Any]]</code></td>
<td>
<p>Dictionary overriding the default parameters for MCMC.
The following parameters are supported: <code>thin</code> to set the thinning
factor for the chain, <code>warmup_steps</code> to set the initial number of
samples to discard, <code>num_chains</code> for the number of chains,
<code>init_strategy</code> for the initialisation strategy for chains; <code>prior</code>
will draw init locations from prior, whereas <code>sir</code> will use Sequential-
Importance-Resampling using <code>init_strategy_num_candidates</code> to find init
locations.</p>
</td>
<td><code>None</code></td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>Tensor</code></td>
<td>
<p>Samples from posterior.</p>
</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Source code in <code>sbi/inference/posteriors/ratio_based_posterior.py</code></summary>
<table class="highlighttable">
<tr>
<td class="linenos">
<div class="linenodiv">
<pre><span></span>123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170</pre>
</div>
</td>
<td class="code">
<div class="highlight">
<pre><span></span><code><span class="k">def</span> <span class="nf">sample</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">sample_shape</span><span class="p">:</span> <span class="n">Shape</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(),</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">sample_with_mcmc</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">mcmc_method</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">mcmc_parameters</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">"""</span>
<span class="sd">    Return samples from posterior distribution $p(\theta|x)$ with MCMC.</span>

<span class="sd">    Args:</span>
<span class="sd">        sample_shape: Desired shape of samples that are drawn from posterior. If</span>
<span class="sd">            sample_shape is multidimensional we simply draw `sample_shape.numel()`</span>
<span class="sd">            samples and then reshape into the desired shape.</span>
<span class="sd">        x: Conditioning context for posterior $p(\theta|x)$. If not provided,</span>
<span class="sd">            fall back onto `x_o` if previously provided for multiround training, or</span>
<span class="sd">            to a set default (see `set_default_x()` method).</span>
<span class="sd">        show_progress_bars: Whether to show sampling progress monitor.</span>
<span class="sd">        sample_with_mcmc: Optional parameter to override `self.sample_with_mcmc`.</span>
<span class="sd">        mcmc_method: Optional parameter to override `self.mcmc_method`.</span>
<span class="sd">        mcmc_parameters: Dictionary overriding the default parameters for MCMC.</span>
<span class="sd">            The following parameters are supported: `thin` to set the thinning</span>
<span class="sd">            factor for the chain, `warmup_steps` to set the initial number of</span>
<span class="sd">            samples to discard, `num_chains` for the number of chains,</span>
<span class="sd">            `init_strategy` for the initialisation strategy for chains; `prior`</span>
<span class="sd">            will draw init locations from prior, whereas `sir` will use Sequential-</span>
<span class="sd">            Importance-Resampling using `init_strategy_num_candidates` to find init</span>
<span class="sd">            locations.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Samples from posterior.</span>
<span class="sd">    """</span>

    <span class="n">x</span><span class="p">,</span> <span class="n">num_samples</span><span class="p">,</span> <span class="n">mcmc_method</span><span class="p">,</span> <span class="n">mcmc_parameters</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_for_sample</span><span class="p">(</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">sample_shape</span><span class="p">,</span> <span class="n">mcmc_method</span><span class="p">,</span> <span class="n">mcmc_parameters</span>
    <span class="p">)</span>

    <span class="n">samples</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sample_posterior_mcmc</span><span class="p">(</span>
        <span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span>
        <span class="n">num_samples</span><span class="o">=</span><span class="n">num_samples</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="o">=</span><span class="n">show_progress_bars</span><span class="p">,</span>
        <span class="n">mcmc_method</span><span class="o">=</span><span class="n">mcmc_method</span><span class="p">,</span>
        <span class="o">**</span><span class="n">mcmc_parameters</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">samples</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">*</span><span class="n">sample_shape</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
</code></pre>
</div>
</td>
</tr>
</table>
</details>
</div>
</div>
<div class="doc doc-object doc-method">
<h4 class="doc doc-heading" data-toc-label="set_default_x()" id="sbi.inference.posteriors.ratio_based_posterior.RatioBasedPosterior.set_default_x">
<code class="highlight language-python">
set_default_x<span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> </code>
<span class="doc doc-properties">
<small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
</span>
</h4>
<div class="doc doc-contents">
<p>Set new default x for <code>.sample(), .log_prob</code> to use as conditioning context.</p>
<p>This is a pure convenience to avoid having to repeatedly specify <code>x</code> in calls to
<code>.sample()</code> and <code>.log_prob()</code> - only θ needs to be passed.</p>
<p>This convenience is particularly useful when the posterior is focused, i.e.
has been trained over multiple rounds to be accurate in the vicinity of a
particular <code>x=x_o</code> (you can check if your posterior object is focused by
printing it).</p>
<p>NOTE: this method is chainable, i.e. will return the NeuralPosterior object so
that calls like <code>posterior.set_default_x(my_x).sample(mytheta)</code> are possible.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>x</code></td>
<td><code>Tensor</code></td>
<td>
<p>The default observation to set for the posterior <span><span class="MathJax_Preview">p(theta|x)</span><script type="math/tex">p(theta|x)</script></span>.</p>
</td>
<td><em>required</em></td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>NeuralPosterior</code></td>
<td>
<p><code>NeuralPosterior</code> that will use a default <code>x</code> when not explicitly passed.</p>
</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Source code in <code>sbi/inference/posteriors/ratio_based_posterior.py</code></summary>
<table class="highlighttable">
<tr>
<td class="linenos">
<div class="linenodiv">
<pre><span></span>101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124</pre>
</div>
</td>
<td class="code">
<div class="highlight">
<pre><span></span><code><span class="k">def</span> <span class="nf">set_default_x</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">"NeuralPosterior"</span><span class="p">:</span>
    <span class="sd">"""Set new default x for `.sample(), .log_prob` to use as conditioning context.</span>

<span class="sd">    This is a pure convenience to avoid having to repeatedly specify `x` in calls to</span>
<span class="sd">    `.sample()` and `.log_prob()` - only θ needs to be passed.</span>

<span class="sd">    This convenience is particularly useful when the posterior is focused, i.e.</span>
<span class="sd">    has been trained over multiple rounds to be accurate in the vicinity of a</span>
<span class="sd">    particular `x=x_o` (you can check if your posterior object is focused by</span>
<span class="sd">    printing it).</span>

<span class="sd">    NOTE: this method is chainable, i.e. will return the NeuralPosterior object so</span>
<span class="sd">    that calls like `posterior.set_default_x(my_x).sample(mytheta)` are possible.</span>

<span class="sd">    Args:</span>
<span class="sd">        x: The default observation to set for the posterior $p(theta|x)$.</span>

<span class="sd">    Returns:</span>
<span class="sd">        `NeuralPosterior` that will use a default `x` when not explicitly passed.</span>
<span class="sd">    """</span>
    <span class="n">processed_x</span> <span class="o">=</span> <span class="n">process_x</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_x_shape</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_x</span> <span class="o">=</span> <span class="n">processed_x</span>

    <span class="k">return</span> <span class="bp">self</span>
</code></pre>
</div>
</td>
</tr>
</table>
</details>
</div>
</div>
<div class="doc doc-object doc-method">
<h4 class="doc doc-heading" data-toc-label="set_mcmc_method()" id="sbi.inference.posteriors.ratio_based_posterior.RatioBasedPosterior.set_mcmc_method">
<code class="highlight language-python">
set_mcmc_method<span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">method</span><span class="p">)</span> </code>
<span class="doc doc-properties">
<small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
</span>
</h4>
<div class="doc doc-contents">
<p>Sets sampling method to for MCMC and returns <code>NeuralPosterior</code>.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>method</code></td>
<td><code>str</code></td>
<td>
<p>Method to use.</p>
</td>
<td><em>required</em></td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>NeuralPosterior</code></td>
<td>
<p><code>NeuralPosterior</code> for chainable calls.</p>
</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Source code in <code>sbi/inference/posteriors/ratio_based_posterior.py</code></summary>
<table class="highlighttable">
<tr>
<td class="linenos">
<div class="linenodiv">
<pre><span></span>136
137
138
139
140
141
142
143
144
145
146</pre>
</div>
</td>
<td class="code">
<div class="highlight">
<pre><span></span><code><span class="k">def</span> <span class="nf">set_mcmc_method</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">method</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">"NeuralPosterior"</span><span class="p">:</span>
    <span class="sd">"""Sets sampling method to for MCMC and returns `NeuralPosterior`.</span>

<span class="sd">    Args:</span>
<span class="sd">        method: Method to use.</span>

<span class="sd">    Returns:</span>
<span class="sd">        `NeuralPosterior` for chainable calls.</span>
<span class="sd">    """</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_mcmc_method</span> <span class="o">=</span> <span class="n">method</span>
    <span class="k">return</span> <span class="bp">self</span>
</code></pre>
</div>
</td>
</tr>
</table>
</details>
</div>
</div>
<div class="doc doc-object doc-method">
<h4 class="doc doc-heading" data-toc-label="set_mcmc_parameters()" id="sbi.inference.posteriors.ratio_based_posterior.RatioBasedPosterior.set_mcmc_parameters">
<code class="highlight language-python">
set_mcmc_parameters<span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">parameters</span><span class="p">)</span> </code>
<span class="doc doc-properties">
<small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
</span>
</h4>
<div class="doc doc-contents">
<p>Sets parameters for MCMC and returns <code>NeuralPosterior</code>.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>parameters</code></td>
<td><code>Dict[str, Any]</code></td>
<td>
<p>Dictionary overriding the default parameters for MCMC.
The following parameters are supported: <code>thin</code> to set the thinning
factor for the chain, <code>warmup_steps</code> to set the initial number of
samples to discard, <code>num_chains</code> for the number of chains,
<code>init_strategy</code> for the initialisation strategy for chains; <code>prior</code>
will draw init locations from prior, whereas <code>sir</code> will use Sequential-
Importance-Resampling using <code>init_strategy_num_candidates</code> to find init
locations.</p>
</td>
<td><em>required</em></td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>NeuralPosterior</code></td>
<td>
<p><code>NeuralPosterior</code> for chainable calls.</p>
</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Source code in <code>sbi/inference/posteriors/ratio_based_posterior.py</code></summary>
<table class="highlighttable">
<tr>
<td class="linenos">
<div class="linenodiv">
<pre><span></span>161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178</pre>
</div>
</td>
<td class="code">
<div class="highlight">
<pre><span></span><code><span class="k">def</span> <span class="nf">set_mcmc_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">parameters</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="s2">"NeuralPosterior"</span><span class="p">:</span>
    <span class="sd">"""Sets parameters for MCMC and returns `NeuralPosterior`.</span>

<span class="sd">    Args:</span>
<span class="sd">        parameters: Dictionary overriding the default parameters for MCMC.</span>
<span class="sd">            The following parameters are supported: `thin` to set the thinning</span>
<span class="sd">            factor for the chain, `warmup_steps` to set the initial number of</span>
<span class="sd">            samples to discard, `num_chains` for the number of chains,</span>
<span class="sd">            `init_strategy` for the initialisation strategy for chains; `prior`</span>
<span class="sd">            will draw init locations from prior, whereas `sir` will use Sequential-</span>
<span class="sd">            Importance-Resampling using `init_strategy_num_candidates` to find init</span>
<span class="sd">            locations.</span>

<span class="sd">    Returns:</span>
<span class="sd">        `NeuralPosterior` for chainable calls.</span>
<span class="sd">    """</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_mcmc_parameters</span> <span class="o">=</span> <span class="n">parameters</span>
    <span class="k">return</span> <span class="bp">self</span>
</code></pre>
</div>
</td>
</tr>
</table>
</details>
</div>
</div>
</div>
</div>
</div>
<h2 id="models">Models<a class="headerlink" href="#models" title="Permanent link">¶</a></h2>
<div class="doc doc-object doc-function">
<h3 class="doc doc-heading" id="sbi.utils.get_nn_models.posterior_nn">
<code class="highlight language-python">
sbi.utils.get_nn_models.posterior_nn<span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">z_score_theta</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">z_score_x</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">hidden_features</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">num_transforms</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">embedding_net</span><span class="o">=</span><span class="n">Identity</span><span class="p">(),</span> <span class="n">num_components</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span> </code>
<a class="headerlink" href="#sbi.utils.get_nn_models.posterior_nn" title="Permanent link">¶</a></h3>
<div class="doc doc-contents first">
<p>Returns a function that builds a density estimator for learning the posterior.</p>
<p>This function will usually be used for SNPE. The returned function is to be passed
to the inference class when using the flexible interface.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>model</code></td>
<td><code>str</code></td>
<td>
<p>The type of density estimator that will be created. One of [<code>mdn</code>,
<code>made</code>, <code>maf</code>, <code>nsf</code>].</p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>z_score_theta</code></td>
<td><code>bool</code></td>
<td>
<p>Whether to z-score parameters <span><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span> before passing them into
the network.</p>
</td>
<td><code>True</code></td>
</tr>
<tr>
<td><code>z_score_x</code></td>
<td><code>bool</code></td>
<td>
<p>Whether to z-score simulation outputs <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> before passing them into
the network.</p>
</td>
<td><code>True</code></td>
</tr>
<tr>
<td><code>hidden_features</code></td>
<td><code>int</code></td>
<td>
<p>Number of hidden features.</p>
</td>
<td><code>50</code></td>
</tr>
<tr>
<td><code>num_transforms</code></td>
<td><code>int</code></td>
<td>
<p>Number of transforms when a flow is used. Only relevant if
density estimator is a normalizing flow (i.e. currently either a <code>maf</code> or a
<code>nsf</code>). Ignored if density estimator is a <code>mdn</code> or <code>made</code>.</p>
</td>
<td><code>5</code></td>
</tr>
<tr>
<td><code>embedding_net</code></td>
<td><code>Module</code></td>
<td>
<p>Optional embedding network for simulation outputs <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span>. This
embedding net allows to learn features from potentially high-dimensional
simulation outputs.</p>
</td>
<td><code>Identity()</code></td>
</tr>
<tr>
<td><code>num_components</code></td>
<td><code>int</code></td>
<td>
<p>Number of mixture components for a mixture of Gaussians.
Ignored if density estimator is not an mdn.</p>
</td>
<td><code>10</code></td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Source code in <code>sbi/utils/get_nn_models.py</code></summary>
<table class="highlighttable">
<tr>
<td class="linenos">
<div class="linenodiv">
<pre><span></span>160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226</pre>
</div>
</td>
<td class="code">
<div class="highlight">
<pre><span></span><code><span class="k">def</span> <span class="nf">posterior_nn</span><span class="p">(</span>
    <span class="n">model</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">z_score_theta</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">z_score_x</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">hidden_features</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span>
    <span class="n">num_transforms</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span>
    <span class="n">embedding_net</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">(),</span>
    <span class="n">num_components</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Callable</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">"""</span>
<span class="sd">    Returns a function that builds a density estimator for learning the posterior.</span>

<span class="sd">    This function will usually be used for SNPE. The returned function is to be passed</span>
<span class="sd">    to the inference class when using the flexible interface.</span>

<span class="sd">    Args:</span>
<span class="sd">        model: The type of density estimator that will be created. One of [`mdn`,</span>
<span class="sd">            `made`, `maf`, `nsf`].</span>
<span class="sd">        z_score_theta: Whether to z-score parameters $\theta$ before passing them into</span>
<span class="sd">            the network.</span>
<span class="sd">        z_score_x: Whether to z-score simulation outputs $x$ before passing them into</span>
<span class="sd">            the network.</span>
<span class="sd">        hidden_features: Number of hidden features.</span>
<span class="sd">        num_transforms: Number of transforms when a flow is used. Only relevant if</span>
<span class="sd">            density estimator is a normalizing flow (i.e. currently either a `maf` or a</span>
<span class="sd">            `nsf`). Ignored if density estimator is a `mdn` or `made`.</span>
<span class="sd">        embedding_net: Optional embedding network for simulation outputs $x$. This</span>
<span class="sd">            embedding net allows to learn features from potentially high-dimensional</span>
<span class="sd">            simulation outputs.</span>
<span class="sd">        num_components: Number of mixture components for a mixture of Gaussians.</span>
<span class="sd">            Ignored if density estimator is not an mdn.</span>
<span class="sd">    """</span>

    <span class="n">kwargs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
        <span class="nb">zip</span><span class="p">(</span>
            <span class="p">(</span>
                <span class="s2">"z_score_x"</span><span class="p">,</span>
                <span class="s2">"z_score_y"</span><span class="p">,</span>
                <span class="s2">"hidden_features"</span><span class="p">,</span>
                <span class="s2">"num_transforms"</span><span class="p">,</span>
                <span class="s2">"embedding_net"</span><span class="p">,</span>
                <span class="s2">"num_components"</span><span class="p">,</span>
            <span class="p">),</span>
            <span class="p">(</span>
                <span class="n">z_score_theta</span><span class="p">,</span>
                <span class="n">z_score_x</span><span class="p">,</span>
                <span class="n">hidden_features</span><span class="p">,</span>
                <span class="n">num_transforms</span><span class="p">,</span>
                <span class="n">embedding_net</span><span class="p">,</span>
                <span class="n">num_components</span><span class="p">,</span>
            <span class="p">),</span>
        <span class="p">)</span>
    <span class="p">)</span>

    <span class="k">def</span> <span class="nf">build_fn</span><span class="p">(</span><span class="n">batch_theta</span><span class="p">,</span> <span class="n">batch_x</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">model</span> <span class="o">==</span> <span class="s2">"mdn"</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">build_mdn</span><span class="p">(</span><span class="n">batch_x</span><span class="o">=</span><span class="n">batch_theta</span><span class="p">,</span> <span class="n">batch_y</span><span class="o">=</span><span class="n">batch_x</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">model</span> <span class="o">==</span> <span class="s2">"made"</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">build_made</span><span class="p">(</span><span class="n">batch_x</span><span class="o">=</span><span class="n">batch_theta</span><span class="p">,</span> <span class="n">batch_y</span><span class="o">=</span><span class="n">batch_x</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">model</span> <span class="o">==</span> <span class="s2">"maf"</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">build_maf</span><span class="p">(</span><span class="n">batch_x</span><span class="o">=</span><span class="n">batch_theta</span><span class="p">,</span> <span class="n">batch_y</span><span class="o">=</span><span class="n">batch_x</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">model</span> <span class="o">==</span> <span class="s2">"nsf"</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">build_nsf</span><span class="p">(</span><span class="n">batch_x</span><span class="o">=</span><span class="n">batch_theta</span><span class="p">,</span> <span class="n">batch_y</span><span class="o">=</span><span class="n">batch_x</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="k">return</span> <span class="n">build_fn</span>
</code></pre>
</div>
</td>
</tr>
</table>
</details>
</div>
</div>
<div class="doc doc-object doc-function">
<h3 class="doc doc-heading" id="sbi.utils.get_nn_models.likelihood_nn">
<code class="highlight language-python">
sbi.utils.get_nn_models.likelihood_nn<span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">z_score_theta</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">z_score_x</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">hidden_features</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">num_transforms</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">embedding_net</span><span class="o">=</span><span class="n">Identity</span><span class="p">(),</span> <span class="n">num_components</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span> </code>
<a class="headerlink" href="#sbi.utils.get_nn_models.likelihood_nn" title="Permanent link">¶</a></h3>
<div class="doc doc-contents first">
<p>Returns a function that builds a density estimator for learning the likelihood.</p>
<p>This function will usually be used for SNLE. The returned function is to be passed
to the inference class when using the flexible interface.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>model</code></td>
<td><code>str</code></td>
<td>
<p>The type of density estimator that will be created. One of [<code>mdn</code>,
<code>made</code>, <code>maf</code>, <code>nsf</code>].</p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>z_score_theta</code></td>
<td><code>bool</code></td>
<td>
<p>Whether to z-score parameters <span><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span> before passing them into
the network.</p>
</td>
<td><code>True</code></td>
</tr>
<tr>
<td><code>z_score_x</code></td>
<td><code>bool</code></td>
<td>
<p>Whether to z-score simulation outputs <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> before passing them into
the network.</p>
</td>
<td><code>True</code></td>
</tr>
<tr>
<td><code>hidden_features</code></td>
<td><code>int</code></td>
<td>
<p>Number of hidden features.</p>
</td>
<td><code>50</code></td>
</tr>
<tr>
<td><code>num_transforms</code></td>
<td><code>int</code></td>
<td>
<p>Number of transforms when a flow is used. Only relevant if
density estimator is a normalizing flow (i.e. currently either a <code>maf</code> or a
<code>nsf</code>). Ignored if density estimator is a <code>mdn</code> or <code>made</code>.</p>
</td>
<td><code>5</code></td>
</tr>
<tr>
<td><code>embedding_net</code></td>
<td><code>Module</code></td>
<td>
<p>Optional embedding network for parameters <span><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span>.</p>
</td>
<td><code>Identity()</code></td>
</tr>
<tr>
<td><code>num_components</code></td>
<td><code>int</code></td>
<td>
<p>Number of mixture components for a mixture of Gaussians.
Ignored if density estimator is not an mdn.</p>
</td>
<td><code>10</code></td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Source code in <code>sbi/utils/get_nn_models.py</code></summary>
<table class="highlighttable">
<tr>
<td class="linenos">
<div class="linenodiv">
<pre><span></span> 93
 94
 95
 96
 97
 98
 99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157</pre>
</div>
</td>
<td class="code">
<div class="highlight">
<pre><span></span><code><span class="k">def</span> <span class="nf">likelihood_nn</span><span class="p">(</span>
    <span class="n">model</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">z_score_theta</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">z_score_x</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">hidden_features</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span>
    <span class="n">num_transforms</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span>
    <span class="n">embedding_net</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">(),</span>
    <span class="n">num_components</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Callable</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">"""</span>
<span class="sd">    Returns a function that builds a density estimator for learning the likelihood.</span>

<span class="sd">    This function will usually be used for SNLE. The returned function is to be passed</span>
<span class="sd">    to the inference class when using the flexible interface.</span>

<span class="sd">    Args:</span>
<span class="sd">        model: The type of density estimator that will be created. One of [`mdn`,</span>
<span class="sd">            `made`, `maf`, `nsf`].</span>
<span class="sd">        z_score_theta: Whether to z-score parameters $\theta$ before passing them into</span>
<span class="sd">            the network.</span>
<span class="sd">        z_score_x: Whether to z-score simulation outputs $x$ before passing them into</span>
<span class="sd">            the network.</span>
<span class="sd">        hidden_features: Number of hidden features.</span>
<span class="sd">        num_transforms: Number of transforms when a flow is used. Only relevant if</span>
<span class="sd">            density estimator is a normalizing flow (i.e. currently either a `maf` or a</span>
<span class="sd">            `nsf`). Ignored if density estimator is a `mdn` or `made`.</span>
<span class="sd">        embedding_net: Optional embedding network for parameters $\theta$.</span>
<span class="sd">        num_components: Number of mixture components for a mixture of Gaussians.</span>
<span class="sd">            Ignored if density estimator is not an mdn.</span>
<span class="sd">    """</span>

    <span class="n">kwargs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
        <span class="nb">zip</span><span class="p">(</span>
            <span class="p">(</span>
                <span class="s2">"z_score_x"</span><span class="p">,</span>
                <span class="s2">"z_score_y"</span><span class="p">,</span>
                <span class="s2">"hidden_features"</span><span class="p">,</span>
                <span class="s2">"num_transforms"</span><span class="p">,</span>
                <span class="s2">"embedding_net"</span><span class="p">,</span>
                <span class="s2">"num_components"</span><span class="p">,</span>
            <span class="p">),</span>
            <span class="p">(</span>
                <span class="n">z_score_x</span><span class="p">,</span>
                <span class="n">z_score_theta</span><span class="p">,</span>
                <span class="n">hidden_features</span><span class="p">,</span>
                <span class="n">num_transforms</span><span class="p">,</span>
                <span class="n">embedding_net</span><span class="p">,</span>
                <span class="n">num_components</span><span class="p">,</span>
            <span class="p">),</span>
        <span class="p">)</span>
    <span class="p">)</span>

    <span class="k">def</span> <span class="nf">build_fn</span><span class="p">(</span><span class="n">batch_theta</span><span class="p">,</span> <span class="n">batch_x</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">model</span> <span class="o">==</span> <span class="s2">"mdn"</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">build_mdn</span><span class="p">(</span><span class="n">batch_x</span><span class="o">=</span><span class="n">batch_x</span><span class="p">,</span> <span class="n">batch_y</span><span class="o">=</span><span class="n">batch_theta</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">model</span> <span class="o">==</span> <span class="s2">"made"</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">build_made</span><span class="p">(</span><span class="n">batch_x</span><span class="o">=</span><span class="n">batch_x</span><span class="p">,</span> <span class="n">batch_y</span><span class="o">=</span><span class="n">batch_theta</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">model</span> <span class="o">==</span> <span class="s2">"maf"</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">build_maf</span><span class="p">(</span><span class="n">batch_x</span><span class="o">=</span><span class="n">batch_x</span><span class="p">,</span> <span class="n">batch_y</span><span class="o">=</span><span class="n">batch_theta</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">model</span> <span class="o">==</span> <span class="s2">"nsf"</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">build_nsf</span><span class="p">(</span><span class="n">batch_x</span><span class="o">=</span><span class="n">batch_x</span><span class="p">,</span> <span class="n">batch_y</span><span class="o">=</span><span class="n">batch_theta</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="k">return</span> <span class="n">build_fn</span>
</code></pre>
</div>
</td>
</tr>
</table>
</details>
</div>
</div>
<div class="doc doc-object doc-function">
<h3 class="doc doc-heading" id="sbi.utils.get_nn_models.classifier_nn">
<code class="highlight language-python">
sbi.utils.get_nn_models.classifier_nn<span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">z_score_theta</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">z_score_x</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">hidden_features</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">embedding_net_theta</span><span class="o">=</span><span class="n">Identity</span><span class="p">(),</span> <span class="n">embedding_net_x</span><span class="o">=</span><span class="n">Identity</span><span class="p">())</span> </code>
<a class="headerlink" href="#sbi.utils.get_nn_models.classifier_nn" title="Permanent link">¶</a></h3>
<div class="doc doc-contents first">
<p>Returns a function that builds a classifier for learning density ratios.</p>
<p>This function will usually be used for SNRE. The returned function is to be passed
to the inference class when using the flexible interface.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>model</code></td>
<td><code>str</code></td>
<td>
<p>The type of classifier that will be created. One of [<code>linear</code>, <code>mlp</code>,
<code>resnet</code>].</p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>z_score_theta</code></td>
<td><code>bool</code></td>
<td>
<p>Whether to z-score parameters <span><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span> before passing them into
the network.</p>
</td>
<td><code>True</code></td>
</tr>
<tr>
<td><code>z_score_x</code></td>
<td><code>bool</code></td>
<td>
<p>Whether to z-score simulation outputs <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> before passing them into
the network.</p>
</td>
<td><code>True</code></td>
</tr>
<tr>
<td><code>hidden_features</code></td>
<td><code>int</code></td>
<td>
<p>Number of hidden features.</p>
</td>
<td><code>50</code></td>
</tr>
<tr>
<td><code>embedding_net_theta</code></td>
<td><code>Module</code></td>
<td>
<p>Optional embedding network for parameters <span><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span>.</p>
</td>
<td><code>Identity()</code></td>
</tr>
<tr>
<td><code>embedding_net_x</code></td>
<td><code>Module</code></td>
<td>
<p>Optional embedding network for simulation outputs <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span>. This
embedding net allows to learn features from potentially high-dimensional
simulation outputs.</p>
</td>
<td><code>Identity()</code></td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Source code in <code>sbi/utils/get_nn_models.py</code></summary>
<table class="highlighttable">
<tr>
<td class="linenos">
<div class="linenodiv">
<pre><span></span>29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90</pre>
</div>
</td>
<td class="code">
<div class="highlight">
<pre><span></span><code><span class="k">def</span> <span class="nf">classifier_nn</span><span class="p">(</span>
    <span class="n">model</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">z_score_theta</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">z_score_x</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">hidden_features</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span>
    <span class="n">embedding_net_theta</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">(),</span>
    <span class="n">embedding_net_x</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">(),</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Callable</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">"""</span>
<span class="sd">    Returns a function that builds a classifier for learning density ratios.</span>

<span class="sd">    This function will usually be used for SNRE. The returned function is to be passed</span>
<span class="sd">    to the inference class when using the flexible interface.</span>

<span class="sd">    Args:</span>
<span class="sd">        model: The type of classifier that will be created. One of [`linear`, `mlp`,</span>
<span class="sd">            `resnet`].</span>
<span class="sd">        z_score_theta: Whether to z-score parameters $\theta$ before passing them into</span>
<span class="sd">            the network.</span>
<span class="sd">        z_score_x: Whether to z-score simulation outputs $x$ before passing them into</span>
<span class="sd">            the network.</span>
<span class="sd">        hidden_features: Number of hidden features.</span>
<span class="sd">        embedding_net_theta:  Optional embedding network for parameters $\theta$.</span>
<span class="sd">        embedding_net_x:  Optional embedding network for simulation outputs $x$. This</span>
<span class="sd">            embedding net allows to learn features from potentially high-dimensional</span>
<span class="sd">            simulation outputs.</span>
<span class="sd">    """</span>

    <span class="n">kwargs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
        <span class="nb">zip</span><span class="p">(</span>
            <span class="p">(</span>
                <span class="s2">"z_score_x"</span><span class="p">,</span>
                <span class="s2">"z_score_y"</span><span class="p">,</span>
                <span class="s2">"hidden_features"</span><span class="p">,</span>
                <span class="s2">"embedding_net_x"</span><span class="p">,</span>
                <span class="s2">"embedding_net_y"</span><span class="p">,</span>
            <span class="p">),</span>
            <span class="p">(</span>
                <span class="n">z_score_x</span><span class="p">,</span>
                <span class="n">z_score_theta</span><span class="p">,</span>
                <span class="n">hidden_features</span><span class="p">,</span>
                <span class="n">embedding_net_x</span><span class="p">,</span>
                <span class="n">embedding_net_theta</span><span class="p">,</span>
            <span class="p">),</span>
        <span class="p">)</span>
    <span class="p">)</span>

    <span class="k">def</span> <span class="nf">build_fn</span><span class="p">(</span><span class="n">batch_theta</span><span class="p">,</span> <span class="n">batch_x</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">model</span> <span class="o">==</span> <span class="s2">"linear"</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">build_linear_classifier</span><span class="p">(</span>
                <span class="n">batch_x</span><span class="o">=</span><span class="n">batch_x</span><span class="p">,</span> <span class="n">batch_y</span><span class="o">=</span><span class="n">batch_theta</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">model</span> <span class="o">==</span> <span class="s2">"mlp"</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">build_mlp_classifier</span><span class="p">(</span><span class="n">batch_x</span><span class="o">=</span><span class="n">batch_x</span><span class="p">,</span> <span class="n">batch_y</span><span class="o">=</span><span class="n">batch_theta</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">model</span> <span class="o">==</span> <span class="s2">"resnet"</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">build_resnet_classifier</span><span class="p">(</span>
                <span class="n">batch_x</span><span class="o">=</span><span class="n">batch_x</span><span class="p">,</span> <span class="n">batch_y</span><span class="o">=</span><span class="n">batch_theta</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="k">return</span> <span class="n">build_fn</span>
</code></pre>
</div>
</td>
</tr>
</table>
</details>
</div>
</div>
<h2 id="utils">Utils<a class="headerlink" href="#utils" title="Permanent link">¶</a></h2>
<div class="doc doc-object doc-function">
<h3 class="doc doc-heading" id="sbi.utils.plot.pairplot">
<code class="highlight language-python">
sbi.utils.plot.pairplot<span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">points</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">upper</span><span class="o">=</span><span class="s1">'hist'</span><span class="p">,</span> <span class="n">diag</span><span class="o">=</span><span class="s1">'hist'</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">legend</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">labels_points</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">labels_samples</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">samples_colors</span><span class="o">=</span><span class="p">[</span><span class="s1">'#1f77b4'</span><span class="p">,</span> <span class="s1">'#ff7f0e'</span><span class="p">,</span> <span class="s1">'#2ca02c'</span><span class="p">,</span> <span class="s1">'#d62728'</span><span class="p">,</span> <span class="s1">'#9467bd'</span><span class="p">,</span> <span class="s1">'#8c564b'</span><span class="p">,</span> <span class="s1">'#e377c2'</span><span class="p">,</span> <span class="s1">'#7f7f7f'</span><span class="p">,</span> <span class="s1">'#bcbd22'</span><span class="p">,</span> <span class="s1">'#17becf'</span><span class="p">],</span> <span class="n">points_colors</span><span class="o">=</span><span class="p">[</span><span class="s1">'#1f77b4'</span><span class="p">,</span> <span class="s1">'#ff7f0e'</span><span class="p">,</span> <span class="s1">'#2ca02c'</span><span class="p">,</span> <span class="s1">'#d62728'</span><span class="p">,</span> <span class="s1">'#9467bd'</span><span class="p">,</span> <span class="s1">'#8c564b'</span><span class="p">,</span> <span class="s1">'#e377c2'</span><span class="p">,</span> <span class="s1">'#7f7f7f'</span><span class="p">,</span> <span class="s1">'#bcbd22'</span><span class="p">,</span> <span class="s1">'#17becf'</span><span class="p">],</span> <span class="n">subset</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">limits</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ticks</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">tickformatter</span><span class="o">=&lt;</span><span class="n">matplotlib</span><span class="o">.</span><span class="n">ticker</span><span class="o">.</span><span class="n">FormatStrFormatter</span> <span class="nb">object</span> <span class="n">at</span> <span class="mh">0x121afa820</span><span class="o">&gt;</span><span class="p">,</span> <span class="n">tick_labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">hist_diag</span><span class="o">=</span><span class="p">{</span><span class="s1">'alpha'</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span> <span class="s1">'bins'</span><span class="p">:</span> <span class="mi">50</span><span class="p">,</span> <span class="s1">'density'</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span> <span class="s1">'histtype'</span><span class="p">:</span> <span class="s1">'step'</span><span class="p">},</span> <span class="n">hist_offdiag</span><span class="o">=</span><span class="p">{</span><span class="s1">'bins'</span><span class="p">:</span> <span class="mi">50</span><span class="p">},</span> <span class="n">kde_diag</span><span class="o">=</span><span class="p">{</span><span class="s1">'bw_method'</span><span class="p">:</span> <span class="s1">'scott'</span><span class="p">,</span> <span class="s1">'bins'</span><span class="p">:</span> <span class="mi">50</span><span class="p">,</span> <span class="s1">'color'</span><span class="p">:</span> <span class="s1">'black'</span><span class="p">},</span> <span class="n">kde_offdiag</span><span class="o">=</span><span class="p">{</span><span class="s1">'bw_method'</span><span class="p">:</span> <span class="s1">'scott'</span><span class="p">,</span> <span class="s1">'bins'</span><span class="p">:</span> <span class="mi">50</span><span class="p">},</span> <span class="n">contour_offdiag</span><span class="o">=</span><span class="p">{</span><span class="s1">'levels'</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.68</span><span class="p">],</span> <span class="s1">'percentile'</span><span class="p">:</span> <span class="kc">True</span><span class="p">},</span> <span class="n">scatter_offdiag</span><span class="o">=</span><span class="p">{</span><span class="s1">'alpha'</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">,</span> <span class="s1">'edgecolor'</span><span class="p">:</span> <span class="s1">'none'</span><span class="p">,</span> <span class="s1">'rasterized'</span><span class="p">:</span> <span class="kc">False</span><span class="p">},</span> <span class="n">plot_offdiag</span><span class="o">=</span><span class="p">{},</span> <span class="n">points_diag</span><span class="o">=</span><span class="p">{},</span> <span class="n">points_offdiag</span><span class="o">=</span><span class="p">{</span><span class="s1">'marker'</span><span class="p">:</span> <span class="s1">'.'</span><span class="p">,</span> <span class="s1">'markersize'</span><span class="p">:</span> <span class="mi">20</span><span class="p">},</span> <span class="n">fig_size</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">fig_bg_colors</span><span class="o">=</span><span class="p">{</span><span class="s1">'upper'</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">'diag'</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">'lower'</span><span class="p">:</span> <span class="kc">None</span><span class="p">},</span> <span class="n">fig_subplots_adjust</span><span class="o">=</span><span class="p">{</span><span class="s1">'top'</span><span class="p">:</span> <span class="mf">0.9</span><span class="p">},</span> <span class="n">subplots</span><span class="o">=</span><span class="p">{},</span> <span class="n">despine</span><span class="o">=</span><span class="p">{</span><span class="s1">'offset'</span><span class="p">:</span> <span class="mi">5</span><span class="p">},</span> <span class="n">title_format</span><span class="o">=</span><span class="p">{</span><span class="s1">'fontsize'</span><span class="p">:</span> <span class="mi">16</span><span class="p">})</span> </code>
<a class="headerlink" href="#sbi.utils.plot.pairplot" title="Permanent link">¶</a></h3>
<div class="doc doc-contents first">
<p>Plot samples and points.</p>
<p>For developers: if you add arguments that expect dictionaries, make sure to access
them via the opts dictionary instantiated below. E.g. if you want to access the dict
stored in the input variable hist_diag, use opts[<code>hist_diag</code>].</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>samples</code></td>
<td><code>Union[List[numpy.ndarray], List[torch.Tensor], numpy.ndarray, torch.Tensor]</code></td>
<td>
<p>posterior samples used to build the histogram</p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>points</code></td>
<td><code>Optional[Union[List[numpy.ndarray], List[torch.Tensor], numpy.ndarray, torch.Tensor]]</code></td>
<td>
<p>list of additional points to scatter</p>
</td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>upper</code></td>
<td><code>Optional[str]</code></td>
<td>
<p>plotting style for upper diagonal, {hist, scatter, contour, None}</p>
</td>
<td><code>'hist'</code></td>
</tr>
<tr>
<td><code>diag</code></td>
<td><code>Optional[str]</code></td>
<td>
<p>plotting style for diagonal, {hist, None}</p>
</td>
<td><code>'hist'</code></td>
</tr>
<tr>
<td><code>title</code></td>
<td><code>Optional[str]</code></td>
<td>
<p>title string</p>
</td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>legend</code></td>
<td><code>Optional[bool]</code></td>
<td>
<p>whether to plot a legend for the points</p>
</td>
<td><code>False</code></td>
</tr>
<tr>
<td><code>labels</code></td>
<td><code></code></td>
<td>
<p>np.ndarray of strings specifying the names of the parameters</p>
</td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>labels_points</code></td>
<td><code></code></td>
<td>
<p>np.ndarray of strings specifying the names of the passed points</p>
</td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>labels_samples</code></td>
<td><code></code></td>
<td>
<p>np.ndarray of strings specifying the names of the passed samples</p>
</td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>samples_colors</code></td>
<td><code></code></td>
<td>
<p>colors of the samples</p>
</td>
<td><code>['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']</code></td>
</tr>
<tr>
<td><code>points_colors</code></td>
<td><code></code></td>
<td>
<p>colors of the points</p>
</td>
<td><code>['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']</code></td>
</tr>
<tr>
<td><code>subset</code></td>
<td><code></code></td>
<td>
<p>List containing the dimensions to plot. E.g. subset=[1,3] will plot
plot only the 1<sup>st</sup> and 3<sup>rd</sup> dimension but will discard the 0<sup>th</sup> and 2<sup>nd</sup> (and,
if they exist, the 4<sup>th</sup>, 5<sup>th</sup> and so on)</p>
</td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>limits</code></td>
<td><code></code></td>
<td>
<p>array containing the plot xlim for each parameter dimension. If None,
just use the min and max of the passed samples</p>
</td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>ticks</code></td>
<td><code></code></td>
<td>
<p>location of the ticks for each parameter. If None, just use the min and
max along each parameter dimension</p>
</td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>tickformatter</code></td>
<td><code></code></td>
<td>
<p>passed to _format_axis()</p>
</td>
<td><code>&lt;matplotlib.ticker.FormatStrFormatter object at 0x121afa820&gt;</code></td>
</tr>
<tr>
<td><code>tick_labels</code></td>
<td><code></code></td>
<td>
<p>np.ndarray containing the ticklabels.</p>
</td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>hist_diag</code></td>
<td><code></code></td>
<td>
<p>dictionary passed to plt.hist() for diagonal plots</p>
</td>
<td><code>{'alpha': 1.0, 'bins': 50, 'density': False, 'histtype': 'step'}</code></td>
</tr>
<tr>
<td><code>hist_offdiag</code></td>
<td><code></code></td>
<td>
<p>dictionary passed to np.histogram2d() for off diagonal plots</p>
</td>
<td><code>{'bins': 50}</code></td>
</tr>
<tr>
<td><code>kde_diag</code></td>
<td><code></code></td>
<td>
<p>dictionary passed to gaussian_kde() for diagonal plots</p>
</td>
<td><code>{'bw_method': 'scott', 'bins': 50, 'color': 'black'}</code></td>
</tr>
<tr>
<td><code>kde_offdiag</code></td>
<td><code></code></td>
<td>
<p>dictionary passed to gaussian_kde() for off diagonal plots</p>
</td>
<td><code>{'bw_method': 'scott', 'bins': 50}</code></td>
</tr>
<tr>
<td><code>contour_offdiag</code></td>
<td><code></code></td>
<td>
<p>dictionary that should contain <code>percentile</code> and <code>levels</code> keys.
<code>percentile</code>: bool.
    If  <code>percentile</code>==True,
    the levels are made with respect to the max probability of the posterior
    If <code>percentile</code>==False,
    the levels are drawn at absolute positions
<code>levels</code>: list or np.ndarray: specifies the location where the contours are
    drawn.</p>
</td>
<td><code>{'levels': [0.68], 'percentile': True}</code></td>
</tr>
<tr>
<td><code>scatter_offdiag</code></td>
<td><code></code></td>
<td>
<p>dictionary for plt.scatter() on off diagonal</p>
</td>
<td><code>{'alpha': 0.5, 'edgecolor': 'none', 'rasterized': False}</code></td>
</tr>
<tr>
<td><code>plot_offdiag</code></td>
<td><code></code></td>
<td>
<p>dictionary for plt.plot() on off diagonal</p>
</td>
<td><code>{}</code></td>
</tr>
<tr>
<td><code>points_diag</code></td>
<td><code></code></td>
<td>
<p>dictionary for plt.plot() used for plotting points on diagonal</p>
</td>
<td><code>{}</code></td>
</tr>
<tr>
<td><code>points_offdiag</code></td>
<td><code></code></td>
<td>
<p>dictionary for plt.plot() used for plotting points on off
diagonal</p>
</td>
<td><code>{'marker': '.', 'markersize': 20}</code></td>
</tr>
<tr>
<td><code>fig_size</code></td>
<td><code>Tuple</code></td>
<td>
<p>size of the entire figure</p>
</td>
<td><code>(10, 10)</code></td>
</tr>
<tr>
<td><code>fig_bg_colors</code></td>
<td><code></code></td>
<td>
<p>Dictionary that contains <code>upper</code>, <code>diag</code>, <code>lower</code>, and specifies
the respective background colors. Passed to ax.set_facecolor()</p>
</td>
<td><code>{'upper': None, 'diag': None, 'lower': None}</code></td>
</tr>
<tr>
<td><code>fig_subplots_adjust</code></td>
<td><code></code></td>
<td>
<p>dictionary passed to fig.subplots_adjust()</p>
</td>
<td><code>{'top': 0.9}</code></td>
</tr>
<tr>
<td><code>subplots</code></td>
<td><code></code></td>
<td>
<p>dictionary passed to plt.subplots()</p>
</td>
<td><code>{}</code></td>
</tr>
<tr>
<td><code>despine</code></td>
<td><code></code></td>
<td>
<p>dictionary passed to set_position() for axis position</p>
</td>
<td><code>{'offset': 5}</code></td>
</tr>
<tr>
<td><code>title_format</code></td>
<td><code></code></td>
<td>
<p>dictionary passed to plt.title()</p>
</td>
<td><code>{'fontsize': 16}</code></td>
</tr>
</tbody>
</table>
<p>Returns: figure and axis of posterior distribution plot</p>
<details class="quote">
<summary>Source code in <code>sbi/utils/plot.py</code></summary>
<table class="highlighttable">
<tr>
<td class="linenos">
<div class="linenodiv">
<pre><span></span>129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
378
379
380
381
382
383
384
385
386
387
388
389
390
391
392
393
394
395
396
397
398
399
400
401
402
403
404
405
406
407
408
409
410
411
412
413
414
415
416
417
418
419
420
421
422
423
424
425
426
427
428
429
430
431
432
433
434
435
436
437
438
439
440
441
442
443
444
445
446
447
448
449
450
451
452
453
454
455
456
457
458
459
460
461
462
463
464
465
466
467
468
469
470
471
472
473
474
475
476
477
478
479
480
481
482
483
484
485
486
487
488
489
490
491
492
493
494
495
496
497
498
499
500
501
502
503
504
505
506
507
508
509
510
511
512
513
514
515
516
517
518
519
520
521
522
523
524
525
526
527
528
529
530
531
532
533
534
535
536
537
538
539
540
541
542
543
544
545
546
547
548
549
550
551
552
553
554
555
556
557
558
559
560
561
562
563
564
565
566
567
568
569
570
571
572
573
574</pre>
</div>
</td>
<td class="code">
<div class="highlight">
<pre><span></span><code><span class="k">def</span> <span class="nf">pairplot</span><span class="p">(</span>
    <span class="n">samples</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
    <span class="n">points</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
        <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span>
    <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">upper</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"hist"</span><span class="p">,</span>
    <span class="n">diag</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"hist"</span><span class="p">,</span>
    <span class="n">title</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">legend</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">labels_points</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">labels_samples</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">samples_colors</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s2">"axes.prop_cycle"</span><span class="p">]</span><span class="o">.</span><span class="n">by_key</span><span class="p">()[</span><span class="s2">"color"</span><span class="p">],</span>
    <span class="n">points_colors</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s2">"axes.prop_cycle"</span><span class="p">]</span><span class="o">.</span><span class="n">by_key</span><span class="p">()[</span><span class="s2">"color"</span><span class="p">],</span>
    <span class="n">subset</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">limits</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">ticks</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">tickformatter</span><span class="o">=</span><span class="n">mpl</span><span class="o">.</span><span class="n">ticker</span><span class="o">.</span><span class="n">FormatStrFormatter</span><span class="p">(</span><span class="s2">"</span><span class="si">%g</span><span class="s2">"</span><span class="p">),</span>
    <span class="n">tick_labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">hist_diag</span><span class="o">=</span><span class="p">{</span><span class="s2">"alpha"</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span> <span class="s2">"bins"</span><span class="p">:</span> <span class="mi">50</span><span class="p">,</span> <span class="s2">"density"</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span> <span class="s2">"histtype"</span><span class="p">:</span> <span class="s2">"step"</span><span class="p">},</span>
    <span class="n">hist_offdiag</span><span class="o">=</span><span class="p">{</span><span class="s2">"bins"</span><span class="p">:</span> <span class="mi">50</span><span class="p">,},</span>
    <span class="n">kde_diag</span><span class="o">=</span><span class="p">{</span><span class="s2">"bw_method"</span><span class="p">:</span> <span class="s2">"scott"</span><span class="p">,</span> <span class="s2">"bins"</span><span class="p">:</span> <span class="mi">50</span><span class="p">,</span> <span class="s2">"color"</span><span class="p">:</span> <span class="s2">"black"</span><span class="p">},</span>
    <span class="n">kde_offdiag</span><span class="o">=</span><span class="p">{</span><span class="s2">"bw_method"</span><span class="p">:</span> <span class="s2">"scott"</span><span class="p">,</span> <span class="s2">"bins"</span><span class="p">:</span> <span class="mi">50</span><span class="p">},</span>
    <span class="n">contour_offdiag</span><span class="o">=</span><span class="p">{</span><span class="s2">"levels"</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.68</span><span class="p">],</span> <span class="s2">"percentile"</span><span class="p">:</span> <span class="kc">True</span><span class="p">},</span>
    <span class="n">scatter_offdiag</span><span class="o">=</span><span class="p">{</span><span class="s2">"alpha"</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">,</span> <span class="s2">"edgecolor"</span><span class="p">:</span> <span class="s2">"none"</span><span class="p">,</span> <span class="s2">"rasterized"</span><span class="p">:</span> <span class="kc">False</span><span class="p">},</span>
    <span class="n">plot_offdiag</span><span class="o">=</span><span class="p">{},</span>
    <span class="n">points_diag</span><span class="o">=</span><span class="p">{},</span>
    <span class="n">points_offdiag</span><span class="o">=</span><span class="p">{</span><span class="s2">"marker"</span><span class="p">:</span> <span class="s2">"."</span><span class="p">,</span> <span class="s2">"markersize"</span><span class="p">:</span> <span class="mi">20</span><span class="p">},</span>
    <span class="n">fig_size</span><span class="p">:</span> <span class="n">Tuple</span> <span class="o">=</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span>
    <span class="n">fig_bg_colors</span><span class="o">=</span><span class="p">{</span><span class="s2">"upper"</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">"diag"</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">"lower"</span><span class="p">:</span> <span class="kc">None</span><span class="p">},</span>
    <span class="n">fig_subplots_adjust</span><span class="o">=</span><span class="p">{</span><span class="s2">"top"</span><span class="p">:</span> <span class="mf">0.9</span><span class="p">},</span>
    <span class="n">subplots</span><span class="o">=</span><span class="p">{},</span>
    <span class="n">despine</span><span class="o">=</span><span class="p">{</span><span class="s2">"offset"</span><span class="p">:</span> <span class="mi">5</span><span class="p">},</span>
    <span class="n">title_format</span><span class="o">=</span><span class="p">{</span><span class="s2">"fontsize"</span><span class="p">:</span> <span class="mi">16</span><span class="p">},</span>
<span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Plot samples and points.</span>

<span class="sd">    For developers: if you add arguments that expect dictionaries, make sure to access</span>
<span class="sd">    them via the opts dictionary instantiated below. E.g. if you want to access the dict</span>
<span class="sd">    stored in the input variable hist_diag, use opts[`hist_diag`].</span>

<span class="sd">    Args:</span>
<span class="sd">        samples: posterior samples used to build the histogram</span>
<span class="sd">        points: list of additional points to scatter</span>
<span class="sd">        upper: plotting style for upper diagonal, {hist, scatter, contour, None}</span>
<span class="sd">        diag: plotting style for diagonal, {hist, None}</span>
<span class="sd">        title: title string</span>
<span class="sd">        legend: whether to plot a legend for the points</span>
<span class="sd">        labels: np.ndarray of strings specifying the names of the parameters</span>
<span class="sd">        labels_points: np.ndarray of strings specifying the names of the passed points</span>
<span class="sd">        labels_samples: np.ndarray of strings specifying the names of the passed samples</span>
<span class="sd">        samples_colors: colors of the samples</span>
<span class="sd">        points_colors: colors of the points</span>
<span class="sd">        subset: List containing the dimensions to plot. E.g. subset=[1,3] will plot</span>
<span class="sd">            plot only the 1st and 3rd dimension but will discard the 0th and 2nd (and,</span>
<span class="sd">            if they exist, the 4th, 5th and so on)</span>
<span class="sd">        limits: array containing the plot xlim for each parameter dimension. If None,</span>
<span class="sd">            just use the min and max of the passed samples</span>
<span class="sd">        ticks: location of the ticks for each parameter. If None, just use the min and</span>
<span class="sd">            max along each parameter dimension</span>
<span class="sd">        tickformatter: passed to _format_axis()</span>
<span class="sd">        tick_labels: np.ndarray containing the ticklabels.</span>
<span class="sd">        hist_diag: dictionary passed to plt.hist() for diagonal plots</span>
<span class="sd">        hist_offdiag: dictionary passed to np.histogram2d() for off diagonal plots</span>
<span class="sd">        kde_diag: dictionary passed to gaussian_kde() for diagonal plots</span>
<span class="sd">        kde_offdiag: dictionary passed to gaussian_kde() for off diagonal plots</span>
<span class="sd">        contour_offdiag: dictionary that should contain `percentile` and `levels` keys.</span>
<span class="sd">            `percentile`: bool.</span>
<span class="sd">                If  `percentile`==True,</span>
<span class="sd">                the levels are made with respect to the max probability of the posterior</span>
<span class="sd">                If `percentile`==False,</span>
<span class="sd">                the levels are drawn at absolute positions</span>
<span class="sd">            `levels`: list or np.ndarray: specifies the location where the contours are</span>
<span class="sd">                drawn.</span>
<span class="sd">        scatter_offdiag: dictionary for plt.scatter() on off diagonal</span>
<span class="sd">        plot_offdiag: dictionary for plt.plot() on off diagonal</span>
<span class="sd">        points_diag: dictionary for plt.plot() used for plotting points on diagonal</span>
<span class="sd">        points_offdiag: dictionary for plt.plot() used for plotting points on off</span>
<span class="sd">            diagonal</span>
<span class="sd">        fig_size: size of the entire figure</span>
<span class="sd">        fig_bg_colors: Dictionary that contains `upper`, `diag`, `lower`, and specifies</span>
<span class="sd">            the respective background colors. Passed to ax.set_facecolor()</span>
<span class="sd">        fig_subplots_adjust: dictionary passed to fig.subplots_adjust()</span>
<span class="sd">        subplots: dictionary passed to plt.subplots()</span>
<span class="sd">        despine: dictionary passed to set_position() for axis position</span>
<span class="sd">        title_format: dictionary passed to plt.title()</span>

<span class="sd">    Returns: figure and axis of posterior distribution plot</span>

<span class="sd">    """</span>

    <span class="c1"># TODO: add color map support</span>
    <span class="c1"># TODO: automatically determine good bin sizes for histograms</span>
    <span class="c1"># TODO: add legend (if legend is True)</span>

    <span class="c1"># get default values of function arguments</span>
    <span class="c1"># https://stackoverflow.com/questions/12627118/get-a-function-arguments-default-value</span>
    <span class="n">spec</span> <span class="o">=</span> <span class="n">inspect</span><span class="o">.</span><span class="n">getfullargspec</span><span class="p">(</span><span class="n">pairplot</span><span class="p">)</span>

    <span class="c1"># build a dict for the defaults</span>
    <span class="c1"># https://stackoverflow.com/questions/12627118/get-a-function-arguments-default-value</span>
    <span class="c1"># answer by gnr</span>
    <span class="n">default_val_dict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">spec</span><span class="o">.</span><span class="n">args</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">(</span><span class="n">spec</span><span class="o">.</span><span class="n">defaults</span> <span class="ow">or</span> <span class="p">())[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>

    <span class="c1"># update the defaults dictionary by the current values of the variables (passed by</span>
    <span class="c1"># the user)</span>
    <span class="n">opts</span> <span class="o">=</span> <span class="n">_update</span><span class="p">(</span><span class="n">default_val_dict</span><span class="p">,</span> <span class="nb">locals</span><span class="p">())</span>

    <span class="c1"># Prepare samples</span>
    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">list</span><span class="p">:</span>
        <span class="n">samples</span> <span class="o">=</span> <span class="n">ensure_numpy</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span>
        <span class="n">samples</span> <span class="o">=</span> <span class="p">[</span><span class="n">samples</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">sample_pack</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">samples</span><span class="p">):</span>
            <span class="n">samples</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">ensure_numpy</span><span class="p">(</span><span class="n">samples</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>

    <span class="c1"># Prepare points</span>
    <span class="k">if</span> <span class="n">points</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">points</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">points</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">list</span><span class="p">:</span>
        <span class="n">points</span> <span class="o">=</span> <span class="n">ensure_numpy</span><span class="p">(</span><span class="n">points</span><span class="p">)</span>
        <span class="n">points</span> <span class="o">=</span> <span class="p">[</span><span class="n">points</span><span class="p">]</span>
    <span class="n">points</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">atleast_2d</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">points</span><span class="p">]</span>
    <span class="n">points</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">atleast_2d</span><span class="p">(</span><span class="n">ensure_numpy</span><span class="p">(</span><span class="n">p</span><span class="p">))</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">points</span><span class="p">]</span>

    <span class="c1"># Dimensions</span>
    <span class="n">dim</span> <span class="o">=</span> <span class="n">samples</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">num_samples</span> <span class="o">=</span> <span class="n">samples</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="c1"># TODO: add asserts checking compatibility of dimensions</span>

    <span class="c1"># Prepare labels</span>
    <span class="k">if</span> <span class="n">opts</span><span class="p">[</span><span class="s2">"labels"</span><span class="p">]</span> <span class="o">==</span> <span class="p">[]</span> <span class="ow">or</span> <span class="n">opts</span><span class="p">[</span><span class="s2">"labels"</span><span class="p">]</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">labels_dim</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"dim </span><span class="si">{}</span><span class="s2">"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">dim</span><span class="p">)]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">labels_dim</span> <span class="o">=</span> <span class="n">opts</span><span class="p">[</span><span class="s2">"labels"</span><span class="p">]</span>

    <span class="c1"># Prepare limits</span>
    <span class="k">if</span> <span class="n">opts</span><span class="p">[</span><span class="s2">"limits"</span><span class="p">]</span> <span class="o">==</span> <span class="p">[]</span> <span class="ow">or</span> <span class="n">opts</span><span class="p">[</span><span class="s2">"limits"</span><span class="p">]</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">limits</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">dim</span><span class="p">):</span>
            <span class="nb">min</span> <span class="o">=</span> <span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span>
            <span class="nb">max</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span>
            <span class="k">for</span> <span class="n">sample</span> <span class="ow">in</span> <span class="n">samples</span><span class="p">:</span>
                <span class="n">min_</span> <span class="o">=</span> <span class="n">sample</span><span class="p">[:,</span> <span class="n">d</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span>
                <span class="nb">min</span> <span class="o">=</span> <span class="n">min_</span> <span class="k">if</span> <span class="n">min_</span> <span class="o">&lt;</span> <span class="nb">min</span> <span class="k">else</span> <span class="nb">min</span>
                <span class="n">max_</span> <span class="o">=</span> <span class="n">sample</span><span class="p">[:,</span> <span class="n">d</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
                <span class="nb">max</span> <span class="o">=</span> <span class="n">max_</span> <span class="k">if</span> <span class="n">max_</span> <span class="o">&gt;</span> <span class="nb">max</span> <span class="k">else</span> <span class="nb">max</span>
            <span class="n">limits</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="nb">min</span><span class="p">,</span> <span class="nb">max</span><span class="p">])</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">opts</span><span class="p">[</span><span class="s2">"limits"</span><span class="p">])</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">limits</span> <span class="o">=</span> <span class="p">[</span><span class="n">opts</span><span class="p">[</span><span class="s2">"limits"</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">dim</span><span class="p">)]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">limits</span> <span class="o">=</span> <span class="n">opts</span><span class="p">[</span><span class="s2">"limits"</span><span class="p">]</span>

    <span class="c1"># Prepare ticks</span>
    <span class="k">if</span> <span class="n">opts</span><span class="p">[</span><span class="s2">"ticks"</span><span class="p">]</span> <span class="o">==</span> <span class="p">[]</span> <span class="ow">or</span> <span class="n">opts</span><span class="p">[</span><span class="s2">"ticks"</span><span class="p">]</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">ticks</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">opts</span><span class="p">[</span><span class="s2">"ticks"</span><span class="p">])</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">ticks</span> <span class="o">=</span> <span class="p">[</span><span class="n">opts</span><span class="p">[</span><span class="s2">"ticks"</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">dim</span><span class="p">)]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">ticks</span> <span class="o">=</span> <span class="n">opts</span><span class="p">[</span><span class="s2">"ticks"</span><span class="p">]</span>

    <span class="c1"># Prepare diag/upper/lower</span>
    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">opts</span><span class="p">[</span><span class="s2">"diag"</span><span class="p">])</span> <span class="ow">is</span> <span class="ow">not</span> <span class="nb">list</span><span class="p">:</span>
        <span class="n">opts</span><span class="p">[</span><span class="s2">"diag"</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">opts</span><span class="p">[</span><span class="s2">"diag"</span><span class="p">]</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">samples</span><span class="p">))]</span>
    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">opts</span><span class="p">[</span><span class="s2">"upper"</span><span class="p">])</span> <span class="ow">is</span> <span class="ow">not</span> <span class="nb">list</span><span class="p">:</span>
        <span class="n">opts</span><span class="p">[</span><span class="s2">"upper"</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">opts</span><span class="p">[</span><span class="s2">"upper"</span><span class="p">]</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">samples</span><span class="p">))]</span>
    <span class="c1"># if type(opts['lower']) is not list:</span>
    <span class="c1">#    opts['lower'] = [opts['lower'] for _ in range(len(samples))]</span>
    <span class="n">opts</span><span class="p">[</span><span class="s2">"lower"</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="c1"># Figure out if we subset the plot</span>
    <span class="n">subset</span> <span class="o">=</span> <span class="n">opts</span><span class="p">[</span><span class="s2">"subset"</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">subset</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">rows</span> <span class="o">=</span> <span class="n">cols</span> <span class="o">=</span> <span class="n">dim</span>
        <span class="n">subset</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">dim</span><span class="p">)]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">subset</span><span class="p">)</span> <span class="o">==</span> <span class="nb">int</span><span class="p">:</span>
            <span class="n">subset</span> <span class="o">=</span> <span class="p">[</span><span class="n">subset</span><span class="p">]</span>
        <span class="k">elif</span> <span class="nb">type</span><span class="p">(</span><span class="n">subset</span><span class="p">)</span> <span class="o">==</span> <span class="nb">list</span><span class="p">:</span>
            <span class="k">pass</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span>
        <span class="n">rows</span> <span class="o">=</span> <span class="n">cols</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">subset</span><span class="p">)</span>

    <span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">rows</span><span class="p">,</span> <span class="n">cols</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="n">opts</span><span class="p">[</span><span class="s2">"fig_size"</span><span class="p">],</span> <span class="o">**</span><span class="n">opts</span><span class="p">[</span><span class="s2">"subplots"</span><span class="p">])</span>
    <span class="c1"># Cast to ndarray in case of 1D subplots.</span>
    <span class="n">axes</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">axes</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">rows</span><span class="p">,</span> <span class="n">cols</span><span class="p">)</span>

    <span class="c1"># Style figure</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="o">**</span><span class="n">opts</span><span class="p">[</span><span class="s2">"fig_subplots_adjust"</span><span class="p">])</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="n">opts</span><span class="p">[</span><span class="s2">"title"</span><span class="p">],</span> <span class="o">**</span><span class="n">opts</span><span class="p">[</span><span class="s2">"title_format"</span><span class="p">])</span>

    <span class="c1"># Style axes</span>
    <span class="n">row_idx</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
    <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">dim</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">row</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">subset</span><span class="p">:</span>
            <span class="k">continue</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">row_idx</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="n">col_idx</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
        <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">dim</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">col</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">subset</span><span class="p">:</span>
                <span class="k">continue</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">col_idx</span> <span class="o">+=</span> <span class="mi">1</span>

            <span class="k">if</span> <span class="n">row</span> <span class="o">==</span> <span class="n">col</span><span class="p">:</span>
                <span class="n">current</span> <span class="o">=</span> <span class="s2">"diag"</span>
            <span class="k">elif</span> <span class="n">row</span> <span class="o">&lt;</span> <span class="n">col</span><span class="p">:</span>
                <span class="n">current</span> <span class="o">=</span> <span class="s2">"upper"</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">current</span> <span class="o">=</span> <span class="s2">"lower"</span>

            <span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="n">row_idx</span><span class="p">,</span> <span class="n">col_idx</span><span class="p">]</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">sca</span><span class="p">(</span><span class="n">ax</span><span class="p">)</span>

            <span class="c1"># Background color</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="n">current</span> <span class="ow">in</span> <span class="n">opts</span><span class="p">[</span><span class="s2">"fig_bg_colors"</span><span class="p">]</span>
                <span class="ow">and</span> <span class="n">opts</span><span class="p">[</span><span class="s2">"fig_bg_colors"</span><span class="p">][</span><span class="n">current</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="p">):</span>
                <span class="n">ax</span><span class="o">.</span><span class="n">set_facecolor</span><span class="p">(</span><span class="n">opts</span><span class="p">[</span><span class="s2">"fig_bg_colors"</span><span class="p">][</span><span class="n">current</span><span class="p">])</span>

            <span class="c1"># Axes</span>
            <span class="k">if</span> <span class="n">opts</span><span class="p">[</span><span class="n">current</span><span class="p">]</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s2">"off"</span><span class="p">)</span>
                <span class="k">continue</span>

            <span class="c1"># Limits</span>
            <span class="k">if</span> <span class="n">limits</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">((</span><span class="n">limits</span><span class="p">[</span><span class="n">col</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">limits</span><span class="p">[</span><span class="n">col</span><span class="p">][</span><span class="mi">1</span><span class="p">]))</span>
                <span class="k">if</span> <span class="n">current</span> <span class="o">!=</span> <span class="s2">"diag"</span><span class="p">:</span>
                    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">((</span><span class="n">limits</span><span class="p">[</span><span class="n">row</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">limits</span><span class="p">[</span><span class="n">row</span><span class="p">][</span><span class="mi">1</span><span class="p">]))</span>
            <span class="n">xmin</span><span class="p">,</span> <span class="n">xmax</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">get_xlim</span><span class="p">()</span>
            <span class="n">ymin</span><span class="p">,</span> <span class="n">ymax</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">get_ylim</span><span class="p">()</span>

            <span class="c1"># Ticks</span>
            <span class="k">if</span> <span class="n">ticks</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">((</span><span class="n">ticks</span><span class="p">[</span><span class="n">col</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">ticks</span><span class="p">[</span><span class="n">col</span><span class="p">][</span><span class="mi">1</span><span class="p">]))</span>
                <span class="k">if</span> <span class="n">current</span> <span class="o">!=</span> <span class="s2">"diag"</span><span class="p">:</span>
                    <span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">((</span><span class="n">ticks</span><span class="p">[</span><span class="n">row</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">ticks</span><span class="p">[</span><span class="n">row</span><span class="p">][</span><span class="mi">1</span><span class="p">]))</span>

            <span class="c1"># Despine</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">spines</span><span class="p">[</span><span class="s2">"right"</span><span class="p">]</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">spines</span><span class="p">[</span><span class="s2">"top"</span><span class="p">]</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">spines</span><span class="p">[</span><span class="s2">"bottom"</span><span class="p">]</span><span class="o">.</span><span class="n">set_position</span><span class="p">((</span><span class="s2">"outward"</span><span class="p">,</span> <span class="n">despine</span><span class="p">[</span><span class="s2">"offset"</span><span class="p">]))</span>

            <span class="c1"># Formatting axes</span>
            <span class="k">if</span> <span class="n">current</span> <span class="o">==</span> <span class="s2">"diag"</span><span class="p">:</span>  <span class="c1"># off-diagnoals</span>
                <span class="k">if</span> <span class="n">opts</span><span class="p">[</span><span class="s2">"lower"</span><span class="p">]</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">col</span> <span class="o">==</span> <span class="n">dim</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
                    <span class="n">_format_axis</span><span class="p">(</span>
                        <span class="n">ax</span><span class="p">,</span>
                        <span class="n">xhide</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                        <span class="n">xlabel</span><span class="o">=</span><span class="n">labels_dim</span><span class="p">[</span><span class="n">col</span><span class="p">],</span>
                        <span class="n">yhide</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                        <span class="n">tickformatter</span><span class="o">=</span><span class="n">opts</span><span class="p">[</span><span class="s2">"tickformatter"</span><span class="p">],</span>
                    <span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">_format_axis</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="n">xhide</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">yhide</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>  <span class="c1"># off-diagnoals</span>
                <span class="k">if</span> <span class="n">row</span> <span class="o">==</span> <span class="n">dim</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
                    <span class="n">_format_axis</span><span class="p">(</span>
                        <span class="n">ax</span><span class="p">,</span>
                        <span class="n">xhide</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                        <span class="n">xlabel</span><span class="o">=</span><span class="n">labels_dim</span><span class="p">[</span><span class="n">col</span><span class="p">],</span>
                        <span class="n">yhide</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                        <span class="n">tickformatter</span><span class="o">=</span><span class="n">opts</span><span class="p">[</span><span class="s2">"tickformatter"</span><span class="p">],</span>
                    <span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">_format_axis</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="n">xhide</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">yhide</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">opts</span><span class="p">[</span><span class="s2">"tick_labels"</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">ax</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">(</span>
                    <span class="p">(</span>
                        <span class="nb">str</span><span class="p">(</span><span class="n">opts</span><span class="p">[</span><span class="s2">"tick_labels"</span><span class="p">][</span><span class="n">col</span><span class="p">][</span><span class="mi">0</span><span class="p">]),</span>
                        <span class="nb">str</span><span class="p">(</span><span class="n">opts</span><span class="p">[</span><span class="s2">"tick_labels"</span><span class="p">][</span><span class="n">col</span><span class="p">][</span><span class="mi">1</span><span class="p">]),</span>
                    <span class="p">)</span>
                <span class="p">)</span>

            <span class="c1"># Diagonals</span>
            <span class="k">if</span> <span class="n">current</span> <span class="o">==</span> <span class="s2">"diag"</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">samples</span><span class="p">):</span>
                        <span class="k">if</span> <span class="n">opts</span><span class="p">[</span><span class="s2">"diag"</span><span class="p">][</span><span class="n">n</span><span class="p">]</span> <span class="o">==</span> <span class="s2">"hist"</span><span class="p">:</span>
                            <span class="n">h</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span>
                                <span class="n">v</span><span class="p">[:,</span> <span class="n">row</span><span class="p">],</span>
                                <span class="n">color</span><span class="o">=</span><span class="n">opts</span><span class="p">[</span><span class="s2">"samples_colors"</span><span class="p">][</span><span class="n">n</span><span class="p">],</span>
                                <span class="o">**</span><span class="n">opts</span><span class="p">[</span><span class="s2">"hist_diag"</span><span class="p">]</span>
                            <span class="p">)</span>
                        <span class="k">elif</span> <span class="n">opts</span><span class="p">[</span><span class="s2">"diag"</span><span class="p">][</span><span class="n">n</span><span class="p">]</span> <span class="o">==</span> <span class="s2">"kde"</span><span class="p">:</span>
                            <span class="n">density</span> <span class="o">=</span> <span class="n">gaussian_kde</span><span class="p">(</span>
                                <span class="n">v</span><span class="p">[:,</span> <span class="n">row</span><span class="p">],</span> <span class="n">bw_method</span><span class="o">=</span><span class="n">opts</span><span class="p">[</span><span class="s2">"kde_diag"</span><span class="p">][</span><span class="s2">"bw_method"</span><span class="p">]</span>
                            <span class="p">)</span>
                            <span class="n">xs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">xmin</span><span class="p">,</span> <span class="n">xmax</span><span class="p">,</span> <span class="n">opts</span><span class="p">[</span><span class="s2">"kde_diag"</span><span class="p">][</span><span class="s2">"bins"</span><span class="p">])</span>
                            <span class="n">ys</span> <span class="o">=</span> <span class="n">density</span><span class="p">(</span><span class="n">xs</span><span class="p">)</span>
                            <span class="n">h</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">ys</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">opts</span><span class="p">[</span><span class="s2">"samples_colors"</span><span class="p">][</span><span class="n">n</span><span class="p">],)</span>
                        <span class="k">else</span><span class="p">:</span>
                            <span class="k">pass</span>

                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">points</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">extent</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">get_ylim</span><span class="p">()</span>
                    <span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">points</span><span class="p">):</span>
                        <span class="n">h</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
                            <span class="p">[</span><span class="n">v</span><span class="p">[:,</span> <span class="n">row</span><span class="p">],</span> <span class="n">v</span><span class="p">[:,</span> <span class="n">row</span><span class="p">]],</span>
                            <span class="n">extent</span><span class="p">,</span>
                            <span class="n">color</span><span class="o">=</span><span class="n">opts</span><span class="p">[</span><span class="s2">"points_colors"</span><span class="p">][</span><span class="n">n</span><span class="p">],</span>
                            <span class="o">**</span><span class="n">opts</span><span class="p">[</span><span class="s2">"points_diag"</span><span class="p">]</span>
                        <span class="p">)</span>

            <span class="c1"># Off-diagonals</span>
            <span class="k">else</span><span class="p">:</span>

                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">samples</span><span class="p">):</span>
                        <span class="k">if</span> <span class="n">opts</span><span class="p">[</span><span class="s2">"upper"</span><span class="p">][</span><span class="n">n</span><span class="p">]</span> <span class="o">==</span> <span class="s2">"hist"</span> <span class="ow">or</span> <span class="n">opts</span><span class="p">[</span><span class="s2">"upper"</span><span class="p">][</span><span class="n">n</span><span class="p">]</span> <span class="o">==</span> <span class="s2">"hist2d"</span><span class="p">:</span>
                            <span class="c1"># h = plt.hist2d(</span>
                            <span class="c1">#     v[:, col], v[:, row],</span>
                            <span class="c1">#     range=(</span>
                            <span class="c1">#         [limits[col][0], limits[col][1]],</span>
                            <span class="c1">#         [limits[row][0], limits[row][1]]),</span>
                            <span class="c1">#     **opts['hist_offdiag']</span>
                            <span class="c1">#     )</span>
                            <span class="n">hist</span><span class="p">,</span> <span class="n">xedges</span><span class="p">,</span> <span class="n">yedges</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">histogram2d</span><span class="p">(</span>
                                <span class="n">v</span><span class="p">[:,</span> <span class="n">col</span><span class="p">],</span>
                                <span class="n">v</span><span class="p">[:,</span> <span class="n">row</span><span class="p">],</span>
                                <span class="nb">range</span><span class="o">=</span><span class="p">[</span>
                                    <span class="p">[</span><span class="n">limits</span><span class="p">[</span><span class="n">col</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">limits</span><span class="p">[</span><span class="n">col</span><span class="p">][</span><span class="mi">1</span><span class="p">]],</span>
                                    <span class="p">[</span><span class="n">limits</span><span class="p">[</span><span class="n">row</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">limits</span><span class="p">[</span><span class="n">row</span><span class="p">][</span><span class="mi">1</span><span class="p">]],</span>
                                <span class="p">],</span>
                                <span class="o">**</span><span class="n">opts</span><span class="p">[</span><span class="s2">"hist_offdiag"</span><span class="p">]</span>
                            <span class="p">)</span>
                            <span class="n">h</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span>
                                <span class="n">hist</span><span class="o">.</span><span class="n">T</span><span class="p">,</span>
                                <span class="n">origin</span><span class="o">=</span><span class="s2">"lower"</span><span class="p">,</span>
                                <span class="n">extent</span><span class="o">=</span><span class="p">[</span><span class="n">xedges</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">xedges</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">yedges</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">yedges</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],],</span>
                                <span class="n">aspect</span><span class="o">=</span><span class="s2">"auto"</span><span class="p">,</span>
                            <span class="p">)</span>

                        <span class="k">elif</span> <span class="n">opts</span><span class="p">[</span><span class="s2">"upper"</span><span class="p">][</span><span class="n">n</span><span class="p">]</span> <span class="ow">in</span> <span class="p">[</span>
                            <span class="s2">"kde"</span><span class="p">,</span>
                            <span class="s2">"kde2d"</span><span class="p">,</span>
                            <span class="s2">"contour"</span><span class="p">,</span>
                            <span class="s2">"contourf"</span><span class="p">,</span>
                        <span class="p">]:</span>
                            <span class="n">density</span> <span class="o">=</span> <span class="n">gaussian_kde</span><span class="p">(</span>
                                <span class="n">v</span><span class="p">[:,</span> <span class="p">[</span><span class="n">col</span><span class="p">,</span> <span class="n">row</span><span class="p">]]</span><span class="o">.</span><span class="n">T</span><span class="p">,</span>
                                <span class="n">bw_method</span><span class="o">=</span><span class="n">opts</span><span class="p">[</span><span class="s2">"kde_offdiag"</span><span class="p">][</span><span class="s2">"bw_method"</span><span class="p">],</span>
                            <span class="p">)</span>
                            <span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span>
                                <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span>
                                    <span class="n">limits</span><span class="p">[</span><span class="n">col</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span>
                                    <span class="n">limits</span><span class="p">[</span><span class="n">col</span><span class="p">][</span><span class="mi">1</span><span class="p">],</span>
                                    <span class="n">opts</span><span class="p">[</span><span class="s2">"kde_offdiag"</span><span class="p">][</span><span class="s2">"bins"</span><span class="p">],</span>
                                <span class="p">),</span>
                                <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span>
                                    <span class="n">limits</span><span class="p">[</span><span class="n">row</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span>
                                    <span class="n">limits</span><span class="p">[</span><span class="n">row</span><span class="p">][</span><span class="mi">1</span><span class="p">],</span>
                                    <span class="n">opts</span><span class="p">[</span><span class="s2">"kde_offdiag"</span><span class="p">][</span><span class="s2">"bins"</span><span class="p">],</span>
                                <span class="p">),</span>
                            <span class="p">)</span>
                            <span class="n">positions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">X</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">Y</span><span class="o">.</span><span class="n">ravel</span><span class="p">()])</span>
                            <span class="n">Z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">density</span><span class="p">(</span><span class="n">positions</span><span class="p">)</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

                            <span class="k">if</span> <span class="n">opts</span><span class="p">[</span><span class="s2">"upper"</span><span class="p">][</span><span class="n">n</span><span class="p">]</span> <span class="o">==</span> <span class="s2">"kde"</span> <span class="ow">or</span> <span class="n">opts</span><span class="p">[</span><span class="s2">"upper"</span><span class="p">][</span><span class="n">n</span><span class="p">]</span> <span class="o">==</span> <span class="s2">"kde2d"</span><span class="p">:</span>
                                <span class="n">h</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span>
                                    <span class="n">Z</span><span class="p">,</span>
                                    <span class="n">extent</span><span class="o">=</span><span class="p">[</span>
                                        <span class="n">limits</span><span class="p">[</span><span class="n">col</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span>
                                        <span class="n">limits</span><span class="p">[</span><span class="n">col</span><span class="p">][</span><span class="mi">1</span><span class="p">],</span>
                                        <span class="n">limits</span><span class="p">[</span><span class="n">row</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span>
                                        <span class="n">limits</span><span class="p">[</span><span class="n">row</span><span class="p">][</span><span class="mi">1</span><span class="p">],</span>
                                    <span class="p">],</span>
                                    <span class="n">origin</span><span class="o">=</span><span class="s2">"lower"</span><span class="p">,</span>
                                    <span class="n">aspect</span><span class="o">=</span><span class="s2">"auto"</span><span class="p">,</span>
                                <span class="p">)</span>
                            <span class="k">elif</span> <span class="n">opts</span><span class="p">[</span><span class="s2">"upper"</span><span class="p">][</span><span class="n">n</span><span class="p">]</span> <span class="o">==</span> <span class="s2">"contour"</span><span class="p">:</span>
                                <span class="k">if</span> <span class="n">opts</span><span class="p">[</span><span class="s2">"contour_offdiag"</span><span class="p">][</span><span class="s2">"percentile"</span><span class="p">]:</span>
                                    <span class="n">Z</span> <span class="o">=</span> <span class="n">probs2contours</span><span class="p">(</span>
                                        <span class="n">Z</span><span class="p">,</span> <span class="n">opts</span><span class="p">[</span><span class="s2">"contour_offdiag"</span><span class="p">][</span><span class="s2">"levels"</span><span class="p">]</span>
                                    <span class="p">)</span>
                                <span class="k">else</span><span class="p">:</span>
                                    <span class="n">Z</span> <span class="o">=</span> <span class="p">(</span><span class="n">Z</span> <span class="o">-</span> <span class="n">Z</span><span class="o">.</span><span class="n">min</span><span class="p">())</span> <span class="o">/</span> <span class="p">(</span><span class="n">Z</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">-</span> <span class="n">Z</span><span class="o">.</span><span class="n">min</span><span class="p">())</span>
                                <span class="n">h</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span>
                                    <span class="n">X</span><span class="p">,</span>
                                    <span class="n">Y</span><span class="p">,</span>
                                    <span class="n">Z</span><span class="p">,</span>
                                    <span class="n">origin</span><span class="o">=</span><span class="s2">"lower"</span><span class="p">,</span>
                                    <span class="n">extent</span><span class="o">=</span><span class="p">[</span>
                                        <span class="n">limits</span><span class="p">[</span><span class="n">col</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span>
                                        <span class="n">limits</span><span class="p">[</span><span class="n">col</span><span class="p">][</span><span class="mi">1</span><span class="p">],</span>
                                        <span class="n">limits</span><span class="p">[</span><span class="n">row</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span>
                                        <span class="n">limits</span><span class="p">[</span><span class="n">row</span><span class="p">][</span><span class="mi">1</span><span class="p">],</span>
                                    <span class="p">],</span>
                                    <span class="n">colors</span><span class="o">=</span><span class="n">opts</span><span class="p">[</span><span class="s2">"samples_colors"</span><span class="p">][</span><span class="n">n</span><span class="p">],</span>
                                    <span class="n">levels</span><span class="o">=</span><span class="n">opts</span><span class="p">[</span><span class="s2">"contour_offdiag"</span><span class="p">][</span><span class="s2">"levels"</span><span class="p">],</span>
                                <span class="p">)</span>
                            <span class="k">else</span><span class="p">:</span>
                                <span class="k">pass</span>
                        <span class="k">elif</span> <span class="n">opts</span><span class="p">[</span><span class="s2">"upper"</span><span class="p">][</span><span class="n">n</span><span class="p">]</span> <span class="o">==</span> <span class="s2">"scatter"</span><span class="p">:</span>
                            <span class="n">h</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span>
                                <span class="n">v</span><span class="p">[:,</span> <span class="n">col</span><span class="p">],</span>
                                <span class="n">v</span><span class="p">[:,</span> <span class="n">row</span><span class="p">],</span>
                                <span class="n">color</span><span class="o">=</span><span class="n">opts</span><span class="p">[</span><span class="s2">"samples_colors"</span><span class="p">][</span><span class="n">n</span><span class="p">],</span>
                                <span class="o">**</span><span class="n">opts</span><span class="p">[</span><span class="s2">"scatter_offdiag"</span><span class="p">]</span>
                            <span class="p">)</span>
                        <span class="k">elif</span> <span class="n">opts</span><span class="p">[</span><span class="s2">"upper"</span><span class="p">][</span><span class="n">n</span><span class="p">]</span> <span class="o">==</span> <span class="s2">"plot"</span><span class="p">:</span>
                            <span class="n">h</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
                                <span class="n">v</span><span class="p">[:,</span> <span class="n">col</span><span class="p">],</span>
                                <span class="n">v</span><span class="p">[:,</span> <span class="n">row</span><span class="p">],</span>
                                <span class="n">color</span><span class="o">=</span><span class="n">opts</span><span class="p">[</span><span class="s2">"samples_colors"</span><span class="p">][</span><span class="n">n</span><span class="p">],</span>
                                <span class="o">**</span><span class="n">opts</span><span class="p">[</span><span class="s2">"plot_offdiag"</span><span class="p">]</span>
                            <span class="p">)</span>
                        <span class="k">else</span><span class="p">:</span>
                            <span class="k">pass</span>

                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">points</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>

                    <span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">points</span><span class="p">):</span>
                        <span class="n">h</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
                            <span class="n">v</span><span class="p">[:,</span> <span class="n">col</span><span class="p">],</span>
                            <span class="n">v</span><span class="p">[:,</span> <span class="n">row</span><span class="p">],</span>
                            <span class="n">color</span><span class="o">=</span><span class="n">opts</span><span class="p">[</span><span class="s2">"points_colors"</span><span class="p">][</span><span class="n">n</span><span class="p">],</span>
                            <span class="o">**</span><span class="n">opts</span><span class="p">[</span><span class="s2">"points_offdiag"</span><span class="p">]</span>
                        <span class="p">)</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">subset</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">dim</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">subset</span><span class="p">)):</span>
            <span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="n">row</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">subset</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>
            <span class="n">x0</span><span class="p">,</span> <span class="n">x1</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">get_xlim</span><span class="p">()</span>
            <span class="n">y0</span><span class="p">,</span> <span class="n">y1</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">get_ylim</span><span class="p">()</span>
            <span class="n">text_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s2">"fontsize"</span><span class="p">:</span> <span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s2">"font.size"</span><span class="p">]</span> <span class="o">*</span> <span class="mf">2.0</span><span class="p">}</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">x1</span> <span class="o">+</span> <span class="p">(</span><span class="n">x1</span> <span class="o">-</span> <span class="n">x0</span><span class="p">)</span> <span class="o">/</span> <span class="mf">8.0</span><span class="p">,</span> <span class="p">(</span><span class="n">y0</span> <span class="o">+</span> <span class="n">y1</span><span class="p">)</span> <span class="o">/</span> <span class="mf">2.0</span><span class="p">,</span> <span class="s2">"..."</span><span class="p">,</span> <span class="o">**</span><span class="n">text_kwargs</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">row</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">subset</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span>
                    <span class="n">x1</span> <span class="o">+</span> <span class="p">(</span><span class="n">x1</span> <span class="o">-</span> <span class="n">x0</span><span class="p">)</span> <span class="o">/</span> <span class="mf">12.0</span><span class="p">,</span>
                    <span class="n">y0</span> <span class="o">-</span> <span class="p">(</span><span class="n">y1</span> <span class="o">-</span> <span class="n">y0</span><span class="p">)</span> <span class="o">/</span> <span class="mf">1.5</span><span class="p">,</span>
                    <span class="s2">"..."</span><span class="p">,</span>
                    <span class="n">rotation</span><span class="o">=-</span><span class="mi">45</span><span class="p">,</span>
                    <span class="o">**</span><span class="n">text_kwargs</span>
                <span class="p">)</span>

    <span class="k">return</span> <span class="n">fig</span><span class="p">,</span> <span class="n">axes</span>
</code></pre>
</div>
</td>
</tr>
</table>
</details>
</div>
</div>
</article>
</div>
</div>
</main>
<footer class="md-footer">
<div class="md-footer-nav">
<nav aria-label="Footer" class="md-footer-nav__inner md-grid">
<a class="md-footer-nav__link md-footer-nav__link--prev" href="../contribute/" rel="prev" title="Contribute">
<div class="md-footer-nav__button md-icon">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"></path></svg>
</div>
<div class="md-footer-nav__title">
<div class="md-ellipsis">
<span class="md-footer-nav__direction">
                  Previous
                </span>
                Contribute
              </div>
</div>
</a>
<a class="md-footer-nav__link md-footer-nav__link--next" href="../faq/" rel="next" title="FAQ">
<div class="md-footer-nav__title">
<div class="md-ellipsis">
<span class="md-footer-nav__direction">
                  Next
                </span>
                FAQ
              </div>
</div>
<div class="md-footer-nav__button md-icon">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4z"></path></svg>
</div>
</a>
</nav>
</div>
<div class="md-footer-meta md-typeset">
<div class="md-footer-meta__inner md-grid">
<div class="md-footer-copyright">
        
        Made with
        <a href="https://squidfunk.github.io/mkdocs-material/" rel="noopener" target="_blank">
          Material for MkDocs
        </a>
</div>
<div class="md-footer-social">
<a class="md-footer-social__link" href="https://github.com/mackelab/sbi" rel="noopener" target="_blank" title="github.com">
<svg viewbox="0 0 480 512" xmlns="http://www.w3.org/2000/svg"><path d="M186.1 328.7c0 20.9-10.9 55.1-36.7 55.1s-36.7-34.2-36.7-55.1 10.9-55.1 36.7-55.1 36.7 34.2 36.7 55.1zM480 278.2c0 31.9-3.2 65.7-17.5 95-37.9 76.6-142.1 74.8-216.7 74.8-75.8 0-186.2 2.7-225.6-74.8-14.6-29-20.2-63.1-20.2-95 0-41.9 13.9-81.5 41.5-113.6-5.2-15.8-7.7-32.4-7.7-48.8 0-21.5 4.9-32.3 14.6-51.8 45.3 0 74.3 9 108.8 36 29-6.9 58.8-10 88.7-10 27 0 54.2 2.9 80.4 9.2 34-26.7 63-35.2 107.8-35.2 9.8 19.5 14.6 30.3 14.6 51.8 0 16.4-2.6 32.7-7.7 48.2 27.5 32.4 39 72.3 39 114.2zm-64.3 50.5c0-43.9-26.7-82.6-73.5-82.6-18.9 0-37 3.4-56 6-14.9 2.3-29.8 3.2-45.1 3.2-15.2 0-30.1-.9-45.1-3.2-18.7-2.6-37-6-56-6-46.8 0-73.5 38.7-73.5 82.6 0 87.8 80.4 101.3 150.4 101.3h48.2c70.3 0 150.6-13.4 150.6-101.3zm-82.6-55.1c-25.8 0-36.7 34.2-36.7 55.1s10.9 55.1 36.7 55.1 36.7-34.2 36.7-55.1-10.9-55.1-36.7-55.1z"></path></svg>
</a>
</div>
</div>
</div>
</footer>
</div>
<script src="../assets/javascripts/vendor.d710d30a.min.js"></script>
<script src="../assets/javascripts/bundle.5f27aba8.min.js"></script><script id="__lang" type="application/json">{"clipboard.copy": "Copy to clipboard", "clipboard.copied": "Copied to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.result.placeholder": "Type to start searching", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents"}</script>
<script>
        app = initialize({
          base: "..",
          features: [],
          search: Object.assign({
            worker: "../assets/javascripts/worker/search.27c6a5e6.min.js"
          }, typeof search !== "undefined" && search)
        })
      </script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML"></script>
</body>
</html>