{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"sbi : simulation-based inference \u00b6 sbi : A Python toolbox for simulation-based inference. Inference can be run in a single line of code: posterior = infer ( prior , simulator , num_simulations = 1000 , method = 'SNPE' ) To learn about the general motivation behind simulation-based inference, and the inference methods included in sbi , read on below. For example applications to canonical problems in neuroscience, browse the recent preprint Training deep neural density estimators to identify mechanistic models of neural dynamics . If you want to get started using sbi on your own problem, jump to installation and then check out the tutorial . Motivation and approach \u00b6 Many areas of science and engineering make extensive use of complex, stochastic, numerical simulations to describe the structure and dynamics of the processes being investigated. A key challenge in simulation-based science is constraining these simulation models\u2019 parameters, which are intepretable quantities, with observational data. Bayesian inference provides a general and powerful framework to invert the simulators, i.e. describe the parameters which are consistent both with empirical data and prior knowledge. In the case of simulators, a key quantity required for statistical inference, the likelihood of observed data given parameters, \\(\\mathcal{L}(\\theta) = p(x_o|\\theta)\\) , is typically intractable, rendering conventional statistical approaches inapplicable. sbi implements three powerful machine-learning methods that address this problem: Sequential Neural Posterior Estimation (SNPE), Sequential Neural Likelihood Estimation (SNLE), and Sequential Neural Ratio Estimation (SNRE). Depending on the characteristics of the problem, e.g. the dimensionalities of the parameter space and the observation space, one of the methods will be more suitable. Goal: Algorithmically identify mechanistic models which are consistent with data. Each of the methods above needs three inputs: A candidate mechanistic model, prior knowledge or constraints on model parameters, and observational data (or summary statistics thereof). The methods then proceed by sampling parameters from the prior followed by simulating synthetic data from these parameters, learning the (probabilistic) association between data (or data features) and underlying parameters, i.e. to learn statistical inference from simulated data. They way in which this association is learned differs between the above methods, but all use deep neural networks. This learned neural network is then applied to empirical data to derive the full space of parameters consistent with the data and the prior, i.e. the posterior distribution. High posterior probability is assigned to parameters which are consistent with both the data and the prior, low probability to inconsistent parameters. While SNPE directly learns the posterior distribution, SNLE and SNRE need an extra MCMC sampling step to construct a posterior. If needed, an initial estimate of the posterior can be used to adaptively generate additional informative simulations. Publications \u00b6 See Cranmer, Brehmer, Louppe (2020) for a recent review on simulation-based inference. The following papers offer additional details on the inference methods included in sbi : SNPE \u00b6 Fast \u03b5-free Inference of Simulation Models with Bayesian Conditional Density Estimation by Papamakarios & Murray (NeurIPS 2016) [PDF] [BibTeX] Flexible statistical inference for mechanistic models of neural dynamics by Lueckmann, Goncalves, Bassetto, \u00d6cal, Nonnenmacher & Macke (NeurIPS 2017) [PDF] [BibTeX] Automatic posterior transformation for likelihood-free inference by Greenberg, Nonnenmacher & Macke (ICML 2019) [PDF] [BibTeX] SNLE \u00b6 Sequential neural likelihood: Fast likelihood-free inference with autoregressive flows by Papamakarios, Sterratt & Murray (AISTATS 2019) [PDF] [BibTeX] SNRE \u00b6 Likelihood-free MCMC with Amortized Approximate Likelihood Ratios by Hermans, Begy & Louppe (ICML 2020) [PDF] On Contrastive Learning for Likelihood-free Inference Durkan, Murray & Papamakarios (ICML 2020) [PDF] .","title":"Home"},{"location":"#sbi-simulation-based-inference","text":"sbi : A Python toolbox for simulation-based inference. Inference can be run in a single line of code: posterior = infer ( prior , simulator , num_simulations = 1000 , method = 'SNPE' ) To learn about the general motivation behind simulation-based inference, and the inference methods included in sbi , read on below. For example applications to canonical problems in neuroscience, browse the recent preprint Training deep neural density estimators to identify mechanistic models of neural dynamics . If you want to get started using sbi on your own problem, jump to installation and then check out the tutorial .","title":"sbi: simulation-based inference"},{"location":"#motivation-and-approach","text":"Many areas of science and engineering make extensive use of complex, stochastic, numerical simulations to describe the structure and dynamics of the processes being investigated. A key challenge in simulation-based science is constraining these simulation models\u2019 parameters, which are intepretable quantities, with observational data. Bayesian inference provides a general and powerful framework to invert the simulators, i.e. describe the parameters which are consistent both with empirical data and prior knowledge. In the case of simulators, a key quantity required for statistical inference, the likelihood of observed data given parameters, \\(\\mathcal{L}(\\theta) = p(x_o|\\theta)\\) , is typically intractable, rendering conventional statistical approaches inapplicable. sbi implements three powerful machine-learning methods that address this problem: Sequential Neural Posterior Estimation (SNPE), Sequential Neural Likelihood Estimation (SNLE), and Sequential Neural Ratio Estimation (SNRE). Depending on the characteristics of the problem, e.g. the dimensionalities of the parameter space and the observation space, one of the methods will be more suitable. Goal: Algorithmically identify mechanistic models which are consistent with data. Each of the methods above needs three inputs: A candidate mechanistic model, prior knowledge or constraints on model parameters, and observational data (or summary statistics thereof). The methods then proceed by sampling parameters from the prior followed by simulating synthetic data from these parameters, learning the (probabilistic) association between data (or data features) and underlying parameters, i.e. to learn statistical inference from simulated data. They way in which this association is learned differs between the above methods, but all use deep neural networks. This learned neural network is then applied to empirical data to derive the full space of parameters consistent with the data and the prior, i.e. the posterior distribution. High posterior probability is assigned to parameters which are consistent with both the data and the prior, low probability to inconsistent parameters. While SNPE directly learns the posterior distribution, SNLE and SNRE need an extra MCMC sampling step to construct a posterior. If needed, an initial estimate of the posterior can be used to adaptively generate additional informative simulations.","title":"Motivation and approach"},{"location":"#publications","text":"See Cranmer, Brehmer, Louppe (2020) for a recent review on simulation-based inference. The following papers offer additional details on the inference methods included in sbi :","title":"Publications"},{"location":"#snpe","text":"Fast \u03b5-free Inference of Simulation Models with Bayesian Conditional Density Estimation by Papamakarios & Murray (NeurIPS 2016) [PDF] [BibTeX] Flexible statistical inference for mechanistic models of neural dynamics by Lueckmann, Goncalves, Bassetto, \u00d6cal, Nonnenmacher & Macke (NeurIPS 2017) [PDF] [BibTeX] Automatic posterior transformation for likelihood-free inference by Greenberg, Nonnenmacher & Macke (ICML 2019) [PDF] [BibTeX]","title":"SNPE"},{"location":"#snle","text":"Sequential neural likelihood: Fast likelihood-free inference with autoregressive flows by Papamakarios, Sterratt & Murray (AISTATS 2019) [PDF] [BibTeX]","title":"SNLE"},{"location":"#snre","text":"Likelihood-free MCMC with Amortized Approximate Likelihood Ratios by Hermans, Begy & Louppe (ICML 2020) [PDF] On Contrastive Learning for Likelihood-free Inference Durkan, Murray & Papamakarios (ICML 2020) [PDF] .","title":"SNRE"},{"location":"contribute/","text":"User experiences, bugs, and feature requests \u00b6 If you are using sbi to infer the parameters of a simulator, we would be delighted to know how it worked for you. If it didn\u2019t work according to plan, please open up an issue and tell us more about your use case: the dimensionality of the input parameters and of the output, as well as the setup you used to run inference (i.e. number of simulations, number of rounds,\u2026). To report bugs and suggest features (including better documentation), please equally head over to issues on GitHub . Code contributions \u00b6 In general, we use pull requests to make changes to sbi . Development environment \u00b6 Clone the repo and install all the dependencies using the environment.yml file to create a conda environment: conda env create -f environment.yml . If you already have an sbi environment and want to refresh dependencies, just run conda env update -f environment.yml --prune . Alternatively, you can install via setup.py using pip install -e \".[dev]\" (the dev flag installs development and testing dependencies). Contributing inference algorithms \u00b6 sbi was developed to be extensible and we welcome implementations of additional inference algorithms. Your new inference algorithm should be a class in sbi/inference/your_type_of_algorithm/your_algorithm.py . The class should have a __call__() function which runs inference and returns a posterior object. The posterior object itself should have a .sample() function following the signature of sbi/inference/NeuralPosterior , allowing to draw samples from the posterior. Currently, SNPE , SNLE , and SNRE all share the NeuralPosterior class in sbi/inference/posterior.py , but future versions of sbi will refactor them into separate classes. Style conventions \u00b6 For docstrings and comments, we use Google Style . Code needs to pass through the following tools, which are installed alongside sbi : black : Automatic code formatting for Python. You can run black manually from the console using black . in the top directory of the repository, which will format all files. isort : Used to consistently order imports. You can run isort manually from the console using isort -y in the top directory. Online documentation \u00b6 Most of the documentation is written in markdown ( basic markdown guide ). You can directly fix mistakes and suggest clearer formulations in markdown files simply by initiating a PR on through GitHub. Click on documentation file and look for the little pencil at top right.","title":"Contribute"},{"location":"contribute/#user-experiences-bugs-and-feature-requests","text":"If you are using sbi to infer the parameters of a simulator, we would be delighted to know how it worked for you. If it didn\u2019t work according to plan, please open up an issue and tell us more about your use case: the dimensionality of the input parameters and of the output, as well as the setup you used to run inference (i.e. number of simulations, number of rounds,\u2026). To report bugs and suggest features (including better documentation), please equally head over to issues on GitHub .","title":"User experiences, bugs, and feature requests"},{"location":"contribute/#code-contributions","text":"In general, we use pull requests to make changes to sbi .","title":"Code contributions"},{"location":"contribute/#development-environment","text":"Clone the repo and install all the dependencies using the environment.yml file to create a conda environment: conda env create -f environment.yml . If you already have an sbi environment and want to refresh dependencies, just run conda env update -f environment.yml --prune . Alternatively, you can install via setup.py using pip install -e \".[dev]\" (the dev flag installs development and testing dependencies).","title":"Development environment"},{"location":"contribute/#contributing-inference-algorithms","text":"sbi was developed to be extensible and we welcome implementations of additional inference algorithms. Your new inference algorithm should be a class in sbi/inference/your_type_of_algorithm/your_algorithm.py . The class should have a __call__() function which runs inference and returns a posterior object. The posterior object itself should have a .sample() function following the signature of sbi/inference/NeuralPosterior , allowing to draw samples from the posterior. Currently, SNPE , SNLE , and SNRE all share the NeuralPosterior class in sbi/inference/posterior.py , but future versions of sbi will refactor them into separate classes.","title":"Contributing inference algorithms"},{"location":"contribute/#style-conventions","text":"For docstrings and comments, we use Google Style . Code needs to pass through the following tools, which are installed alongside sbi : black : Automatic code formatting for Python. You can run black manually from the console using black . in the top directory of the repository, which will format all files. isort : Used to consistently order imports. You can run isort manually from the console using isort -y in the top directory.","title":"Style conventions"},{"location":"contribute/#online-documentation","text":"Most of the documentation is written in markdown ( basic markdown guide ). You can directly fix mistakes and suggest clearer formulations in markdown files simply by initiating a PR on through GitHub. Click on documentation file and look for the little pencil at top right.","title":"Online documentation"},{"location":"credits/","text":"Credits \u00b6 sbi is licensed under the Affero General Public License version 3 (AGPLv3) and Copyright (C) 2020 \u00c1lvaro Tejero-Cantero, Jakob H. Macke, Jan-Matthis L\u00fcckmann, Michael Deistler, Jan F. B\u00f6lts. Copyright (C) 2020 Conor M. Durkan. Important dependencies and prior art \u00b6 sbi is the successor to delfi , a Theano-based toolbox for sequential neural posterior estimation developed at mackelab . If you were using delfi , we strongly recommend to move your inference over to sbi . Please open issues if you find unexpected behaviour or missing features. We will consider these bugs and give them priority. sbi as a PyTorch-based toolbox started as a fork of conormdurkan/lfi , by Conor M.Durkan . sbi uses density estimators from bayesiains/nflows by Conor M.Durkan , George Papamakarios and Artur Bekasov . These are proxied through pyknos , a package focused on density estimation. sbi uses PyTorch and tries to align with the interfaces (e.g. for probability distributions) adopted by PyTorch . See README.md for a list of publications describing the methods implemented in sbi .","title":"Credits"},{"location":"credits/#credits","text":"sbi is licensed under the Affero General Public License version 3 (AGPLv3) and Copyright (C) 2020 \u00c1lvaro Tejero-Cantero, Jakob H. Macke, Jan-Matthis L\u00fcckmann, Michael Deistler, Jan F. B\u00f6lts. Copyright (C) 2020 Conor M. Durkan.","title":"Credits"},{"location":"credits/#important-dependencies-and-prior-art","text":"sbi is the successor to delfi , a Theano-based toolbox for sequential neural posterior estimation developed at mackelab . If you were using delfi , we strongly recommend to move your inference over to sbi . Please open issues if you find unexpected behaviour or missing features. We will consider these bugs and give them priority. sbi as a PyTorch-based toolbox started as a fork of conormdurkan/lfi , by Conor M.Durkan . sbi uses density estimators from bayesiains/nflows by Conor M.Durkan , George Papamakarios and Artur Bekasov . These are proxied through pyknos , a package focused on density estimation. sbi uses PyTorch and tries to align with the interfaces (e.g. for probability distributions) adopted by PyTorch . See README.md for a list of publications describing the methods implemented in sbi .","title":"Important dependencies and prior art"},{"location":"install/","text":"Installation \u00b6 sbi requires Python 3.7 or higher. It can be installed using pip : $ pip install sbi We recommend to use a conda virtual environment ( Miniconda installation instructions ). If conda is installed on the system, an environment for installing sbi can be created as follows: # Create an environment for sbi (indicate Python 3.7 or higher); activate it $ conda create -n sbi_env python=3.7 && conda activate sbi_env To test the installation, drop into a python prompt and run from sbi.examples.minimal import simple posterior = simple () print ( posterior )","title":"Installation"},{"location":"install/#installation","text":"sbi requires Python 3.7 or higher. It can be installed using pip : $ pip install sbi We recommend to use a conda virtual environment ( Miniconda installation instructions ). If conda is installed on the system, an environment for installing sbi can be created as follows: # Create an environment for sbi (indicate Python 3.7 or higher); activate it $ conda create -n sbi_env python=3.7 && conda activate sbi_env To test the installation, drop into a python prompt and run from sbi.examples.minimal import simple posterior = simple () print ( posterior )","title":"Installation"},{"location":"reference/","text":"API Reference \u00b6 Inference \u00b6 sbi.inference.base.infer ( simulator , prior , method , num_simulations , num_workers = 1 ) \u00b6 Return posterior distribution by running simulation-based inference. This function provides a simple interface to run sbi. Inference is run for a single round and hence the returned posterior p(\\theta|x) p(\\theta|x) can be sampled and evaluated for any x x (i.e. it is amortized). The scope of this function is limited to the most essential features of sbi. For more flexibility (e.g. multi-round inference, different density estimators) please use the flexible interface described here: https://www.mackelab.org/sbi/tutorial/03_flexible_interface/ Parameters: Name Type Description Default simulator Callable A function that takes parameters \\theta \\theta and maps them to simulations, or observations, x , \\mathrm{sim}(\\theta)\\to x \\mathrm{sim}(\\theta)\\to x . Any regular Python callable (i.e. function or class with __call__ method) can be used. required prior A probability distribution that expresses prior knowledge about the parameters, e.g. which ranges are meaningful for them. Any object with .log_prob() and .sample() (for example, a PyTorch distribution) can be used. required method str What inference method to use. Either of SNPE, SNLE or SNRE. required num_simulations int Number of simulation calls. More simulations means a longer runtime, but a better posterior estimate. required num_workers int Number of parallel workers to use for simulations. 1 Returns: Posterior over parameters conditional on observations (amortized). Source code in sbi/inference/base.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 def infer ( simulator : Callable , prior , method : str , num_simulations : int , num_workers : int = 1 ) -> NeuralPosterior : r \"\"\" Return posterior distribution by running simulation-based inference. This function provides a simple interface to run sbi. Inference is run for a single round and hence the returned posterior $p(\\theta|x)$ can be sampled and evaluated for any $x$ (i.e. it is amortized). The scope of this function is limited to the most essential features of sbi. For more flexibility (e.g. multi-round inference, different density estimators) please use the flexible interface described here: https://www.mackelab.org/sbi/tutorial/03_flexible_interface/ Args: simulator: A function that takes parameters $\\theta$ and maps them to simulations, or observations, `x`, $\\mathrm{sim}(\\theta)\\to x$. Any regular Python callable (i.e. function or class with `__call__` method) can be used. prior: A probability distribution that expresses prior knowledge about the parameters, e.g. which ranges are meaningful for them. Any object with `.log_prob()`and `.sample()` (for example, a PyTorch distribution) can be used. method: What inference method to use. Either of SNPE, SNLE or SNRE. num_simulations: Number of simulation calls. More simulations means a longer runtime, but a better posterior estimate. num_workers: Number of parallel workers to use for simulations. Returns: Posterior over parameters conditional on observations (amortized). \"\"\" try : method_fun : Callable = getattr ( sbi . inference , method . upper ()) except AttributeError : raise NameError ( \"Method not available. `method` must be one of 'SNPE', 'SNLE', 'SNRE'.\" ) simulator , prior = prepare_for_sbi ( simulator , prior ) infer_ = method_fun ( simulator , prior , num_workers = num_workers ) posterior = infer_ ( num_simulations = num_simulations ) return posterior sbi.user_input.user_input_checks.prepare_for_sbi ( simulator , prior ) \u00b6 Prepare simulator, prior and for usage in sbi. One of the goals is to allow you to use sbi with inputs computed in numpy. Attempts to meet the following requirements by reshaping and type-casting: - the simulator function receives as input and returns a Tensor. - the simulator can simulate batches of parameters and return batches of data. - the prior does not produce batches and samples and evaluates to Tensor. - the output shape is a torch.Size((1,N)) (i.e, has a leading batch dimension 1). If this is not possible, a suitable exception will be raised. Parameters: Name Type Description Default simulator Callable Simulator as provided by the user. required prior Prior as provided by the user. required Returns: Type Description Tuple[Callable, torch.distributions.distribution.Distribution] Tuple (simulator, prior, x_shape) checked and matching the requirements of sbi. Source code in sbi/user_input/user_input_checks.py 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 def prepare_for_sbi ( simulator : Callable , prior ,) -> Tuple [ Callable , Distribution ]: \"\"\"Prepare simulator, prior and for usage in sbi. One of the goals is to allow you to use sbi with inputs computed in numpy. Attempts to meet the following requirements by reshaping and type-casting: - the simulator function receives as input and returns a Tensor. - the simulator can simulate batches of parameters and return batches of data. - the prior does not produce batches and samples and evaluates to Tensor. - the output shape is a `torch.Size((1,N))` (i.e, has a leading batch dimension 1). If this is not possible, a suitable exception will be raised. Args: simulator: Simulator as provided by the user. prior: Prior as provided by the user. Returns: Tuple (simulator, prior, x_shape) checked and matching the requirements of sbi. \"\"\" # Check prior, return PyTorch prior. prior , _ , prior_returns_numpy = process_prior ( prior ) # Check simulator, returns PyTorch simulator able to simulate batches. simulator = process_simulator ( simulator , prior , prior_returns_numpy ) # Consistency check after making ready for sbi. check_sbi_inputs ( simulator , prior ) return simulator , prior sbi.inference.snpe.snpe_c.SNPE_C \u00b6 __call__ ( self , num_simulations , proposal = None , num_atoms = 10 , training_batch_size = 50 , learning_rate = 0.0005 , validation_fraction = 0.1 , stop_after_epochs = 20 , max_num_epochs = None , clip_max_norm = 5.0 , calibration_kernel = None , exclude_invalid_x = True , discard_prior_samples = False , retrain_from_scratch_each_round = False ) special Run SNPE. Return posterior p(\\theta|x) p(\\theta|x) after inference. Parameters: Name Type Description Default num_simulations int Number of simulator calls. required proposal Optional[Any] Distribution that the parameters \\theta \\theta are drawn from. proposal=None uses the prior. Setting the proposal to a distribution targeted on a specific observation, e.g. a posterior p(\\theta|x_o) p(\\theta|x_o) obtained previously, can lead to less required simulations. None num_atoms int Number of atoms to use for classification. 10 training_batch_size int Training batch size. 50 learning_rate float Learning rate for Adam optimizer. 0.0005 validation_fraction float The fraction of data to use for validation. 0.1 stop_after_epochs int The number of epochs to wait for improvement on the validation set before terminating training. 20 max_num_epochs Optional[int] Maximum number of epochs to run. If reached, we stop training even when the validation loss is still decreasing. If None, we train until validation loss increases (see also stop_after_epochs ). None clip_max_norm Optional[float] Value at which to clip the total gradient norm in order to prevent exploding gradients. Use None for no clipping. 5.0 calibration_kernel Optional[Callable] A function to calibrate the loss with respect to the simulations x . See Lueckmann, Gon\u00e7alves et al., NeurIPS 2017. None exclude_invalid_x bool Whether to exclude simulation outputs x=NaN or x=\u00b1\u221e during training. Expect errors, silent or explicit, when False . True discard_prior_samples bool Whether to discard samples simulated in round 1, i.e. from the prior. Training may be sped up by ignoring such less targeted samples. False retrain_from_scratch_each_round bool Whether to retrain the conditional density estimator for the posterior from scratch each round. False Returns: Type Description NeuralPosterior Posterior p(\\theta|x) p(\\theta|x) that can be sampled and evaluated. Source code in sbi/inference/snpe/snpe_c.py 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 def __call__ ( self , num_simulations : int , proposal : Optional [ Any ] = None , num_atoms : int = 10 , training_batch_size : int = 50 , learning_rate : float = 5e-4 , validation_fraction : float = 0.1 , stop_after_epochs : int = 20 , max_num_epochs : Optional [ int ] = None , clip_max_norm : Optional [ float ] = 5.0 , calibration_kernel : Optional [ Callable ] = None , exclude_invalid_x : bool = True , discard_prior_samples : bool = False , retrain_from_scratch_each_round : bool = False , ) -> NeuralPosterior : r \"\"\"Run SNPE. Return posterior $p(\\theta|x)$ after inference. Args: num_simulations: Number of simulator calls. proposal: Distribution that the parameters $\\theta$ are drawn from. `proposal=None` uses the prior. Setting the proposal to a distribution targeted on a specific observation, e.g. a posterior $p(\\theta|x_o)$ obtained previously, can lead to less required simulations. num_atoms: Number of atoms to use for classification. training_batch_size: Training batch size. learning_rate: Learning rate for Adam optimizer. validation_fraction: The fraction of data to use for validation. stop_after_epochs: The number of epochs to wait for improvement on the validation set before terminating training. max_num_epochs: Maximum number of epochs to run. If reached, we stop training even when the validation loss is still decreasing. If None, we train until validation loss increases (see also `stop_after_epochs`). clip_max_norm: Value at which to clip the total gradient norm in order to prevent exploding gradients. Use None for no clipping. calibration_kernel: A function to calibrate the loss with respect to the simulations `x`. See Lueckmann, Gon\u00e7alves et al., NeurIPS 2017. exclude_invalid_x: Whether to exclude simulation outputs `x=NaN` or `x=\u00b1\u221e` during training. Expect errors, silent or explicit, when `False`. discard_prior_samples: Whether to discard samples simulated in round 1, i.e. from the prior. Training may be sped up by ignoring such less targeted samples. retrain_from_scratch_each_round: Whether to retrain the conditional density estimator for the posterior from scratch each round. Returns: Posterior $p(\\theta|x)$ that can be sampled and evaluated. \"\"\" # WARNING: sneaky trick ahead. We proxy the parent's `__call__` here, # requiring the signature to have `num_atoms`, save it for use below, and # continue. It's sneaky because we are using the object (self) as a namespace # to pass arguments between functions, and that's implicit state management. self . _num_atoms = num_atoms kwargs = del_entries ( locals (), entries = ( \"self\" , \"__class__\" , \"num_atoms\" )) return super () . __call__ ( ** kwargs ) __init__ ( self , simulator , prior , num_workers = 1 , simulation_batch_size = 1 , density_estimator = 'maf' , sample_with_mcmc = False , mcmc_method = 'slice_np' , mcmc_parameters = None , use_combined_loss = False , device = 'cpu' , logging_level = 'WARNING' , summary_writer = None , show_progress_bars = True , show_round_summary = False ) special SNPE-C / APT [1]. [1] Automatic Posterior Transformation for Likelihood-free Inference , Greenberg et al., ICML 2019, https://arxiv.org/abs/1905.07488 . Parameters: Name Type Description Default simulator Callable A function that takes parameters \\theta \\theta and maps them to simulations, or observations, x , \\mathrm{sim}(\\theta)\\to x \\mathrm{sim}(\\theta)\\to x . Any regular Python callable (i.e. function or class with __call__ method) can be used. required prior A probability distribution that expresses prior knowledge about the parameters, e.g. which ranges are meaningful for them. Any object with .log_prob() and .sample() (for example, a PyTorch distribution) can be used. required num_workers int Number of parallel workers to use for simulations. 1 simulation_batch_size int Number of parameter sets that the simulator maps to data x at once. If None, we simulate all parameter sets at the same time. If >= 1, the simulator has to process data of shape (simulation_batch_size, parameter_dimension). 1 density_estimator Union[str, Callable] If it is a string, use a pre-configured network of the provided type (one of nsf, maf, mdn, made). Alternatively, a function that builds a custom neural network can be provided. The function will be called with the first batch of simulations (theta, x), which can thus be used for shape inference and potentially for z-scoring. It needs to return a PyTorch nn.Module implementing the density estimator. The density estimator needs to provide the methods .log_prob and .sample() . 'maf' sample_with_mcmc bool Whether to sample with MCMC. MCMC can be used to deal with high leakage. False mcmc_method str Method used for MCMC sampling, one of slice_np , slice , hmc , nuts . Currently defaults to slice_np for a custom numpy implementation of slice sampling; select hmc , nuts or slice for Pyro-based sampling. 'slice_np' mcmc_parameters Optional[Dict[str, Any]] Dictionary overriding the default parameters for MCMC. The following parameters are supported: thin to set the thinning factor for the chain, warmup_steps to set the initial number of samples to discard, num_chains for the number of chains, init_strategy for the initialisation strategy for chains; prior will draw init locations from prior, whereas sir will use Sequential-Importance- Resampling using init_strategy_num_candidates to find init locations. None use_combined_loss bool Whether to train the neural net also on prior samples using maximum likelihood in addition to training it on all samples using atomic loss. The extra MLE loss helps prevent density leaking with bounded priors. False device str torch device on which to compute, e.g. gpu, cpu. 'cpu' logging_level Union[int, str] Minimum severity of messages to log. One of the strings INFO, WARNING, DEBUG, ERROR and CRITICAL. 'WARNING' summary_writer Optional[Writer] A tensorboard SummaryWriter to control, among others, log file location (default is <current working directory>/logs .) None show_progress_bars bool Whether to show a progressbar during simulation and sampling. True show_round_summary bool Whether to show the validation loss and leakage after each round. False Source code in sbi/inference/snpe/snpe_c.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 def __init__ ( self , simulator : Callable , prior , num_workers : int = 1 , simulation_batch_size : int = 1 , density_estimator : Union [ str , Callable ] = \"maf\" , sample_with_mcmc : bool = False , mcmc_method : str = \"slice_np\" , mcmc_parameters : Optional [ Dict [ str , Any ]] = None , use_combined_loss : bool = False , device : str = \"cpu\" , logging_level : Union [ int , str ] = \"WARNING\" , summary_writer : Optional [ TensorboardSummaryWriter ] = None , show_progress_bars : bool = True , show_round_summary : bool = False , ): r \"\"\"SNPE-C / APT [1]. [1] _Automatic Posterior Transformation for Likelihood-free Inference_, Greenberg et al., ICML 2019, https://arxiv.org/abs/1905.07488. Args: simulator: A function that takes parameters $\\theta$ and maps them to simulations, or observations, `x`, $\\mathrm{sim}(\\theta)\\to x$. Any regular Python callable (i.e. function or class with `__call__` method) can be used. prior: A probability distribution that expresses prior knowledge about the parameters, e.g. which ranges are meaningful for them. Any object with `.log_prob()`and `.sample()` (for example, a PyTorch distribution) can be used. num_workers: Number of parallel workers to use for simulations. simulation_batch_size: Number of parameter sets that the simulator maps to data x at once. If None, we simulate all parameter sets at the same time. If >= 1, the simulator has to process data of shape (simulation_batch_size, parameter_dimension). density_estimator: If it is a string, use a pre-configured network of the provided type (one of nsf, maf, mdn, made). Alternatively, a function that builds a custom neural network can be provided. The function will be called with the first batch of simulations (theta, x), which can thus be used for shape inference and potentially for z-scoring. It needs to return a PyTorch `nn.Module` implementing the density estimator. The density estimator needs to provide the methods `.log_prob` and `.sample()`. sample_with_mcmc: Whether to sample with MCMC. MCMC can be used to deal with high leakage. mcmc_method: Method used for MCMC sampling, one of `slice_np`, `slice`, `hmc`, `nuts`. Currently defaults to `slice_np` for a custom numpy implementation of slice sampling; select `hmc`, `nuts` or `slice` for Pyro-based sampling. mcmc_parameters: Dictionary overriding the default parameters for MCMC. The following parameters are supported: `thin` to set the thinning factor for the chain, `warmup_steps` to set the initial number of samples to discard, `num_chains` for the number of chains, `init_strategy` for the initialisation strategy for chains; `prior` will draw init locations from prior, whereas `sir` will use Sequential-Importance- Resampling using `init_strategy_num_candidates` to find init locations. use_combined_loss: Whether to train the neural net also on prior samples using maximum likelihood in addition to training it on all samples using atomic loss. The extra MLE loss helps prevent density leaking with bounded priors. device: torch device on which to compute, e.g. gpu, cpu. logging_level: Minimum severity of messages to log. One of the strings INFO, WARNING, DEBUG, ERROR and CRITICAL. summary_writer: A tensorboard `SummaryWriter` to control, among others, log file location (default is `<current working directory>/logs`.) show_progress_bars: Whether to show a progressbar during simulation and sampling. show_round_summary: Whether to show the validation loss and leakage after each round. \"\"\" self . _use_combined_loss = use_combined_loss kwargs = del_entries ( locals (), entries = ( \"self\" , \"__class__\" , \"use_combined_loss\" ) ) super () . __init__ ( ** kwargs ) provide_presimulated ( self , theta , x , from_round = 0 ) inherited Provide external \\theta \\theta and x x to be used for training later on. Parameters: Name Type Description Default theta Tensor Parameter sets used to generate presimulated data. required x Tensor Simulation outputs of presimulated data. required from_round int Which round the data was simulated from. from_round=0 means that the data came from the first round, i.e. the prior. 0 Source code in sbi/inference/snpe/snpe_c.py 164 165 166 167 168 169 170 171 172 173 174 175 176 def provide_presimulated ( self , theta : Tensor , x : Tensor , from_round : int = 0 ) -> None : r \"\"\" Provide external $\\theta$ and $x$ to be used for training later on. Args: theta: Parameter sets used to generate presimulated data. x: Simulation outputs of presimulated data. from_round: Which round the data was simulated from. `from_round=0` means that the data came from the first round, i.e. the prior. \"\"\" self . _append_to_data_bank ( theta , x , from_round ) sbi.inference.snle.snle_a.SNLE_A \u00b6 __call__ ( self , num_simulations , proposal = None , training_batch_size = 50 , learning_rate = 0.0005 , validation_fraction = 0.1 , stop_after_epochs = 20 , max_num_epochs = None , clip_max_norm = 5.0 , exclude_invalid_x = True , discard_prior_samples = False , retrain_from_scratch_each_round = False ) special Run SNLE. Return posterior p(\\theta|x) p(\\theta|x) after inference. Parameters: Name Type Description Default num_simulations int Number of simulator calls. required proposal Optional[Any] Distribution that the parameters \\theta \\theta are drawn from. proposal=None uses the prior. Setting the proposal to a distribution targeted on a specific observation, e.g. a posterior p(\\theta|x_o) p(\\theta|x_o) obtained previously, can lead to less required simulations. None training_batch_size int Training batch size. 50 learning_rate float Learning rate for Adam optimizer. 0.0005 validation_fraction float The fraction of data to use for validation. 0.1 stop_after_epochs int The number of epochs to wait for improvement on the validation set before terminating training. 20 max_num_epochs Optional[int] Maximum number of epochs to run. If reached, we stop training even when the validation loss is still decreasing. If None, we train until validation loss increases (see also stop_after_epochs ). None clip_max_norm Optional[float] Value at which to clip the total gradient norm in order to prevent exploding gradients. Use None for no clipping. 5.0 exclude_invalid_x bool Whether to exclude simulation outputs x=NaN or x=\u00b1\u221e during training. Expect errors, silent or explicit, when False . True discard_prior_samples bool Whether to discard samples simulated in round 1, i.e. from the prior. Training may be sped up by ignoring such less targeted samples. False retrain_from_scratch_each_round bool Whether to retrain the conditional density estimator for the posterior from scratch each round. False Returns: Type Description NeuralPosterior Posterior p(\\theta|x_o) p(\\theta|x_o) that can be sampled and evaluated. Source code in sbi/inference/snle/snle_a.py 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 def __call__ ( self , num_simulations : int , proposal : Optional [ Any ] = None , training_batch_size : int = 50 , learning_rate : float = 5e-4 , validation_fraction : float = 0.1 , stop_after_epochs : int = 20 , max_num_epochs : Optional [ int ] = None , clip_max_norm : Optional [ float ] = 5.0 , exclude_invalid_x : bool = True , discard_prior_samples : bool = False , retrain_from_scratch_each_round : bool = False , ) -> NeuralPosterior : r \"\"\"Run SNLE. Return posterior $p(\\theta|x)$ after inference. Args: num_simulations: Number of simulator calls. proposal: Distribution that the parameters $\\theta$ are drawn from. `proposal=None` uses the prior. Setting the proposal to a distribution targeted on a specific observation, e.g. a posterior $p(\\theta|x_o)$ obtained previously, can lead to less required simulations. training_batch_size: Training batch size. learning_rate: Learning rate for Adam optimizer. validation_fraction: The fraction of data to use for validation. stop_after_epochs: The number of epochs to wait for improvement on the validation set before terminating training. max_num_epochs: Maximum number of epochs to run. If reached, we stop training even when the validation loss is still decreasing. If None, we train until validation loss increases (see also `stop_after_epochs`). clip_max_norm: Value at which to clip the total gradient norm in order to prevent exploding gradients. Use None for no clipping. exclude_invalid_x: Whether to exclude simulation outputs `x=NaN` or `x=\u00b1\u221e` during training. Expect errors, silent or explicit, when `False`. discard_prior_samples: Whether to discard samples simulated in round 1, i.e. from the prior. Training may be sped up by ignoring such less targeted samples. retrain_from_scratch_each_round: Whether to retrain the conditional density estimator for the posterior from scratch each round. Returns: Posterior $p(\\theta|x_o)$ that can be sampled and evaluated. \"\"\" kwargs = del_entries ( locals (), entries = ( \"self\" , \"__class__\" )) return super () . __call__ ( ** kwargs ) __init__ ( self , simulator , prior , num_workers = 1 , simulation_batch_size = 1 , density_estimator = 'maf' , mcmc_method = 'slice_np' , mcmc_parameters = None , device = 'cpu' , logging_level = 'WARNING' , summary_writer = None , show_progress_bars = True , show_round_summary = False ) special Sequential Neural Likelihood [1]. [1] Sequential Neural Likelihood: Fast Likelihood-free Inference with Autoregressive Flows_, Papamakarios et al., AISTATS 2019, https://arxiv.org/abs/1805.07226 Parameters: Name Type Description Default simulator Callable A function that takes parameters \\theta \\theta and maps them to simulations, or observations, x , \\text{sim}(\\theta)\\to x \\text{sim}(\\theta)\\to x . Any regular Python callable (i.e. function or class with __call__ method) can be used. required prior A probability distribution that expresses prior knowledge about the parameters, e.g. which ranges are meaningful for them. Any object with .log_prob() and .sample() (for example, a PyTorch distribution) can be used. required num_workers int Number of parallel workers to use for simulations. 1 simulation_batch_size int Number of parameter sets that the simulator maps to data x at once. If None, we simulate all parameter sets at the same time. If >= 1, the simulator has to process data of shape (simulation_batch_size, parameter_dimension). 1 density_estimator Union[str, Callable] If it is a string, use a pre-configured network of the provided type (one of nsf, maf, mdn, made). Alternatively, a function that builds a custom neural network can be provided. The function will be called with the first batch of simulations (theta, x), which can thus be used for shape inference and potentially for z-scoring. It needs to return a PyTorch nn.Module implementing the density estimator. The density estimator needs to provide the methods .log_prob and .sample() . 'maf' mcmc_method str Method used for MCMC sampling, one of slice_np , slice , hmc , nuts . Currently defaults to slice_np for a custom numpy implementation of slice sampling; select hmc , nuts or slice for Pyro-based sampling. 'slice_np' mcmc_parameters Optional[Dict[str, Any]] Dictionary overriding the default parameters for MCMC. The following parameters are supported: thin to set the thinning factor for the chain, warmup_steps to set the initial number of samples to discard, num_chains for the number of chains, init_strategy for the initialisation strategy for chains; prior will draw init locations from prior, whereas sir will use Sequential-Importance- Resampling using init_strategy_num_candidates to find init locations. None device str torch device on which to compute, e.g. gpu, cpu. 'cpu' logging_level Union[int, str] Minimum severity of messages to log. One of the strings INFO, WARNING, DEBUG, ERROR and CRITICAL. 'WARNING' summary_writer Optional[Writer] A tensorboard SummaryWriter to control, among others, log file location (default is <current working directory>/logs .) None show_progress_bars bool Whether to show a progressbar during simulation and sampling. True show_round_summary bool Whether to show the validation loss and leakage after each round. False Source code in sbi/inference/snle/snle_a.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 def __init__ ( self , simulator : Callable , prior , num_workers : int = 1 , simulation_batch_size : int = 1 , density_estimator : Union [ str , Callable ] = \"maf\" , mcmc_method : str = \"slice_np\" , mcmc_parameters : Optional [ Dict [ str , Any ]] = None , device : str = \"cpu\" , logging_level : Union [ int , str ] = \"WARNING\" , summary_writer : Optional [ TensorboardSummaryWriter ] = None , show_progress_bars : bool = True , show_round_summary : bool = False , ): r \"\"\"Sequential Neural Likelihood [1]. [1] Sequential Neural Likelihood: Fast Likelihood-free Inference with Autoregressive Flows_, Papamakarios et al., AISTATS 2019, https://arxiv.org/abs/1805.07226 Args: simulator: A function that takes parameters $\\theta$ and maps them to simulations, or observations, `x`, $\\text{sim}(\\theta)\\to x$. Any regular Python callable (i.e. function or class with `__call__` method) can be used. prior: A probability distribution that expresses prior knowledge about the parameters, e.g. which ranges are meaningful for them. Any object with `.log_prob()`and `.sample()` (for example, a PyTorch distribution) can be used. num_workers: Number of parallel workers to use for simulations. simulation_batch_size: Number of parameter sets that the simulator maps to data x at once. If None, we simulate all parameter sets at the same time. If >= 1, the simulator has to process data of shape (simulation_batch_size, parameter_dimension). density_estimator: If it is a string, use a pre-configured network of the provided type (one of nsf, maf, mdn, made). Alternatively, a function that builds a custom neural network can be provided. The function will be called with the first batch of simulations (theta, x), which can thus be used for shape inference and potentially for z-scoring. It needs to return a PyTorch `nn.Module` implementing the density estimator. The density estimator needs to provide the methods `.log_prob` and `.sample()`. mcmc_method: Method used for MCMC sampling, one of `slice_np`, `slice`, `hmc`, `nuts`. Currently defaults to `slice_np` for a custom numpy implementation of slice sampling; select `hmc`, `nuts` or `slice` for Pyro-based sampling. mcmc_parameters: Dictionary overriding the default parameters for MCMC. The following parameters are supported: `thin` to set the thinning factor for the chain, `warmup_steps` to set the initial number of samples to discard, `num_chains` for the number of chains, `init_strategy` for the initialisation strategy for chains; `prior` will draw init locations from prior, whereas `sir` will use Sequential-Importance- Resampling using `init_strategy_num_candidates` to find init locations. device: torch device on which to compute, e.g. gpu, cpu. logging_level: Minimum severity of messages to log. One of the strings INFO, WARNING, DEBUG, ERROR and CRITICAL. summary_writer: A tensorboard `SummaryWriter` to control, among others, log file location (default is `<current working directory>/logs`.) show_progress_bars: Whether to show a progressbar during simulation and sampling. show_round_summary: Whether to show the validation loss and leakage after each round. \"\"\" kwargs = del_entries ( locals (), entries = ( \"self\" , \"__class__\" )) super () . __init__ ( ** kwargs ) provide_presimulated ( self , theta , x , from_round = 0 ) inherited Provide external \\theta \\theta and x x to be used for training later on. Parameters: Name Type Description Default theta Tensor Parameter sets used to generate presimulated data. required x Tensor Simulation outputs of presimulated data. required from_round int Which round the data was simulated from. from_round=0 means that the data came from the first round, i.e. the prior. 0 Source code in sbi/inference/snle/snle_a.py 164 165 166 167 168 169 170 171 172 173 174 175 176 def provide_presimulated ( self , theta : Tensor , x : Tensor , from_round : int = 0 ) -> None : r \"\"\" Provide external $\\theta$ and $x$ to be used for training later on. Args: theta: Parameter sets used to generate presimulated data. x: Simulation outputs of presimulated data. from_round: Which round the data was simulated from. `from_round=0` means that the data came from the first round, i.e. the prior. \"\"\" self . _append_to_data_bank ( theta , x , from_round ) sbi.inference.snre.snre_a.SNRE_A \u00b6 __call__ ( self , num_simulations , proposal = None , training_batch_size = 50 , learning_rate = 0.0005 , validation_fraction = 0.1 , stop_after_epochs = 20 , max_num_epochs = None , clip_max_norm = 5.0 , exclude_invalid_x = True , discard_prior_samples = False , retrain_from_scratch_each_round = False ) special Run AALR / SNRE_A. Return posterior p(\\theta|x) p(\\theta|x) after inference. Parameters: Name Type Description Default num_simulations int Number of simulator calls. required proposal Optional[Any] Distribution that the parameters \\theta \\theta are drawn from. proposal=None uses the prior. Setting the proposal to a distribution targeted on a specific observation, e.g. a posterior p(\\theta|x_o) p(\\theta|x_o) obtained previously, can lead to less required simulations. None training_batch_size int Training batch size. 50 learning_rate float Learning rate for Adam optimizer. 0.0005 validation_fraction float The fraction of data to use for validation. 0.1 stop_after_epochs int The number of epochs to wait for improvement on the validation set before terminating training. 20 max_num_epochs Optional[int] Maximum number of epochs to run. If reached, we stop training even when the validation loss is still decreasing. If None, we train until validation loss increases (see also stop_after_epochs ). None clip_max_norm Optional[float] Value at which to clip the total gradient norm in order to prevent exploding gradients. Use None for no clipping. 5.0 exclude_invalid_x bool Whether to exclude simulation outputs x=NaN or x=\u00b1\u221e during training. Expect errors, silent or explicit, when False . True discard_prior_samples bool Whether to discard samples simulated in round 1, i.e. from the prior. Training may be sped up by ignoring such less targeted samples. False retrain_from_scratch_each_round bool Whether to retrain the conditional density estimator for the posterior from scratch each round. False Returns: Type Description NeuralPosterior Posterior p(\\theta|x) p(\\theta|x) that can be sampled and evaluated. Source code in sbi/inference/snre/snre_a.py 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 def __call__ ( self , num_simulations : int , proposal : Optional [ Any ] = None , training_batch_size : int = 50 , learning_rate : float = 5e-4 , validation_fraction : float = 0.1 , stop_after_epochs : int = 20 , max_num_epochs : Optional [ int ] = None , clip_max_norm : Optional [ float ] = 5.0 , exclude_invalid_x : bool = True , discard_prior_samples : bool = False , retrain_from_scratch_each_round : bool = False , ) -> NeuralPosterior : r \"\"\"Run AALR / SNRE_A. Return posterior $p(\\theta|x)$ after inference. Args: num_simulations: Number of simulator calls. proposal: Distribution that the parameters $\\theta$ are drawn from. `proposal=None` uses the prior. Setting the proposal to a distribution targeted on a specific observation, e.g. a posterior $p(\\theta|x_o)$ obtained previously, can lead to less required simulations. training_batch_size: Training batch size. learning_rate: Learning rate for Adam optimizer. validation_fraction: The fraction of data to use for validation. stop_after_epochs: The number of epochs to wait for improvement on the validation set before terminating training. max_num_epochs: Maximum number of epochs to run. If reached, we stop training even when the validation loss is still decreasing. If None, we train until validation loss increases (see also `stop_after_epochs`). clip_max_norm: Value at which to clip the total gradient norm in order to prevent exploding gradients. Use None for no clipping. exclude_invalid_x: Whether to exclude simulation outputs `x=NaN` or `x=\u00b1\u221e` during training. Expect errors, silent or explicit, when `False`. discard_prior_samples: Whether to discard samples simulated in round 1, i.e. from the prior. Training may be sped up by ignoring such less targeted samples. retrain_from_scratch_each_round: Whether to retrain the conditional density estimator for the posterior from scratch each round. Returns: Posterior $p(\\theta|x)$ that can be sampled and evaluated. \"\"\" # AALR is defined for `num_atoms=2`. # Proxy to `super().__call__` to ensure right parameter. kwargs = del_entries ( locals (), entries = ( \"self\" , \"__class__\" )) return super () . __call__ ( ** kwargs , num_atoms = 2 ) __init__ ( self , simulator , prior , num_workers = 1 , simulation_batch_size = 1 , classifier = 'resnet' , mcmc_method = 'slice_np' , mcmc_parameters = None , device = 'cpu' , logging_level = 'warning' , summary_writer = None , show_progress_bars = True , show_round_summary = False ) special AALR[1], here known as SNRE_A. [1] Likelihood-free MCMC with Amortized Approximate Likelihood Ratios , Hermans et al., ICML 2020, https://arxiv.org/abs/1903.04057 Parameters: Name Type Description Default simulator Callable A function that takes parameters \\theta \\theta and maps them to simulations, or observations, x , \\mathrm{sim}(\\theta)\\to x \\mathrm{sim}(\\theta)\\to x . Any regular Python callable (i.e. function or class with __call__ method) can be used. required prior A probability distribution that expresses prior knowledge about the parameters, e.g. which ranges are meaningful for them. Any object with .log_prob() and .sample() (for example, a PyTorch distribution) can be used. required num_workers int Number of parallel workers to use for simulations. 1 simulation_batch_size int Number of parameter sets that the simulator maps to data x at once. If None, we simulate all parameter sets at the same time. If >= 1, the simulator has to process data of shape (simulation_batch_size, parameter_dimension). 1 classifier Union[str, Callable] Classifier trained to approximate likelihood ratios. If it is a string, use a pre-configured network of the provided type (one of linear, mlp, resnet). Alternatively, a function that builds a custom neural network can be provided. The function will be called with the first batch of simulations (theta, x), which can thus be used for shape inference and potentially for z-scoring. It needs to return a PyTorch nn.Module implementing the classifier. 'resnet' mcmc_method str Method used for MCMC sampling, one of slice_np , slice , hmc , nuts . Currently defaults to slice_np for a custom numpy implementation of slice sampling; select hmc , nuts or slice for Pyro-based sampling. 'slice_np' mcmc_parameters Optional[Dict[str, Any]] Dictionary overriding the default parameters for MCMC. The following parameters are supported: thin to set the thinning factor for the chain, warmup_steps to set the initial number of samples to discard, num_chains for the number of chains, init_strategy for the initialisation strategy for chains; prior will draw init locations from prior, whereas sir will use Sequential-Importance- Resampling using init_strategy_num_candidates to find init locations. None device str torch device on which to compute, e.g. gpu, cpu. 'cpu' logging_level Union[int, str] Minimum severity of messages to log. One of the strings INFO, WARNING, DEBUG, ERROR and CRITICAL. 'warning' summary_writer Optional[Writer] A tensorboard SummaryWriter to control, among others, log file location (default is <current working directory>/logs .) None show_progress_bars bool Whether to show a progressbar during simulation and sampling. True show_round_summary bool Whether to show the validation loss and leakage after each round. False Source code in sbi/inference/snre/snre_a.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 def __init__ ( self , simulator : Callable , prior , num_workers : int = 1 , simulation_batch_size : int = 1 , classifier : Union [ str , Callable ] = \"resnet\" , mcmc_method : str = \"slice_np\" , mcmc_parameters : Optional [ Dict [ str , Any ]] = None , device : str = \"cpu\" , logging_level : Union [ int , str ] = \"warning\" , summary_writer : Optional [ TensorboardSummaryWriter ] = None , show_progress_bars : bool = True , show_round_summary : bool = False , ): r \"\"\"AALR[1], here known as SNRE_A. [1] _Likelihood-free MCMC with Amortized Approximate Likelihood Ratios_, Hermans et al., ICML 2020, https://arxiv.org/abs/1903.04057 Args: simulator: A function that takes parameters $\\theta$ and maps them to simulations, or observations, `x`, $\\mathrm{sim}(\\theta)\\to x$. Any regular Python callable (i.e. function or class with `__call__` method) can be used. prior: A probability distribution that expresses prior knowledge about the parameters, e.g. which ranges are meaningful for them. Any object with `.log_prob()`and `.sample()` (for example, a PyTorch distribution) can be used. num_workers: Number of parallel workers to use for simulations. simulation_batch_size: Number of parameter sets that the simulator maps to data x at once. If None, we simulate all parameter sets at the same time. If >= 1, the simulator has to process data of shape (simulation_batch_size, parameter_dimension). classifier: Classifier trained to approximate likelihood ratios. If it is a string, use a pre-configured network of the provided type (one of linear, mlp, resnet). Alternatively, a function that builds a custom neural network can be provided. The function will be called with the first batch of simulations (theta, x), which can thus be used for shape inference and potentially for z-scoring. It needs to return a PyTorch `nn.Module` implementing the classifier. mcmc_method: Method used for MCMC sampling, one of `slice_np`, `slice`, `hmc`, `nuts`. Currently defaults to `slice_np` for a custom numpy implementation of slice sampling; select `hmc`, `nuts` or `slice` for Pyro-based sampling. mcmc_parameters: Dictionary overriding the default parameters for MCMC. The following parameters are supported: `thin` to set the thinning factor for the chain, `warmup_steps` to set the initial number of samples to discard, `num_chains` for the number of chains, `init_strategy` for the initialisation strategy for chains; `prior` will draw init locations from prior, whereas `sir` will use Sequential-Importance- Resampling using `init_strategy_num_candidates` to find init locations. device: torch device on which to compute, e.g. gpu, cpu. logging_level: Minimum severity of messages to log. One of the strings INFO, WARNING, DEBUG, ERROR and CRITICAL. summary_writer: A tensorboard `SummaryWriter` to control, among others, log file location (default is `<current working directory>/logs`.) show_progress_bars: Whether to show a progressbar during simulation and sampling. show_round_summary: Whether to show the validation loss and leakage after each round. \"\"\" kwargs = del_entries ( locals (), entries = ( \"self\" , \"__class__\" )) super () . __init__ ( ** kwargs ) provide_presimulated ( self , theta , x , from_round = 0 ) inherited Provide external \\theta \\theta and x x to be used for training later on. Parameters: Name Type Description Default theta Tensor Parameter sets used to generate presimulated data. required x Tensor Simulation outputs of presimulated data. required from_round int Which round the data was simulated from. from_round=0 means that the data came from the first round, i.e. the prior. 0 Source code in sbi/inference/snre/snre_a.py 164 165 166 167 168 169 170 171 172 173 174 175 176 def provide_presimulated ( self , theta : Tensor , x : Tensor , from_round : int = 0 ) -> None : r \"\"\" Provide external $\\theta$ and $x$ to be used for training later on. Args: theta: Parameter sets used to generate presimulated data. x: Simulation outputs of presimulated data. from_round: Which round the data was simulated from. `from_round=0` means that the data came from the first round, i.e. the prior. \"\"\" self . _append_to_data_bank ( theta , x , from_round ) sbi.inference.snre.snre_b.SNRE_B \u00b6 __call__ ( self , num_simulations , proposal = None , num_atoms = 10 , training_batch_size = 50 , learning_rate = 0.0005 , validation_fraction = 0.1 , stop_after_epochs = 20 , max_num_epochs = None , clip_max_norm = 5.0 , exclude_invalid_x = True , discard_prior_samples = False , retrain_from_scratch_each_round = False ) special Run SRE / SNRE_B. Return posterior p(\\theta|x) p(\\theta|x) after inference (possibly over several rounds). Parameters: Name Type Description Default num_simulations int Number of simulator calls. required proposal Optional[Any] Distribution that the parameters \\theta \\theta are drawn from. proposal=None uses the prior. Setting the proposal to a distribution targeted on a specific observation, e.g. a posterior p(\\theta|x_o) p(\\theta|x_o) obtained previously, can lead to less required simulations. None training_batch_size int Training batch size. 50 learning_rate float Learning rate for Adam optimizer. 0.0005 validation_fraction float The fraction of data to use for validation. 0.1 stop_after_epochs int The number of epochs to wait for improvement on the validation set before terminating training. 20 max_num_epochs Optional[int] Maximum number of epochs to run. If reached, we stop training even when the validation loss is still decreasing. If None, we train until validation loss increases (see also stop_after_epochs ). None clip_max_norm Optional[float] Value at which to clip the total gradient norm in order to prevent exploding gradients. Use None for no clipping. 5.0 exclude_invalid_x bool Whether to exclude simulation outputs x=NaN or x=\u00b1\u221e during training. Expect errors, silent or explicit, when False . True discard_prior_samples bool Whether to discard samples simulated in round 1, i.e. from the prior. Training may be sped up by ignoring such less targeted samples. False retrain_from_scratch_each_round bool Whether to retrain the conditional density estimator for the posterior from scratch each round. False Returns: Type Description NeuralPosterior Posterior p(\\theta|x) p(\\theta|x) that can be sampled and evaluated. Source code in sbi/inference/snre/snre_b.py 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 def __call__ ( self , num_simulations : int , proposal : Optional [ Any ] = None , num_atoms : int = 10 , training_batch_size : int = 50 , learning_rate : float = 5e-4 , validation_fraction : float = 0.1 , stop_after_epochs : int = 20 , max_num_epochs : Optional [ int ] = None , clip_max_norm : Optional [ float ] = 5.0 , exclude_invalid_x : bool = True , discard_prior_samples : bool = False , retrain_from_scratch_each_round : bool = False , ) -> NeuralPosterior : r \"\"\"Run SRE / SNRE_B. Return posterior $p(\\theta|x)$ after inference (possibly over several rounds). Args: num_simulations: Number of simulator calls. proposal: Distribution that the parameters $\\theta$ are drawn from. `proposal=None` uses the prior. Setting the proposal to a distribution targeted on a specific observation, e.g. a posterior $p(\\theta|x_o)$ obtained previously, can lead to less required simulations. training_batch_size: Training batch size. learning_rate: Learning rate for Adam optimizer. validation_fraction: The fraction of data to use for validation. stop_after_epochs: The number of epochs to wait for improvement on the validation set before terminating training. max_num_epochs: Maximum number of epochs to run. If reached, we stop training even when the validation loss is still decreasing. If None, we train until validation loss increases (see also `stop_after_epochs`). clip_max_norm: Value at which to clip the total gradient norm in order to prevent exploding gradients. Use None for no clipping. exclude_invalid_x: Whether to exclude simulation outputs `x=NaN` or `x=\u00b1\u221e` during training. Expect errors, silent or explicit, when `False`. discard_prior_samples: Whether to discard samples simulated in round 1, i.e. from the prior. Training may be sped up by ignoring such less targeted samples. retrain_from_scratch_each_round: Whether to retrain the conditional density estimator for the posterior from scratch each round. Returns: Posterior $p(\\theta|x)$ that can be sampled and evaluated. \"\"\" kwargs = del_entries ( locals (), entries = ( \"self\" , \"__class__\" )) return super () . __call__ ( ** kwargs ) __init__ ( self , simulator , prior , num_workers = 1 , simulation_batch_size = 1 , classifier = 'resnet' , mcmc_method = 'slice_np' , mcmc_parameters = None , device = 'cpu' , logging_level = 'warning' , summary_writer = None , show_progress_bars = True , show_round_summary = False ) special SRE[1], here known as SNRE_B. [1] On Contrastive Learning for Likelihood-free Inference , Durkan et al., ICML 2020, https://arxiv.org/pdf/2002.03712 Parameters: Name Type Description Default simulator Callable A function that takes parameters \\theta \\theta and maps them to simulations, or observations, x , \\mathrm{sim}(\\theta)\\to x \\mathrm{sim}(\\theta)\\to x . Any regular Python callable (i.e. function or class with __call__ method) can be used. required prior A probability distribution that expresses prior knowledge about the parameters, e.g. which ranges are meaningful for them. Any object with .log_prob() and .sample() (for example, a PyTorch distribution) can be used. required num_workers int Number of parallel workers to use for simulations. 1 simulation_batch_size int Number of parameter sets that the simulator maps to data x at once. If None, we simulate all parameter sets at the same time. If >= 1, the simulator has to process data of shape (simulation_batch_size, parameter_dimension). 1 classifier Union[str, Callable] Classifier trained to approximate likelihood ratios. If it is a string, use a pre-configured network of the provided type (one of linear, mlp, resnet). Alternatively, a function that builds a custom neural network can be provided. The function will be called with the first batch of simulations (theta, x), which can thus be used for shape inference and potentially for z-scoring. It needs to return a PyTorch nn.Module implementing the classifier. 'resnet' mcmc_method str Method used for MCMC sampling, one of slice_np , slice , hmc , nuts . Currently defaults to slice_np for a custom numpy implementation of slice sampling; select hmc , nuts or slice for Pyro-based sampling. 'slice_np' mcmc_parameters Optional[Dict[str, Any]] Dictionary overriding the default parameters for MCMC. The following parameters are supported: thin to set the thinning factor for the chain, warmup_steps to set the initial number of samples to discard, num_chains for the number of chains, init_strategy for the initialisation strategy for chains; prior will draw init locations from prior, whereas sir will use Sequential-Importance- Resampling using init_strategy_num_candidates to find init locations. None device str torch device on which to compute, e.g. gpu, cpu. 'cpu' logging_level Union[int, str] Minimum severity of messages to log. One of the strings INFO, WARNING, DEBUG, ERROR and CRITICAL. 'warning' summary_writer Optional[Writer] A tensorboard SummaryWriter to control, among others, log file location (default is <current working directory>/logs .) None show_progress_bars bool Whether to show a progressbar during simulation and sampling. True show_round_summary bool Whether to show the validation loss and leakage after each round. False Source code in sbi/inference/snre/snre_b.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 def __init__ ( self , simulator : Callable , prior , num_workers : int = 1 , simulation_batch_size : int = 1 , classifier : Union [ str , Callable ] = \"resnet\" , mcmc_method : str = \"slice_np\" , mcmc_parameters : Optional [ Dict [ str , Any ]] = None , device : str = \"cpu\" , logging_level : Union [ int , str ] = \"warning\" , summary_writer : Optional [ TensorboardSummaryWriter ] = None , show_progress_bars : bool = True , show_round_summary : bool = False , ): r \"\"\"SRE[1], here known as SNRE_B. [1] _On Contrastive Learning for Likelihood-free Inference_, Durkan et al., ICML 2020, https://arxiv.org/pdf/2002.03712 Args: simulator: A function that takes parameters $\\theta$ and maps them to simulations, or observations, `x`, $\\mathrm{sim}(\\theta)\\to x$. Any regular Python callable (i.e. function or class with `__call__` method) can be used. prior: A probability distribution that expresses prior knowledge about the parameters, e.g. which ranges are meaningful for them. Any object with `.log_prob()`and `.sample()` (for example, a PyTorch distribution) can be used. num_workers: Number of parallel workers to use for simulations. simulation_batch_size: Number of parameter sets that the simulator maps to data x at once. If None, we simulate all parameter sets at the same time. If >= 1, the simulator has to process data of shape (simulation_batch_size, parameter_dimension). classifier: Classifier trained to approximate likelihood ratios. If it is a string, use a pre-configured network of the provided type (one of linear, mlp, resnet). Alternatively, a function that builds a custom neural network can be provided. The function will be called with the first batch of simulations (theta, x), which can thus be used for shape inference and potentially for z-scoring. It needs to return a PyTorch `nn.Module` implementing the classifier. mcmc_method: Method used for MCMC sampling, one of `slice_np`, `slice`, `hmc`, `nuts`. Currently defaults to `slice_np` for a custom numpy implementation of slice sampling; select `hmc`, `nuts` or `slice` for Pyro-based sampling. mcmc_parameters: Dictionary overriding the default parameters for MCMC. The following parameters are supported: `thin` to set the thinning factor for the chain, `warmup_steps` to set the initial number of samples to discard, `num_chains` for the number of chains, `init_strategy` for the initialisation strategy for chains; `prior` will draw init locations from prior, whereas `sir` will use Sequential-Importance- Resampling using `init_strategy_num_candidates` to find init locations. device: torch device on which to compute, e.g. gpu, cpu. logging_level: Minimum severity of messages to log. One of the strings INFO, WARNING, DEBUG, ERROR and CRITICAL. summary_writer: A tensorboard `SummaryWriter` to control, among others, log file location (default is `<current working directory>/logs`.) show_progress_bars: Whether to show a progressbar during simulation and sampling. show_round_summary: Whether to show the validation loss and leakage after each round. \"\"\" kwargs = del_entries ( locals (), entries = ( \"self\" , \"__class__\" )) super () . __init__ ( ** kwargs ) provide_presimulated ( self , theta , x , from_round = 0 ) inherited Provide external \\theta \\theta and x x to be used for training later on. Parameters: Name Type Description Default theta Tensor Parameter sets used to generate presimulated data. required x Tensor Simulation outputs of presimulated data. required from_round int Which round the data was simulated from. from_round=0 means that the data came from the first round, i.e. the prior. 0 Source code in sbi/inference/snre/snre_b.py 164 165 166 167 168 169 170 171 172 173 174 175 176 def provide_presimulated ( self , theta : Tensor , x : Tensor , from_round : int = 0 ) -> None : r \"\"\" Provide external $\\theta$ and $x$ to be used for training later on. Args: theta: Parameter sets used to generate presimulated data. x: Simulation outputs of presimulated data. from_round: Which round the data was simulated from. `from_round=0` means that the data came from the first round, i.e. the prior. \"\"\" self . _append_to_data_bank ( theta , x , from_round ) sbi.inference.abc.mcabc.MCABC \u00b6 __call__ ( self , x_o , num_simulations , eps = None , quantile = None , return_distances = False ) special Run MCABC. Parameters: Name Type Description Default x_o Union[torch.Tensor, numpy.ndarray] Observed data. required num_simulations int Number of simulations to run. required eps Optional[float] Acceptance threshold \\epsilon \\epsilon for distance between observed and simulated data. None quantile Optional[float] Upper quantile of smallest distances for which the corresponding parameters are returned, e.g, q=0.01 will return the top 1%. Exactly one of quantile or eps have to be passed. None return_distances bool Whether to return the distances corresponding to the selected parameters. False Returns: Type Description Union[torch.distributions.distribution.Distribution, Tuple[torch.distributions.distribution.Distribution, torch.Tensor]] posterior: Empirical distribution based on selected parameters. distances: Tensor of distances of the selected parameters. Source code in sbi/inference/abc/mcabc.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 def __call__ ( self , x_o : Union [ Tensor , ndarray ], num_simulations : int , eps : Optional [ float ] = None , quantile : Optional [ float ] = None , return_distances : bool = False , ) -> Union [ Distribution , Tuple [ Distribution , Tensor ]]: r \"\"\"Run MCABC. Args: x_o: Observed data. num_simulations: Number of simulations to run. eps: Acceptance threshold $\\epsilon$ for distance between observed and simulated data. quantile: Upper quantile of smallest distances for which the corresponding parameters are returned, e.g, q=0.01 will return the top 1%. Exactly one of quantile or `eps` have to be passed. return_distances: Whether to return the distances corresponding to the selected parameters. Returns: posterior: Empirical distribution based on selected parameters. distances: Tensor of distances of the selected parameters. \"\"\" # Exactly one of eps or quantile need to be passed. assert ( eps is not None ) ^ ( quantile is not None ), \"Eps or quantile must be passed, but not both.\" # Simulate and calculate distances. theta = self . prior . sample (( num_simulations ,)) x = self . _batched_simulator ( theta ) # Infer shape of x to test and set x_o. self . x_shape = x [ 0 ] . unsqueeze ( 0 ) . shape self . x_o = process_x ( x_o , self . x_shape ) distances = self . distance ( self . x_o , x ) # Select based on acceptance threshold epsilon. if eps is not None : is_accepted = distances < eps num_accepted = is_accepted . sum () . item () assert num_accepted > 0 , f \"No parameters accepted, eps= {eps} too small\" theta_accepted = theta [ is_accepted ] distances_accepted = distances [ is_accepted ] # Select based on quantile on sorted distances. elif quantile is not None : num_top_samples = int ( num_simulations * quantile ) sort_idx = torch . argsort ( distances ) theta_accepted = theta [ sort_idx ][: num_top_samples ] distances_accepted = distances [ sort_idx ][: num_top_samples ] else : raise ValueError ( \"One of epsilon or quantile has to be passed.\" ) posterior = Empirical ( theta_accepted , log_weights = ones ( theta_accepted . shape [ 0 ])) if return_distances : return posterior , distances_accepted else : return posterior __init__ ( self , simulator , prior , distance = 'l2' , num_workers = 1 , simulation_batch_size = 1 , show_progress_bars = True ) special Monte-Carlo Approximate Bayesian Computation (Rejection ABC) [1]. [1] Pritchard, J. K., Seielstad, M. T., Perez-Lezaun, A., & Feldman, M. W. (1999). Population growth of human Y chromosomes: a study of Y chromosome microsatellites. Molecular biology and evolution, 16(12), 1791-1798. Parameters: Name Type Description Default simulator Callable A function that takes parameters $ heta$ and maps them to simulations, or observations, x , \\mathrm{sim}( heta) o x \\mathrm{sim}( heta) o x . Any regular Python callable (i.e. function or class with __call__ method) can be used. required prior A probability distribution that expresses prior knowledge about the parameters, e.g. which ranges are meaningful for them. Any object with .log_prob() and .sample() (for example, a PyTorch distribution) can be used. required distance Union[str, Callable] Distance function to compare observed and simulated data. Can be a custom function or one of l1 , l2 , mse . 'l2' num_workers int Number of parallel workers to use for simulations. 1 simulation_batch_size int Number of parameter sets that the simulator maps to data x at once. If None, we simulate all parameter sets at the same time. If >= 1, the simulator has to process data of shape (simulation_batch_size, parameter_dimension). 1 show_progress_bars bool Whether to show a progressbar during simulation and sampling. True Source code in sbi/inference/abc/mcabc.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 def __init__ ( self , simulator : Callable , prior , distance : Union [ str , Callable ] = \"l2\" , num_workers : int = 1 , simulation_batch_size : int = 1 , show_progress_bars : bool = True , ): \"\"\"Monte-Carlo Approximate Bayesian Computation (Rejection ABC) [1]. [1] Pritchard, J. K., Seielstad, M. T., Perez-Lezaun, A., & Feldman, M. W. (1999). Population growth of human Y chromosomes: a study of Y chromosome microsatellites. Molecular biology and evolution, 16(12), 1791-1798. Args: simulator: A function that takes parameters $\\theta$ and maps them to simulations, or observations, `x`, $\\mathrm{sim}(\\theta)\\to x$. Any regular Python callable (i.e. function or class with `__call__` method) can be used. prior: A probability distribution that expresses prior knowledge about the parameters, e.g. which ranges are meaningful for them. Any object with `.log_prob()`and `.sample()` (for example, a PyTorch distribution) can be used. distance: Distance function to compare observed and simulated data. Can be a custom function or one of `l1`, `l2`, `mse`. num_workers: Number of parallel workers to use for simulations. simulation_batch_size: Number of parameter sets that the simulator maps to data x at once. If None, we simulate all parameter sets at the same time. If >= 1, the simulator has to process data of shape (simulation_batch_size, parameter_dimension). show_progress_bars: Whether to show a progressbar during simulation and sampling. \"\"\" super () . __init__ ( simulator = simulator , prior = prior , distance = distance , num_workers = num_workers , simulation_batch_size = simulation_batch_size , show_progress_bars = show_progress_bars , ) choose_distance_function ( distance_type = 'l2' ) inherited Return distance function for given distance type. Source code in sbi/inference/abc/mcabc.py 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 @staticmethod def choose_distance_function ( distance_type : str = \"l2\" ) -> Callable : \"\"\"Return distance function for given distance type.\"\"\" if distance_type == \"mse\" : distance = lambda xo , x : torch . mean (( xo - x ) ** 2 , dim =- 1 ) elif distance_type == \"l2\" : distance = lambda xo , x : torch . norm (( xo - x ), dim =- 1 ) elif distance_type == \"l1\" : distance = lambda xo , x : torch . mean ( abs ( xo - x ), dim =- 1 ) else : raise ValueError ( r \"Distance {distance_type} not supported.\" ) def distance_fun ( observed_data : Tensor , simulated_data : Tensor ) -> Tensor : \"\"\"Return distance over batch dimension. Args: observed_data: Observed data, could be 1D. simulated_data: Batch of simulated data, has batch dimension. Returns: Torch tensor with batch of distances. \"\"\" assert simulated_data . ndim == 2 , \"simulated data needs batch dimension\" return distance ( observed_data , simulated_data ) return distance_fun sbi.inference.abc.smcabc.SMCABC \u00b6 __call__ ( self , x_o , num_particles , num_initial_pop , num_simulations , epsilon_decay , distance_based_decay = False , ess_min = 0.5 , kernel_variance_scale = 1.0 , use_last_pop_samples = True , return_summary = False ) special Run SMCABC. Parameters: Name Type Description Default x_o Union[torch.Tensor, numpy.ndarray] Observed data. required num_particles int Number of particles in each population. required num_initial_pop int Number of simulations used for initial population. required num_simulations int Total number of possible simulations. required epsilon_decay float Factor with which the acceptance threshold \\epsilon \\epsilon decays. required distance_based_decay bool Whether the \\epsilon \\epsilon decay is constant over populations or calculated from the previous populations distribution of distances. False ess_min float Threshold of effective sampling size for resampling weights. 0.5 kernel_variance_scale float Factor for scaling the perturbation kernel variance. 1.0 use_last_pop_samples bool Whether to fill up the current population with samples from the previous population when the budget is used up. If False, the current population is discarded and the previous population is returned. True return_summary bool Whether to return a dictionary with all accepted particles, weights, etc. at the end. False Returns: Type Description Union[torch.distributions.distribution.Distribution, Tuple[torch.distributions.distribution.Distribution, dict]] posterior: Empirical posterior distribution defined by the accepted particles and their weights. summary (optional): A dictionary containing particles, weights, epsilons and distances of each population. Source code in sbi/inference/abc/smcabc.py 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 def __call__ ( self , x_o : Union [ Tensor , ndarray ], num_particles : int , num_initial_pop : int , num_simulations : int , epsilon_decay : float , distance_based_decay : bool = False , ess_min : float = 0.5 , kernel_variance_scale : float = 1.0 , use_last_pop_samples : bool = True , return_summary : bool = False , ) -> Union [ Distribution , Tuple [ Distribution , dict ]]: r \"\"\"Run SMCABC. Args: x_o: Observed data. num_particles: Number of particles in each population. num_initial_pop: Number of simulations used for initial population. num_simulations: Total number of possible simulations. epsilon_decay: Factor with which the acceptance threshold $\\epsilon$ decays. distance_based_decay: Whether the $\\epsilon$ decay is constant over populations or calculated from the previous populations distribution of distances. ess_min: Threshold of effective sampling size for resampling weights. kernel_variance_scale: Factor for scaling the perturbation kernel variance. use_last_pop_samples: Whether to fill up the current population with samples from the previous population when the budget is used up. If False, the current population is discarded and the previous population is returned. return_summary: Whether to return a dictionary with all accepted particles, weights, etc. at the end. Returns: posterior: Empirical posterior distribution defined by the accepted particles and their weights. summary (optional): A dictionary containing particles, weights, epsilons and distances of each population. \"\"\" pop_idx = 0 self . num_simulations = num_simulations # run initial population particles , epsilon , distances = self . _set_xo_and_sample_initial_population ( x_o , num_particles , num_initial_pop ) log_weights = torch . log ( 1 / num_particles * ones ( num_particles )) self . logger . info ( ( f \"population= {pop_idx} , eps= {epsilon} , ess= {1.0} , \" f \"num_sims= {num_initial_pop} \" ) ) all_particles = [ particles ] all_log_weights = [ log_weights ] all_distances = [ distances ] all_epsilons = [ epsilon ] while self . simulation_counter < num_simulations : pop_idx += 1 # Decay based on quantile of distances from previous pop. if distance_based_decay : epsilon = self . _get_next_epsilon ( all_distances [ pop_idx - 1 ], epsilon_decay ) # Constant decay. else : epsilon *= epsilon_decay # Get kernel variance from previous pop. self . kernel_variance = self . get_kernel_variance ( all_particles [ pop_idx - 1 ], torch . exp ( all_log_weights [ pop_idx - 1 ]), num_samples = 1000 , kernel_variance_scale = kernel_variance_scale , ) particles , log_weights , distances = self . _sample_next_population ( particles = all_particles [ pop_idx - 1 ], log_weights = all_log_weights [ pop_idx - 1 ], distances = all_distances [ pop_idx - 1 ], epsilon = epsilon , use_last_pop_samples = use_last_pop_samples , ) # Resample population if effective sampling size is too small. if self . algorithm_variant == \"B\" : particles , log_weights = self . resample_if_ess_too_small ( particles , log_weights , num_particles , ess_min , pop_idx ) self . logger . info ( ( f \"population= {pop_idx} done: eps= {epsilon:.6f} ,\" f \" num_sims= {self.simulation_counter} .\" ) ) # collect results all_particles . append ( particles ) all_log_weights . append ( log_weights ) all_distances . append ( distances ) all_epsilons . append ( epsilon ) posterior = Empirical ( all_particles [ - 1 ], log_weights = all_log_weights [ - 1 ]) if return_summary : return ( posterior , dict ( particles = all_particles , weights = all_log_weights , epsilons = all_epsilons , distances = all_distances , ), ) else : return posterior __init__ ( self , simulator , prior , distance = 'l2' , num_workers = 1 , simulation_batch_size = 1 , show_progress_bars = True , kernel = 'gaussian' , algorithm_variant = 'C' ) special Sequential Monte Carlo Approximate Bayesian Computation. We distinguish between three different SMC methods here: - A: Toni et al. 2010 (Phd Thesis) - B: Sisson et al. 2007 (with correction from 2009) - C: Beaumont et al. 2009 In Toni et al. 2010 we find an overview of the differences on page 34: - B: same as A except for resampling of weights if the effective sampling size is too small. - C: same as A except for calculation of the covariance of the perturbation kernel: the kernel covariance is a scaled version of the covariance of the previous population. Parameters: Name Type Description Default simulator Callable A function that takes parameters \\theta \\theta and maps them to simulations, or observations, x , \\mathrm{sim}(\\theta)\\to x \\mathrm{sim}(\\theta)\\to x . Any regular Python callable (i.e. function or class with __call__ method) can be used. required prior Distribution A probability distribution that expresses prior knowledge about the parameters, e.g. which ranges are meaningful for them. Any object with .log_prob() and .sample() (for example, a PyTorch distribution) can be used. required distance Union[str, Callable] Distance function to compare observed and simulated data. Can be a custom function or one of l1 , l2 , mse . 'l2' num_workers int Number of parallel workers to use for simulations. 1 simulation_batch_size int Number of parameter sets that the simulator maps to data x at once. If None, we simulate all parameter sets at the same time. If >= 1, the simulator has to process data of shape (simulation_batch_size, parameter_dimension). 1 show_progress_bars bool Whether to show a progressbar during simulation and sampling. True kernel Optional[str] Perturbation kernel. 'gaussian' algorithm_variant str Indicating the choice of algorithm variant, A, B, or C. 'C' Source code in sbi/inference/abc/smcabc.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 def __init__ ( self , simulator : Callable , prior : Distribution , distance : Union [ str , Callable ] = \"l2\" , num_workers : int = 1 , simulation_batch_size : int = 1 , show_progress_bars : bool = True , kernel : Optional [ str ] = \"gaussian\" , algorithm_variant : str = \"C\" , ): r \"\"\"Sequential Monte Carlo Approximate Bayesian Computation. We distinguish between three different SMC methods here: - A: Toni et al. 2010 (Phd Thesis) - B: Sisson et al. 2007 (with correction from 2009) - C: Beaumont et al. 2009 In Toni et al. 2010 we find an overview of the differences on page 34: - B: same as A except for resampling of weights if the effective sampling size is too small. - C: same as A except for calculation of the covariance of the perturbation kernel: the kernel covariance is a scaled version of the covariance of the previous population. Args: simulator: A function that takes parameters $\\theta$ and maps them to simulations, or observations, `x`, $\\mathrm{sim}(\\theta)\\to x$. Any regular Python callable (i.e. function or class with `__call__` method) can be used. prior: A probability distribution that expresses prior knowledge about the parameters, e.g. which ranges are meaningful for them. Any object with `.log_prob()`and `.sample()` (for example, a PyTorch distribution) can be used. distance: Distance function to compare observed and simulated data. Can be a custom function or one of `l1`, `l2`, `mse`. num_workers: Number of parallel workers to use for simulations. simulation_batch_size: Number of parameter sets that the simulator maps to data x at once. If None, we simulate all parameter sets at the same time. If >= 1, the simulator has to process data of shape (simulation_batch_size, parameter_dimension). show_progress_bars: Whether to show a progressbar during simulation and sampling. kernel: Perturbation kernel. algorithm_variant: Indicating the choice of algorithm variant, A, B, or C. \"\"\" super () . __init__ ( simulator = simulator , prior = prior , distance = distance , num_workers = num_workers , simulation_batch_size = simulation_batch_size , show_progress_bars = show_progress_bars , ) kernels = ( \"gaussian\" , \"uniform\" ) assert ( kernel in kernels ), f \"Kernel ' {kernel} ' not supported. Choose one from {kernels} .\" self . kernel = kernel algorithm_variants = ( \"A\" , \"B\" , \"C\" ) assert algorithm_variant in algorithm_variants , ( f \"SMCABC variant ' {algorithm_variant} ' not supported, choose one from\" \" {algorithm_variants} .\" ) self . algorithm_variant = algorithm_variant self . distance_to_x0 = None self . simulation_counter = 0 self . num_simulations = 0 self . logger = logging . getLogger ( __name__ ) # Define simulator that keeps track of budget. def simulate_with_budget ( theta ): self . simulation_counter += theta . shape [ 0 ] return self . _batched_simulator ( theta ) self . _simulate_with_budget = simulate_with_budget choose_distance_function ( distance_type = 'l2' ) inherited Return distance function for given distance type. Source code in sbi/inference/abc/smcabc.py 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 @staticmethod def choose_distance_function ( distance_type : str = \"l2\" ) -> Callable : \"\"\"Return distance function for given distance type.\"\"\" if distance_type == \"mse\" : distance = lambda xo , x : torch . mean (( xo - x ) ** 2 , dim =- 1 ) elif distance_type == \"l2\" : distance = lambda xo , x : torch . norm (( xo - x ), dim =- 1 ) elif distance_type == \"l1\" : distance = lambda xo , x : torch . mean ( abs ( xo - x ), dim =- 1 ) else : raise ValueError ( r \"Distance {distance_type} not supported.\" ) def distance_fun ( observed_data : Tensor , simulated_data : Tensor ) -> Tensor : \"\"\"Return distance over batch dimension. Args: observed_data: Observed data, could be 1D. simulated_data: Batch of simulated data, has batch dimension. Returns: Torch tensor with batch of distances. \"\"\" assert simulated_data . ndim == 2 , \"simulated data needs batch dimension\" return distance ( observed_data , simulated_data ) return distance_fun get_new_kernel ( self , thetas ) Return new kernel distribution for a given set of paramters. Source code in sbi/inference/abc/smcabc.py 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 def get_new_kernel ( self , thetas : Tensor ) -> Distribution : \"\"\"Return new kernel distribution for a given set of paramters.\"\"\" if self . kernel == \"gaussian\" : assert self . kernel_variance . ndim == 2 return MultivariateNormal ( loc = thetas , covariance_matrix = self . kernel_variance ) elif self . kernel == \"uniform\" : low = thetas - self . kernel_variance high = thetas + self . kernel_variance # Move batch shape to event shape to get Uniform that is multivariate in # parameter dimension. return Uniform ( low = low , high = high ) . to_event ( 1 ) else : raise ValueError ( f \"Kernel, ' {self.kernel} ' not supported.\" ) resample_if_ess_too_small ( self , particles , log_weights , num_particles , ess_min , pop_idx ) Return resampled particles and uniform weights if effectice sampling size is too small. Source code in sbi/inference/abc/smcabc.py 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 def resample_if_ess_too_small ( self , particles : Tensor , log_weights : Tensor , num_particles : int , ess_min : float , pop_idx : int , ) -> Tuple [ Tensor , Tensor ]: \"\"\"Return resampled particles and uniform weights if effectice sampling size is too small. \"\"\" ess = ( 1 / torch . sum ( torch . exp ( 2.0 * log_weights ), dim = 0 )) / num_particles # Resampling of weights for low ESS only for Sisson et al. 2007. if ess < ess_min : self . logger . info ( f \"ESS= {ess:.2f} too low, resampling pop {pop_idx} ...\" ) # First resample, then set to uniform weights in in Sisson et al. 2007. particles = self . sample_from_population_with_weights ( particles , torch . exp ( log_weights ), num_samples = num_particles ) log_weights = torch . log ( 1 / num_particles * ones ( num_particles )) return particles , log_weights sample_from_population_with_weights ( particles , weights , num_samples = 1 ) staticmethod Return samples from particles sampled with weights. Source code in sbi/inference/abc/smcabc.py 392 393 394 395 396 397 398 399 400 401 402 403 404 405 @staticmethod def sample_from_population_with_weights ( particles : Tensor , weights : Tensor , num_samples : int = 1 ) -> Tensor : \"\"\"Return samples from particles sampled with weights.\"\"\" # define multinomial with weights as probs multi = Multinomial ( probs = weights ) # sample num samples, with replacement samples = multi . sample ( sample_shape = ( num_samples ,)) # get indices of success trials indices = torch . where ( samples )[ 1 ] # return those indices from trace return particles [ indices ] Posteriors \u00b6 sbi.inference.posteriors.direct_posterior.DirectPosterior \u00b6 Posterior p(\\theta|x) p(\\theta|x) with log_prob() and sample() methods, obtained with SNPE. SNPE trains a neural network to directly approximate the posterior distribution. However, for bounded priors, the neural network can have leakage: it puts non-zero mass in regions where the prior is zero. The SnpePosterior class wraps the trained network to deal with these cases. Specifically, this class offers the following functionality: - correct the calculation of the log probability such that it compensates for the leakage. - reject samples that lie outside of the prior bounds. - alternatively, if leakage is very high (which can happen for multi-round SNPE), sample from the posterior with MCMC. The neural network itself can be accessed via the .net attribute. default_x: Optional [ torch . Tensor ] inherited property writable Return default x used by .sample(), .log_prob as conditioning context. mcmc_method: str inherited property writable Returns MCMC method. mcmc_parameters: dict inherited property writable Returns MCMC parameters. sample_with_mcmc: bool property writable Return True if NeuralPosterior instance should use MCMC in .sample() . __init__ ( self , method_family , neural_net , prior , x_shape , sample_with_mcmc = True , mcmc_method = 'slice_np' , mcmc_parameters = None , get_potential_function = None ) special Parameters: Name Type Description Default method_family str One of snpe, snl, snre_a or snre_b. required neural_net Module A classifier for SNRE, a density estimator for SNPE and SNL. required prior Prior distribution with .log_prob() and .sample() . required x_shape Size Shape of a single simulator output. required sample_with_mcmc bool Whether to sample with MCMC. Will always be True for SRE and SNL, but can also be set to True for SNPE if MCMC is preferred to deal with leakage over rejection sampling. True mcmc_method str Method used for MCMC sampling, one of slice_np , slice , hmc , nuts . Currently defaults to slice_np for a custom numpy implementation of slice sampling; select hmc , nuts or slice for Pyro-based sampling. 'slice_np' mcmc_parameters Optional[Dict[str, Any]] Dictionary overriding the default parameters for MCMC. The following parameters are supported: thin to set the thinning factor for the chain, warmup_steps to set the initial number of samples to discard, num_chains for the number of chains, init_strategy for the initialisation strategy for chains; prior will draw init locations from prior, whereas sir will use Sequential- Importance-Resampling using init_strategy_num_candidates to find init locations. None get_potential_function Optional[Callable] Callable that returns the potential function used for MCMC sampling. None Source code in sbi/inference/posteriors/direct_posterior.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 def __init__ ( self , method_family : str , neural_net : nn . Module , prior , x_shape : torch . Size , sample_with_mcmc : bool = True , mcmc_method : str = \"slice_np\" , mcmc_parameters : Optional [ Dict [ str , Any ]] = None , get_potential_function : Optional [ Callable ] = None , ): \"\"\" Args: method_family: One of snpe, snl, snre_a or snre_b. neural_net: A classifier for SNRE, a density estimator for SNPE and SNL. prior: Prior distribution with `.log_prob()` and `.sample()`. x_shape: Shape of a single simulator output. sample_with_mcmc: Whether to sample with MCMC. Will always be `True` for SRE and SNL, but can also be set to `True` for SNPE if MCMC is preferred to deal with leakage over rejection sampling. mcmc_method: Method used for MCMC sampling, one of `slice_np`, `slice`, `hmc`, `nuts`. Currently defaults to `slice_np` for a custom numpy implementation of slice sampling; select `hmc`, `nuts` or `slice` for Pyro-based sampling. mcmc_parameters: Dictionary overriding the default parameters for MCMC. The following parameters are supported: `thin` to set the thinning factor for the chain, `warmup_steps` to set the initial number of samples to discard, `num_chains` for the number of chains, `init_strategy` for the initialisation strategy for chains; `prior` will draw init locations from prior, whereas `sir` will use Sequential- Importance-Resampling using `init_strategy_num_candidates` to find init locations. get_potential_function: Callable that returns the potential function used for MCMC sampling. \"\"\" kwargs = del_entries ( locals (), entries = ( \"self\" , \"__class__\" , \"sample_with_mcmc\" ) ) super () . __init__ ( ** kwargs ) self . set_sample_with_mcmc ( sample_with_mcmc ) self . _purpose = ( \"It allows to .sample() and .log_prob() the posterior and wraps the \" \"output of the .net to avoid leakage into regions with 0 prior probability.\" ) leakage_correction ( self , x , num_rejection_samples = 10000 , force_update = False , show_progress_bars = False ) Return leakage correction factor for a leaky posterior density estimate. The factor is estimated from the acceptance probability during rejection sampling from the posterior. This is to avoid re-estimating the acceptance probability from scratch whenever log_prob is called and norm_posterior=True . Here, it is estimated only once for self.default_x and saved for later. We re-evaluate only whenever a new x is passed. Parameters: Name Type Description Default x Tensor Conditioning context for posterior p(\\theta|x) p(\\theta|x) . required num_rejection_samples int Number of samples used to estimate correction factor. 10000 force_update bool Whether to force a reevaluation of the leakage correction even if the context x is the same as self.default_x . This is useful to enforce a new leakage estimate for rounds after the first (2, 3,..). False show_progress_bars bool Whether to show a progress bar during sampling. False Returns: Type Description Tensor Saved or newly-estimated correction factor (as a scalar Tensor ). Source code in sbi/inference/posteriors/direct_posterior.py 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 @torch . no_grad () def leakage_correction ( self , x : Tensor , num_rejection_samples : int = 10_000 , force_update : bool = False , show_progress_bars : bool = False , ) -> Tensor : r \"\"\"Return leakage correction factor for a leaky posterior density estimate. The factor is estimated from the acceptance probability during rejection sampling from the posterior. This is to avoid re-estimating the acceptance probability from scratch whenever `log_prob` is called and `norm_posterior=True`. Here, it is estimated only once for `self.default_x` and saved for later. We re-evaluate only whenever a new `x` is passed. Arguments: x: Conditioning context for posterior $p(\\theta|x)$. num_rejection_samples: Number of samples used to estimate correction factor. force_update: Whether to force a reevaluation of the leakage correction even if the context `x` is the same as `self.default_x`. This is useful to enforce a new leakage estimate for rounds after the first (2, 3,..). show_progress_bars: Whether to show a progress bar during sampling. Returns: Saved or newly-estimated correction factor (as a scalar `Tensor`). \"\"\" def acceptance_at ( x : Tensor ) -> Tensor : return utils . sample_posterior_within_prior ( self . net , self . _prior , x , num_rejection_samples , show_progress_bars )[ 1 ] # Check if the provided x matches the default x (short-circuit on identity). is_new_x = self . default_x is None or ( x is not self . default_x and ( x != self . default_x ) . any () ) not_saved_at_default_x = self . _leakage_density_correction_factor is None if is_new_x : # Calculate at x; don't save. return acceptance_at ( x ) elif not_saved_at_default_x or force_update : # Calculate at default_x; save. self . _leakage_density_correction_factor = acceptance_at ( self . default_x ) return self . _leakage_density_correction_factor # type:ignore log_prob ( self , theta , x = None , norm_posterior = True , track_gradients = False ) Returns the log-probability of the posterior p(\\theta|x). p(\\theta|x). Parameters: Name Type Description Default theta Tensor Parameters \\theta \\theta . required x Optional[torch.Tensor] Conditioning context for posterior p(\\theta|x) p(\\theta|x) . If not provided, fall back onto an x_o if previously provided for multi-round training, or to another default if set later for convenience, see .set_default_x() . None norm_posterior bool Whether to enforce a normalized posterior density. Renormalization of the posterior is useful when some probability falls out or leaks out of the prescribed prior support. The normalizing factor is calculated via rejection sampling, so if you need speedier but unnormalized log posterior estimates set here norm_posterior_snpe=False . The returned log posterior is set to -\u221e outside of the prior support regardless of this setting. True track_gradients bool Whether the returned tensor supports tracking gradients. This can be helpful for e.g. sensitivity analysis, but increases memory consumption. False Returns: Type Description Tensor (len(\u03b8),) -shaped log posterior probability \\log p(\\theta|x) \\log p(\\theta|x) for \u03b8 in the support of the prior, -\u221e (corresponding to 0 probability) outside. Source code in sbi/inference/posteriors/direct_posterior.py 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 def log_prob ( self , theta : Tensor , x : Optional [ Tensor ] = None , norm_posterior : bool = True , track_gradients : bool = False , ) -> Tensor : r \"\"\" Returns the log-probability of the posterior $p(\\theta|x).$ Args: theta: Parameters $\\theta$. x: Conditioning context for posterior $p(\\theta|x)$. If not provided, fall back onto an `x_o` if previously provided for multi-round training, or to another default if set later for convenience, see `.set_default_x()`. norm_posterior: Whether to enforce a normalized posterior density. Renormalization of the posterior is useful when some probability falls out or leaks out of the prescribed prior support. The normalizing factor is calculated via rejection sampling, so if you need speedier but unnormalized log posterior estimates set here `norm_posterior_snpe=False`. The returned log posterior is set to -\u221e outside of the prior support regardless of this setting. track_gradients: Whether the returned tensor supports tracking gradients. This can be helpful for e.g. sensitivity analysis, but increases memory consumption. Returns: `(len(\u03b8),)`-shaped log posterior probability $\\log p(\\theta|x)$ for \u03b8 in the support of the prior, -\u221e (corresponding to 0 probability) outside. \"\"\" # TODO Train exited here, entered after sampling? self . net . eval () theta , x = self . _prepare_theta_and_x_for_log_prob_ ( theta , x ) with torch . set_grad_enabled ( track_gradients ): unnorm_log_prob = self . net . log_prob ( theta , x ) # Force probability to be zero outside prior support. is_prior_finite = torch . isfinite ( self . _prior . log_prob ( theta )) masked_log_prob = torch . where ( is_prior_finite , unnorm_log_prob , torch . tensor ( float ( \"-inf\" ), dtype = torch . float32 ), ) log_factor = ( log ( self . leakage_correction ( x = batched_first_of_batch ( x ))) if norm_posterior else 0 ) return masked_log_prob - log_factor sample ( self , sample_shape = torch . Size ([]), x = None , show_progress_bars = True , sample_with_mcmc = None , mcmc_method = None , mcmc_parameters = None ) Return samples from posterior distribution p(\\theta|x) p(\\theta|x) . Samples are obtained either with rejection sampling or MCMC. Rejection sampling will be a lot faster if leakage is rather low. If leakage is high (e.g. above 99%, which can happen in multi-round SNPE), MCMC can be faster than rejection sampling. Parameters: Name Type Description Default sample_shape Union[torch.Size, Tuple[int, ...]] Desired shape of samples that are drawn from posterior. If sample_shape is multidimensional we simply draw sample_shape.numel() samples and then reshape into the desired shape. torch.Size([]) x Optional[torch.Tensor] Conditioning context for posterior p(\\theta|x) p(\\theta|x) . If not provided, fall back onto x_o if previously provided for multiround training, or to a set default (see set_default_x() method). None show_progress_bars bool Whether to show sampling progress monitor. True sample_with_mcmc Optional[bool] Optional parameter to override self.sample_with_mcmc . None mcmc_method Optional[str] Optional parameter to override self.mcmc_method . None mcmc_parameters Optional[Dict[str, Any]] Dictionary overriding the default parameters for MCMC. The following parameters are supported: thin to set the thinning factor for the chain, warmup_steps to set the initial number of samples to discard, num_chains for the number of chains, init_strategy for the initialisation strategy for chains; prior will draw init locations from prior, whereas sir will use Sequential- Importance-Resampling using init_strategy_num_candidates to find init locations. None Returns: Type Description Tensor Samples from posterior. Source code in sbi/inference/posteriors/direct_posterior.py 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 def sample ( self , sample_shape : Shape = torch . Size (), x : Optional [ Tensor ] = None , show_progress_bars : bool = True , sample_with_mcmc : Optional [ bool ] = None , mcmc_method : Optional [ str ] = None , mcmc_parameters : Optional [ Dict [ str , Any ]] = None , ) -> Tensor : r \"\"\" Return samples from posterior distribution $p(\\theta|x)$. Samples are obtained either with rejection sampling or MCMC. Rejection sampling will be a lot faster if leakage is rather low. If leakage is high (e.g. above 99%, which can happen in multi-round SNPE), MCMC can be faster than rejection sampling. Args: sample_shape: Desired shape of samples that are drawn from posterior. If sample_shape is multidimensional we simply draw `sample_shape.numel()` samples and then reshape into the desired shape. x: Conditioning context for posterior $p(\\theta|x)$. If not provided, fall back onto `x_o` if previously provided for multiround training, or to a set default (see `set_default_x()` method). show_progress_bars: Whether to show sampling progress monitor. sample_with_mcmc: Optional parameter to override `self.sample_with_mcmc`. mcmc_method: Optional parameter to override `self.mcmc_method`. mcmc_parameters: Dictionary overriding the default parameters for MCMC. The following parameters are supported: `thin` to set the thinning factor for the chain, `warmup_steps` to set the initial number of samples to discard, `num_chains` for the number of chains, `init_strategy` for the initialisation strategy for chains; `prior` will draw init locations from prior, whereas `sir` will use Sequential- Importance-Resampling using `init_strategy_num_candidates` to find init locations. Returns: Samples from posterior. \"\"\" x , num_samples , mcmc_method , mcmc_parameters = self . _prepare_for_sample ( x , sample_shape , mcmc_method , mcmc_parameters ) sample_with_mcmc = ( sample_with_mcmc if sample_with_mcmc is not None else self . sample_with_mcmc ) if sample_with_mcmc : samples = self . _sample_posterior_mcmc ( x = x , num_samples = num_samples , show_progress_bars = show_progress_bars , mcmc_method = mcmc_method , ** mcmc_parameters , ) else : # Rejection sampling. samples , _ = utils . sample_posterior_within_prior ( self . net , self . _prior , x , num_samples = num_samples , show_progress_bars = show_progress_bars , ) return samples . reshape (( * sample_shape , - 1 )) set_default_x ( self , x ) inherited Set new default x for .sample(), .log_prob to use as conditioning context. This is a pure convenience to avoid having to repeatedly specify x in calls to .sample() and .log_prob() - only \u03b8 needs to be passed. This convenience is particularly useful when the posterior is focused, i.e. has been trained over multiple rounds to be accurate in the vicinity of a particular x=x_o (you can check if your posterior object is focused by printing it). NOTE: this method is chainable, i.e. will return the NeuralPosterior object so that calls like posterior.set_default_x(my_x).sample(mytheta) are possible. Parameters: Name Type Description Default x Tensor The default observation to set for the posterior p(theta|x) p(theta|x) . required Returns: Type Description NeuralPosterior NeuralPosterior that will use a default x when not explicitly passed. Source code in sbi/inference/posteriors/direct_posterior.py 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 def set_default_x ( self , x : Tensor ) -> \"NeuralPosterior\" : \"\"\"Set new default x for `.sample(), .log_prob` to use as conditioning context. This is a pure convenience to avoid having to repeatedly specify `x` in calls to `.sample()` and `.log_prob()` - only \u03b8 needs to be passed. This convenience is particularly useful when the posterior is focused, i.e. has been trained over multiple rounds to be accurate in the vicinity of a particular `x=x_o` (you can check if your posterior object is focused by printing it). NOTE: this method is chainable, i.e. will return the NeuralPosterior object so that calls like `posterior.set_default_x(my_x).sample(mytheta)` are possible. Args: x: The default observation to set for the posterior $p(theta|x)$. Returns: `NeuralPosterior` that will use a default `x` when not explicitly passed. \"\"\" processed_x = process_x ( x , self . _x_shape ) self . _x = processed_x return self set_mcmc_method ( self , method ) inherited Sets sampling method to for MCMC and returns NeuralPosterior . Parameters: Name Type Description Default method str Method to use. required Returns: Type Description NeuralPosterior NeuralPosterior for chainable calls. Source code in sbi/inference/posteriors/direct_posterior.py 136 137 138 139 140 141 142 143 144 145 146 def set_mcmc_method ( self , method : str ) -> \"NeuralPosterior\" : \"\"\"Sets sampling method to for MCMC and returns `NeuralPosterior`. Args: method: Method to use. Returns: `NeuralPosterior` for chainable calls. \"\"\" self . _mcmc_method = method return self set_mcmc_parameters ( self , parameters ) inherited Sets parameters for MCMC and returns NeuralPosterior . Parameters: Name Type Description Default parameters Dict[str, Any] Dictionary overriding the default parameters for MCMC. The following parameters are supported: thin to set the thinning factor for the chain, warmup_steps to set the initial number of samples to discard, num_chains for the number of chains, init_strategy for the initialisation strategy for chains; prior will draw init locations from prior, whereas sir will use Sequential- Importance-Resampling using init_strategy_num_candidates to find init locations. required Returns: Type Description NeuralPosterior NeuralPosterior for chainable calls. Source code in sbi/inference/posteriors/direct_posterior.py 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 def set_mcmc_parameters ( self , parameters : Dict [ str , Any ]) -> \"NeuralPosterior\" : \"\"\"Sets parameters for MCMC and returns `NeuralPosterior`. Args: parameters: Dictionary overriding the default parameters for MCMC. The following parameters are supported: `thin` to set the thinning factor for the chain, `warmup_steps` to set the initial number of samples to discard, `num_chains` for the number of chains, `init_strategy` for the initialisation strategy for chains; `prior` will draw init locations from prior, whereas `sir` will use Sequential- Importance-Resampling using `init_strategy_num_candidates` to find init locations. Returns: `NeuralPosterior` for chainable calls. \"\"\" self . _mcmc_parameters = parameters return self set_sample_with_mcmc ( self , use_mcmc ) Turns MCMC sampling on or off and returns NeuralPosterior . Parameters: Name Type Description Default use_mcmc bool Flag to set whether or not MCMC sampling is used. required Returns: Type Description NeuralPosterior NeuralPosterior for chainable calls. Exceptions: Type Description ValueError on attempt to turn off MCMC sampling for family of methods that do not support rejection sampling. Source code in sbi/inference/posteriors/direct_posterior.py 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 def set_sample_with_mcmc ( self , use_mcmc : bool ) -> \"NeuralPosterior\" : \"\"\"Turns MCMC sampling on or off and returns `NeuralPosterior`. Args: use_mcmc: Flag to set whether or not MCMC sampling is used. Returns: `NeuralPosterior` for chainable calls. Raises: ValueError: on attempt to turn off MCMC sampling for family of methods that do not support rejection sampling. \"\"\" self . _sample_with_mcmc = use_mcmc return self sbi.inference.posteriors.likelihood_based_posterior.LikelihoodBasedPosterior \u00b6 Posterior p(\\theta|x) p(\\theta|x) with log_prob() and sample() methods, obtained with SNLE. SNLE trains a neural network to approximate the likelihood p(x|\\theta) p(x|\\theta) . The SNLE_Posterior class wraps the trained network such that one can directly evaluate the unnormalized posterior log probability p(\\theta|x) \\propto p(x|\\theta) \\cdot p(\\theta) p(\\theta|x) \\propto p(x|\\theta) \\cdot p(\\theta) and draw samples from the posterior with MCMC. The neural network itself can be accessed via the .net attribute. default_x: Optional [ torch . Tensor ] inherited property writable Return default x used by .sample(), .log_prob as conditioning context. mcmc_method: str inherited property writable Returns MCMC method. mcmc_parameters: dict inherited property writable Returns MCMC parameters. log_prob ( self , theta , x = None , track_gradients = False ) Returns the log-probability of p(x|\\theta) \\cdot p(\\theta). p(x|\\theta) \\cdot p(\\theta). This corresponds to an unnormalized posterior log-probability. Parameters: Name Type Description Default theta Tensor Parameters \\theta \\theta . required x Optional[torch.Tensor] Conditioning context for posterior p(\\theta|x) p(\\theta|x) . If not provided, fall back onto an x_o if previously provided for multi-round training, or to another default if set later for convenience, see .set_default_x() . None track_gradients bool Whether the returned tensor supports tracking gradients. This can be helpful for e.g. sensitivity analysis, but increases memory consumption. False Returns: Type Description Tensor (len(\u03b8),) -shaped log-probability \\log(p(x|\\theta) \\cdot p(\\theta)) \\log(p(x|\\theta) \\cdot p(\\theta)) . Source code in sbi/inference/posteriors/likelihood_based_posterior.py 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 def log_prob ( self , theta : Tensor , x : Optional [ Tensor ] = None , track_gradients : bool = False , ) -> Tensor : r \"\"\" Returns the log-probability of $p(x|\\theta) \\cdot p(\\theta).$ This corresponds to an **unnormalized** posterior log-probability. Args: theta: Parameters $\\theta$. x: Conditioning context for posterior $p(\\theta|x)$. If not provided, fall back onto an `x_o` if previously provided for multi-round training, or to another default if set later for convenience, see `.set_default_x()`. track_gradients: Whether the returned tensor supports tracking gradients. This can be helpful for e.g. sensitivity analysis, but increases memory consumption. Returns: `(len(\u03b8),)`-shaped log-probability $\\log(p(x|\\theta) \\cdot p(\\theta))$. \"\"\" # TODO Train exited here, entered after sampling? self . net . eval () theta , x = self . _prepare_theta_and_x_for_log_prob_ ( theta , x ) warn ( \"The log probability from SNL is only correct up to a normalizing constant.\" ) with torch . set_grad_enabled ( track_gradients ): return self . net . log_prob ( x , theta ) + self . _prior . log_prob ( theta ) sample ( self , sample_shape = torch . Size ([]), x = None , show_progress_bars = True , sample_with_mcmc = None , mcmc_method = None , mcmc_parameters = None ) Return samples from posterior distribution p(\\theta|x) p(\\theta|x) with MCMC. Parameters: Name Type Description Default sample_shape Union[torch.Size, Tuple[int, ...]] Desired shape of samples that are drawn from posterior. If sample_shape is multidimensional we simply draw sample_shape.numel() samples and then reshape into the desired shape. torch.Size([]) x Optional[torch.Tensor] Conditioning context for posterior p(\\theta|x) p(\\theta|x) . If not provided, fall back onto x_o if previously provided for multiround training, or to a set default (see set_default_x() method). None show_progress_bars bool Whether to show sampling progress monitor. True sample_with_mcmc Optional[bool] Optional parameter to override self.sample_with_mcmc . None mcmc_method Optional[str] Optional parameter to override self.mcmc_method . None mcmc_parameters Optional[Dict[str, Any]] Dictionary overriding the default parameters for MCMC. The following parameters are supported: thin to set the thinning factor for the chain, warmup_steps to set the initial number of samples to discard, num_chains for the number of chains, init_strategy for the initialisation strategy for chains; prior will draw init locations from prior, whereas sir will use Sequential- Importance-Resampling using init_strategy_num_candidates to find init locations. None Returns: Type Description Tensor Samples from posterior. Source code in sbi/inference/posteriors/likelihood_based_posterior.py 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 def sample ( self , sample_shape : Shape = torch . Size (), x : Optional [ Tensor ] = None , show_progress_bars : bool = True , sample_with_mcmc : Optional [ bool ] = None , mcmc_method : Optional [ str ] = None , mcmc_parameters : Optional [ Dict [ str , Any ]] = None , ) -> Tensor : r \"\"\" Return samples from posterior distribution $p(\\theta|x)$ with MCMC. Args: sample_shape: Desired shape of samples that are drawn from posterior. If sample_shape is multidimensional we simply draw `sample_shape.numel()` samples and then reshape into the desired shape. x: Conditioning context for posterior $p(\\theta|x)$. If not provided, fall back onto `x_o` if previously provided for multiround training, or to a set default (see `set_default_x()` method). show_progress_bars: Whether to show sampling progress monitor. sample_with_mcmc: Optional parameter to override `self.sample_with_mcmc`. mcmc_method: Optional parameter to override `self.mcmc_method`. mcmc_parameters: Dictionary overriding the default parameters for MCMC. The following parameters are supported: `thin` to set the thinning factor for the chain, `warmup_steps` to set the initial number of samples to discard, `num_chains` for the number of chains, `init_strategy` for the initialisation strategy for chains; `prior` will draw init locations from prior, whereas `sir` will use Sequential- Importance-Resampling using `init_strategy_num_candidates` to find init locations. Returns: Samples from posterior. \"\"\" x , num_samples , mcmc_method , mcmc_parameters = self . _prepare_for_sample ( x , sample_shape , mcmc_method , mcmc_parameters ) samples = self . _sample_posterior_mcmc ( x = x , num_samples = num_samples , show_progress_bars = show_progress_bars , mcmc_method = mcmc_method , ** mcmc_parameters , ) return samples . reshape (( * sample_shape , - 1 )) set_default_x ( self , x ) inherited Set new default x for .sample(), .log_prob to use as conditioning context. This is a pure convenience to avoid having to repeatedly specify x in calls to .sample() and .log_prob() - only \u03b8 needs to be passed. This convenience is particularly useful when the posterior is focused, i.e. has been trained over multiple rounds to be accurate in the vicinity of a particular x=x_o (you can check if your posterior object is focused by printing it). NOTE: this method is chainable, i.e. will return the NeuralPosterior object so that calls like posterior.set_default_x(my_x).sample(mytheta) are possible. Parameters: Name Type Description Default x Tensor The default observation to set for the posterior p(theta|x) p(theta|x) . required Returns: Type Description NeuralPosterior NeuralPosterior that will use a default x when not explicitly passed. Source code in sbi/inference/posteriors/likelihood_based_posterior.py 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 def set_default_x ( self , x : Tensor ) -> \"NeuralPosterior\" : \"\"\"Set new default x for `.sample(), .log_prob` to use as conditioning context. This is a pure convenience to avoid having to repeatedly specify `x` in calls to `.sample()` and `.log_prob()` - only \u03b8 needs to be passed. This convenience is particularly useful when the posterior is focused, i.e. has been trained over multiple rounds to be accurate in the vicinity of a particular `x=x_o` (you can check if your posterior object is focused by printing it). NOTE: this method is chainable, i.e. will return the NeuralPosterior object so that calls like `posterior.set_default_x(my_x).sample(mytheta)` are possible. Args: x: The default observation to set for the posterior $p(theta|x)$. Returns: `NeuralPosterior` that will use a default `x` when not explicitly passed. \"\"\" processed_x = process_x ( x , self . _x_shape ) self . _x = processed_x return self set_mcmc_method ( self , method ) inherited Sets sampling method to for MCMC and returns NeuralPosterior . Parameters: Name Type Description Default method str Method to use. required Returns: Type Description NeuralPosterior NeuralPosterior for chainable calls. Source code in sbi/inference/posteriors/likelihood_based_posterior.py 136 137 138 139 140 141 142 143 144 145 146 def set_mcmc_method ( self , method : str ) -> \"NeuralPosterior\" : \"\"\"Sets sampling method to for MCMC and returns `NeuralPosterior`. Args: method: Method to use. Returns: `NeuralPosterior` for chainable calls. \"\"\" self . _mcmc_method = method return self set_mcmc_parameters ( self , parameters ) inherited Sets parameters for MCMC and returns NeuralPosterior . Parameters: Name Type Description Default parameters Dict[str, Any] Dictionary overriding the default parameters for MCMC. The following parameters are supported: thin to set the thinning factor for the chain, warmup_steps to set the initial number of samples to discard, num_chains for the number of chains, init_strategy for the initialisation strategy for chains; prior will draw init locations from prior, whereas sir will use Sequential- Importance-Resampling using init_strategy_num_candidates to find init locations. required Returns: Type Description NeuralPosterior NeuralPosterior for chainable calls. Source code in sbi/inference/posteriors/likelihood_based_posterior.py 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 def set_mcmc_parameters ( self , parameters : Dict [ str , Any ]) -> \"NeuralPosterior\" : \"\"\"Sets parameters for MCMC and returns `NeuralPosterior`. Args: parameters: Dictionary overriding the default parameters for MCMC. The following parameters are supported: `thin` to set the thinning factor for the chain, `warmup_steps` to set the initial number of samples to discard, `num_chains` for the number of chains, `init_strategy` for the initialisation strategy for chains; `prior` will draw init locations from prior, whereas `sir` will use Sequential- Importance-Resampling using `init_strategy_num_candidates` to find init locations. Returns: `NeuralPosterior` for chainable calls. \"\"\" self . _mcmc_parameters = parameters return self sbi.inference.posteriors.ratio_based_posterior.RatioBasedPosterior \u00b6 Posterior p(\\theta|x) p(\\theta|x) with log_prob() and sample() methods, obtained with SNRE. SNRE trains a neural network to approximate likelihood ratios, which in turn can be used obtain an unnormalized posterior p(\\theta|x) \\propto p(x|\\theta) \\cdot p(\\theta) p(\\theta|x) \\propto p(x|\\theta) \\cdot p(\\theta) . The SNRE_Posterior class wraps the trained network such that one can directly evaluate the unnormalized posterior log-probability p(\\theta|x) \\propto p(x|\\theta) \\cdot p(\\theta) p(\\theta|x) \\propto p(x|\\theta) \\cdot p(\\theta) and draw samples from the posterior with MCMC. Note that, in the case of single-round SNRE_A / AALR, it is possible to evaluate the log-probability of the normalized posterior, but sampling still requires MCMC. The neural network itself can be accessed via the .net attribute. default_x: Optional [ torch . Tensor ] inherited property writable Return default x used by .sample(), .log_prob as conditioning context. mcmc_method: str inherited property writable Returns MCMC method. mcmc_parameters: dict inherited property writable Returns MCMC parameters. log_prob ( self , theta , x = None , track_gradients = False ) Returns the log-probability of p(x|\\theta) \\cdot p(\\theta). p(x|\\theta) \\cdot p(\\theta). This corresponds to an unnormalized posterior log-probability. Only for single-round SNRE_A / AALR, the returned log-probability will correspond to the normalized log-probability. Parameters: Name Type Description Default theta Tensor Parameters \\theta \\theta . required x Optional[torch.Tensor] Conditioning context for posterior p(\\theta|x) p(\\theta|x) . If not provided, fall back onto an x_o if previously provided for multi-round training, or to another default if set later for convenience, see .set_default_x() . None track_gradients bool Whether the returned tensor supports tracking gradients. This can be helpful for e.g. sensitivity analysis, but increases memory consumption. False Returns: Type Description Tensor (len(\u03b8),) -shaped log-probability \\log(p(x|\\theta) \\cdot p(\\theta)) \\log(p(x|\\theta) \\cdot p(\\theta)) . Source code in sbi/inference/posteriors/ratio_based_posterior.py 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 def log_prob ( self , theta : Tensor , x : Optional [ Tensor ] = None , track_gradients : bool = False , ) -> Tensor : r \"\"\" Returns the log-probability of $p(x|\\theta) \\cdot p(\\theta).$ This corresponds to an **unnormalized** posterior log-probability. Only for single-round SNRE_A / AALR, the returned log-probability will correspond to the **normalized** log-probability. Args: theta: Parameters $\\theta$. x: Conditioning context for posterior $p(\\theta|x)$. If not provided, fall back onto an `x_o` if previously provided for multi-round training, or to another default if set later for convenience, see `.set_default_x()`. track_gradients: Whether the returned tensor supports tracking gradients. This can be helpful for e.g. sensitivity analysis, but increases memory consumption. Returns: `(len(\u03b8),)`-shaped log-probability $\\log(p(x|\\theta) \\cdot p(\\theta))$. \"\"\" # TODO Train exited here, entered after sampling? self . net . eval () theta , x = self . _prepare_theta_and_x_for_log_prob_ ( theta , x ) self . _warn_log_prob_snre () with torch . set_grad_enabled ( track_gradients ): log_ratio = self . net ( torch . cat (( theta , x ), dim = 1 )) . reshape ( - 1 ) return log_ratio + self . _prior . log_prob ( theta ) sample ( self , sample_shape = torch . Size ([]), x = None , show_progress_bars = True , sample_with_mcmc = None , mcmc_method = None , mcmc_parameters = None ) Return samples from posterior distribution p(\\theta|x) p(\\theta|x) with MCMC. Parameters: Name Type Description Default sample_shape Union[torch.Size, Tuple[int, ...]] Desired shape of samples that are drawn from posterior. If sample_shape is multidimensional we simply draw sample_shape.numel() samples and then reshape into the desired shape. torch.Size([]) x Optional[torch.Tensor] Conditioning context for posterior p(\\theta|x) p(\\theta|x) . If not provided, fall back onto x_o if previously provided for multiround training, or to a set default (see set_default_x() method). None show_progress_bars bool Whether to show sampling progress monitor. True sample_with_mcmc Optional[bool] Optional parameter to override self.sample_with_mcmc . None mcmc_method Optional[str] Optional parameter to override self.mcmc_method . None mcmc_parameters Optional[Dict[str, Any]] Dictionary overriding the default parameters for MCMC. The following parameters are supported: thin to set the thinning factor for the chain, warmup_steps to set the initial number of samples to discard, num_chains for the number of chains, init_strategy for the initialisation strategy for chains; prior will draw init locations from prior, whereas sir will use Sequential- Importance-Resampling using init_strategy_num_candidates to find init locations. None Returns: Type Description Tensor Samples from posterior. Source code in sbi/inference/posteriors/ratio_based_posterior.py 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 def sample ( self , sample_shape : Shape = torch . Size (), x : Optional [ Tensor ] = None , show_progress_bars : bool = True , sample_with_mcmc : Optional [ bool ] = None , mcmc_method : Optional [ str ] = None , mcmc_parameters : Optional [ Dict [ str , Any ]] = None , ) -> Tensor : r \"\"\" Return samples from posterior distribution $p(\\theta|x)$ with MCMC. Args: sample_shape: Desired shape of samples that are drawn from posterior. If sample_shape is multidimensional we simply draw `sample_shape.numel()` samples and then reshape into the desired shape. x: Conditioning context for posterior $p(\\theta|x)$. If not provided, fall back onto `x_o` if previously provided for multiround training, or to a set default (see `set_default_x()` method). show_progress_bars: Whether to show sampling progress monitor. sample_with_mcmc: Optional parameter to override `self.sample_with_mcmc`. mcmc_method: Optional parameter to override `self.mcmc_method`. mcmc_parameters: Dictionary overriding the default parameters for MCMC. The following parameters are supported: `thin` to set the thinning factor for the chain, `warmup_steps` to set the initial number of samples to discard, `num_chains` for the number of chains, `init_strategy` for the initialisation strategy for chains; `prior` will draw init locations from prior, whereas `sir` will use Sequential- Importance-Resampling using `init_strategy_num_candidates` to find init locations. Returns: Samples from posterior. \"\"\" x , num_samples , mcmc_method , mcmc_parameters = self . _prepare_for_sample ( x , sample_shape , mcmc_method , mcmc_parameters ) samples = self . _sample_posterior_mcmc ( x = x , num_samples = num_samples , show_progress_bars = show_progress_bars , mcmc_method = mcmc_method , ** mcmc_parameters , ) return samples . reshape (( * sample_shape , - 1 )) set_default_x ( self , x ) inherited Set new default x for .sample(), .log_prob to use as conditioning context. This is a pure convenience to avoid having to repeatedly specify x in calls to .sample() and .log_prob() - only \u03b8 needs to be passed. This convenience is particularly useful when the posterior is focused, i.e. has been trained over multiple rounds to be accurate in the vicinity of a particular x=x_o (you can check if your posterior object is focused by printing it). NOTE: this method is chainable, i.e. will return the NeuralPosterior object so that calls like posterior.set_default_x(my_x).sample(mytheta) are possible. Parameters: Name Type Description Default x Tensor The default observation to set for the posterior p(theta|x) p(theta|x) . required Returns: Type Description NeuralPosterior NeuralPosterior that will use a default x when not explicitly passed. Source code in sbi/inference/posteriors/ratio_based_posterior.py 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 def set_default_x ( self , x : Tensor ) -> \"NeuralPosterior\" : \"\"\"Set new default x for `.sample(), .log_prob` to use as conditioning context. This is a pure convenience to avoid having to repeatedly specify `x` in calls to `.sample()` and `.log_prob()` - only \u03b8 needs to be passed. This convenience is particularly useful when the posterior is focused, i.e. has been trained over multiple rounds to be accurate in the vicinity of a particular `x=x_o` (you can check if your posterior object is focused by printing it). NOTE: this method is chainable, i.e. will return the NeuralPosterior object so that calls like `posterior.set_default_x(my_x).sample(mytheta)` are possible. Args: x: The default observation to set for the posterior $p(theta|x)$. Returns: `NeuralPosterior` that will use a default `x` when not explicitly passed. \"\"\" processed_x = process_x ( x , self . _x_shape ) self . _x = processed_x return self set_mcmc_method ( self , method ) inherited Sets sampling method to for MCMC and returns NeuralPosterior . Parameters: Name Type Description Default method str Method to use. required Returns: Type Description NeuralPosterior NeuralPosterior for chainable calls. Source code in sbi/inference/posteriors/ratio_based_posterior.py 136 137 138 139 140 141 142 143 144 145 146 def set_mcmc_method ( self , method : str ) -> \"NeuralPosterior\" : \"\"\"Sets sampling method to for MCMC and returns `NeuralPosterior`. Args: method: Method to use. Returns: `NeuralPosterior` for chainable calls. \"\"\" self . _mcmc_method = method return self set_mcmc_parameters ( self , parameters ) inherited Sets parameters for MCMC and returns NeuralPosterior . Parameters: Name Type Description Default parameters Dict[str, Any] Dictionary overriding the default parameters for MCMC. The following parameters are supported: thin to set the thinning factor for the chain, warmup_steps to set the initial number of samples to discard, num_chains for the number of chains, init_strategy for the initialisation strategy for chains; prior will draw init locations from prior, whereas sir will use Sequential- Importance-Resampling using init_strategy_num_candidates to find init locations. required Returns: Type Description NeuralPosterior NeuralPosterior for chainable calls. Source code in sbi/inference/posteriors/ratio_based_posterior.py 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 def set_mcmc_parameters ( self , parameters : Dict [ str , Any ]) -> \"NeuralPosterior\" : \"\"\"Sets parameters for MCMC and returns `NeuralPosterior`. Args: parameters: Dictionary overriding the default parameters for MCMC. The following parameters are supported: `thin` to set the thinning factor for the chain, `warmup_steps` to set the initial number of samples to discard, `num_chains` for the number of chains, `init_strategy` for the initialisation strategy for chains; `prior` will draw init locations from prior, whereas `sir` will use Sequential- Importance-Resampling using `init_strategy_num_candidates` to find init locations. Returns: `NeuralPosterior` for chainable calls. \"\"\" self . _mcmc_parameters = parameters return self Models \u00b6 sbi.utils.get_nn_models.posterior_nn ( model , ** kwargs ) \u00b6 Source code in sbi/utils/get_nn_models.py 48 49 50 51 52 53 54 55 56 57 58 59 60 61 def posterior_nn ( model : str , ** kwargs : Any ) -> Callable : def build_fn ( batch_theta , batch_x ): if model == \"mdn\" : return build_mdn ( batch_x = batch_theta , batch_y = batch_x , ** kwargs ) if model == \"made\" : return build_made ( batch_x = batch_theta , batch_y = batch_x , ** kwargs ) if model == \"maf\" : return build_maf ( batch_x = batch_theta , batch_y = batch_x , ** kwargs ) elif model == \"nsf\" : return build_nsf ( batch_x = batch_theta , batch_y = batch_x , ** kwargs ) else : raise NotImplementedError return build_fn sbi.utils.get_nn_models.likelihood_nn ( model , ** kwargs ) \u00b6 Source code in sbi/utils/get_nn_models.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 def likelihood_nn ( model : str , ** kwargs : Any ) -> Callable : def build_fn ( batch_theta , batch_x ): if model == \"mdn\" : return build_mdn ( batch_x = batch_x , batch_y = batch_theta , ** kwargs ) if model == \"made\" : return build_made ( batch_x = batch_x , batch_y = batch_theta , ** kwargs ) if model == \"maf\" : return build_maf ( batch_x = batch_x , batch_y = batch_theta , ** kwargs ) elif model == \"nsf\" : return build_nsf ( batch_x = batch_x , batch_y = batch_theta , ** kwargs ) else : raise NotImplementedError return build_fn sbi.utils.get_nn_models.classifier_nn ( model , ** kwargs ) \u00b6 Source code in sbi/utils/get_nn_models.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 def classifier_nn ( model : str , ** kwargs : Any ) -> Callable : def build_fn ( batch_theta , batch_x ): if model == \"linear\" : return build_linear_classifier ( batch_x = batch_x , batch_y = batch_theta , ** kwargs ) if model == \"mlp\" : return build_mlp_classifier ( batch_x = batch_x , batch_y = batch_theta , ** kwargs ) if model == \"resnet\" : return build_resnet_classifier ( batch_x = batch_x , batch_y = batch_theta , ** kwargs ) raise NotImplementedError return build_fn Utils \u00b6 sbi.utils.plot.pairplot ( samples , points = None , upper = 'hist' , diag = 'hist' , title = None , legend = False , labels = None , labels_points = None , labels_samples = None , samples_colors = [ '#1f77b4' , '#ff7f0e' , '#2ca02c' , '#d62728' , '#9467bd' , '#8c564b' , '#e377c2' , '#7f7f7f' , '#bcbd22' , '#17becf' ], points_colors = [ '#1f77b4' , '#ff7f0e' , '#2ca02c' , '#d62728' , '#9467bd' , '#8c564b' , '#e377c2' , '#7f7f7f' , '#bcbd22' , '#17becf' ], subset = None , limits = None , ticks = None , tickformatter =< matplotlib . ticker . FormatStrFormatter object at 0x7fe482235e20 > , tick_labels = None , hist_diag = { 'alpha' : 1.0 , 'bins' : 50 , 'density' : False , 'histtype' : 'step' }, hist_offdiag = { 'bins' : 50 }, kde_diag = { 'bw_method' : 'scott' , 'bins' : 50 , 'color' : 'black' }, kde_offdiag = { 'bw_method' : 'scott' , 'bins' : 50 }, contour_offdiag = { 'levels' : [ 0.68 ], 'percentile' : True }, scatter_offdiag = { 'alpha' : 0.5 , 'edgecolor' : 'none' , 'rasterized' : False }, plot_offdiag = {}, points_diag = {}, points_offdiag = { 'marker' : '.' , 'markersize' : 20 }, fig_size = ( 10 , 10 ), fig_bg_colors = { 'upper' : None , 'diag' : None , 'lower' : None }, fig_subplots_adjust = { 'top' : 0.9 }, subplots = {}, despine = { 'offset' : 5 }, title_format = { 'fontsize' : 16 }) \u00b6 Plot samples and points. For developers: if you add arguments that expect dictionaries, make sure to access them via the opts dictionary instantiated below. E.g. if you want to access the dict stored in the input variable hist_diag, use opts[ hist_diag ]. Parameters: Name Type Description Default samples Union[List[numpy.ndarray], List[torch.Tensor], numpy.ndarray, torch.Tensor] posterior samples used to build the histogram required points Optional[Union[List[numpy.ndarray], List[torch.Tensor], numpy.ndarray, torch.Tensor]] list of additional points to scatter None upper Optional[str] plotting style for upper diagonal, {hist, scatter, contour, None} 'hist' diag Optional[str] plotting style for diagonal, {hist, None} 'hist' title Optional[str] title string None legend Optional[bool] whether to plot a legend for the points False labels np.ndarray of strings specifying the names of the parameters None labels_points np.ndarray of strings specifying the names of the passed points None labels_samples np.ndarray of strings specifying the names of the passed samples None samples_colors colors of the samples ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf'] points_colors colors of the points ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf'] subset List containing the dimensions to plot. E.g. subset=[1,3] will plot plot only the 1 st and 3 rd dimension but will discard the 0 th and 2 nd (and, if they exist, the 4 th , 5 th and so on) None limits array containing the plot xlim for each parameter dimension. If None, just use the min and max of the passed samples None ticks location of the ticks for each parameter. If None, just use the min and max along each parameter dimension None tickformatter passed to _format_axis() <matplotlib.ticker.FormatStrFormatter object at 0x7fe482235e20> tick_labels np.ndarray containing the ticklabels. None hist_diag dictionary passed to plt.hist() for diagonal plots {'alpha': 1.0, 'bins': 50, 'density': False, 'histtype': 'step'} hist_offdiag dictionary passed to np.histogram2d() for off diagonal plots {'bins': 50} kde_diag dictionary passed to gaussian_kde() for diagonal plots {'bw_method': 'scott', 'bins': 50, 'color': 'black'} kde_offdiag dictionary passed to gaussian_kde() for off diagonal plots {'bw_method': 'scott', 'bins': 50} contour_offdiag dictionary that should contain percentile and levels keys. percentile : bool. If percentile ==True, the levels are made with respect to the max probability of the posterior If percentile ==False, the levels are drawn at absolute positions levels : list or np.ndarray: specifies the location where the contours are drawn. {'levels': [0.68], 'percentile': True} scatter_offdiag dictionary for plt.scatter() on off diagonal {'alpha': 0.5, 'edgecolor': 'none', 'rasterized': False} plot_offdiag dictionary for plt.plot() on off diagonal {} points_diag dictionary for plt.plot() used for plotting points on diagonal {} points_offdiag dictionary for plt.plot() used for plotting points on off diagonal {'marker': '.', 'markersize': 20} fig_size Tuple size of the entire figure (10, 10) fig_bg_colors Dictionary that contains upper , diag , lower , and specifies the respective background colors. Passed to ax.set_facecolor() {'upper': None, 'diag': None, 'lower': None} fig_subplots_adjust dictionary passed to fig.subplots_adjust() {'top': 0.9} subplots dictionary passed to plt.subplots() {} despine dictionary passed to set_position() for axis position {'offset': 5} title_format dictionary passed to plt.title() {'fontsize': 16} Returns: figure and axis of posterior distribution plot Source code in sbi/utils/plot.py 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 def pairplot ( samples : Union [ List [ np . ndarray ], List [ torch . Tensor ], np . ndarray , torch . Tensor ], points : Optional [ Union [ List [ np . ndarray ], List [ torch . Tensor ], np . ndarray , torch . Tensor ] ] = None , upper : Optional [ str ] = \"hist\" , diag : Optional [ str ] = \"hist\" , title : Optional [ str ] = None , legend : Optional [ bool ] = False , labels = None , labels_points = None , labels_samples = None , samples_colors = plt . rcParams [ \"axes.prop_cycle\" ] . by_key ()[ \"color\" ], points_colors = plt . rcParams [ \"axes.prop_cycle\" ] . by_key ()[ \"color\" ], subset = None , limits = None , ticks = None , tickformatter = mpl . ticker . FormatStrFormatter ( \" %g \" ), tick_labels = None , hist_diag = { \"alpha\" : 1.0 , \"bins\" : 50 , \"density\" : False , \"histtype\" : \"step\" }, hist_offdiag = { \"bins\" : 50 ,}, kde_diag = { \"bw_method\" : \"scott\" , \"bins\" : 50 , \"color\" : \"black\" }, kde_offdiag = { \"bw_method\" : \"scott\" , \"bins\" : 50 }, contour_offdiag = { \"levels\" : [ 0.68 ], \"percentile\" : True }, scatter_offdiag = { \"alpha\" : 0.5 , \"edgecolor\" : \"none\" , \"rasterized\" : False }, plot_offdiag = {}, points_diag = {}, points_offdiag = { \"marker\" : \".\" , \"markersize\" : 20 }, fig_size : Tuple = ( 10 , 10 ), fig_bg_colors = { \"upper\" : None , \"diag\" : None , \"lower\" : None }, fig_subplots_adjust = { \"top\" : 0.9 }, subplots = {}, despine = { \"offset\" : 5 }, title_format = { \"fontsize\" : 16 }, ): \"\"\" Plot samples and points. For developers: if you add arguments that expect dictionaries, make sure to access them via the opts dictionary instantiated below. E.g. if you want to access the dict stored in the input variable hist_diag, use opts[`hist_diag`]. Args: samples: posterior samples used to build the histogram points: list of additional points to scatter upper: plotting style for upper diagonal, {hist, scatter, contour, None} diag: plotting style for diagonal, {hist, None} title: title string legend: whether to plot a legend for the points labels: np.ndarray of strings specifying the names of the parameters labels_points: np.ndarray of strings specifying the names of the passed points labels_samples: np.ndarray of strings specifying the names of the passed samples samples_colors: colors of the samples points_colors: colors of the points subset: List containing the dimensions to plot. E.g. subset=[1,3] will plot plot only the 1st and 3rd dimension but will discard the 0th and 2nd (and, if they exist, the 4th, 5th and so on) limits: array containing the plot xlim for each parameter dimension. If None, just use the min and max of the passed samples ticks: location of the ticks for each parameter. If None, just use the min and max along each parameter dimension tickformatter: passed to _format_axis() tick_labels: np.ndarray containing the ticklabels. hist_diag: dictionary passed to plt.hist() for diagonal plots hist_offdiag: dictionary passed to np.histogram2d() for off diagonal plots kde_diag: dictionary passed to gaussian_kde() for diagonal plots kde_offdiag: dictionary passed to gaussian_kde() for off diagonal plots contour_offdiag: dictionary that should contain `percentile` and `levels` keys. `percentile`: bool. If `percentile`==True, the levels are made with respect to the max probability of the posterior If `percentile`==False, the levels are drawn at absolute positions `levels`: list or np.ndarray: specifies the location where the contours are drawn. scatter_offdiag: dictionary for plt.scatter() on off diagonal plot_offdiag: dictionary for plt.plot() on off diagonal points_diag: dictionary for plt.plot() used for plotting points on diagonal points_offdiag: dictionary for plt.plot() used for plotting points on off diagonal fig_size: size of the entire figure fig_bg_colors: Dictionary that contains `upper`, `diag`, `lower`, and specifies the respective background colors. Passed to ax.set_facecolor() fig_subplots_adjust: dictionary passed to fig.subplots_adjust() subplots: dictionary passed to plt.subplots() despine: dictionary passed to set_position() for axis position title_format: dictionary passed to plt.title() Returns: figure and axis of posterior distribution plot \"\"\" # TODO: add color map support # TODO: automatically determine good bin sizes for histograms # TODO: add legend (if legend is True) # get default values of function arguments # https://stackoverflow.com/questions/12627118/get-a-function-arguments-default-value spec = inspect . getfullargspec ( pairplot ) # build a dict for the defaults # https://stackoverflow.com/questions/12627118/get-a-function-arguments-default-value # answer by gnr default_val_dict = dict ( zip ( spec . args [:: - 1 ], ( spec . defaults or ())[:: - 1 ])) # update the defaults dictionary by the current values of the variables (passed by # the user) opts = _update ( default_val_dict , locals ()) # Prepare samples if type ( samples ) != list : samples = ensure_numpy ( samples ) samples = [ samples ] else : for i , sample_pack in enumerate ( samples ): samples [ i ] = ensure_numpy ( samples [ i ]) # Prepare points if points is None : points = [] if type ( points ) != list : points = ensure_numpy ( points ) points = [ points ] points = [ np . atleast_2d ( p ) for p in points ] points = [ np . atleast_2d ( ensure_numpy ( p )) for p in points ] # Dimensions dim = samples [ 0 ] . shape [ 1 ] num_samples = samples [ 0 ] . shape [ 0 ] # TODO: add asserts checking compatibility of dimensions # Prepare labels if opts [ \"labels\" ] == [] or opts [ \"labels\" ] is None : labels_dim = [ \"dim {} \" . format ( i + 1 ) for i in range ( dim )] else : labels_dim = opts [ \"labels\" ] # Prepare limits if opts [ \"limits\" ] == [] or opts [ \"limits\" ] is None : limits = [] for d in range ( dim ): min = + np . inf max = - np . inf for sample in samples : min_ = sample [:, d ] . min () min = min_ if min_ < min else min max_ = sample [:, d ] . max () max = max_ if max_ > max else max limits . append ([ min , max ]) else : if len ( opts [ \"limits\" ]) == 1 : limits = [ opts [ \"limits\" ][ 0 ] for _ in range ( dim )] else : limits = opts [ \"limits\" ] # Prepare ticks if opts [ \"ticks\" ] == [] or opts [ \"ticks\" ] is None : ticks = None else : if len ( opts [ \"ticks\" ]) == 1 : ticks = [ opts [ \"ticks\" ][ 0 ] for _ in range ( dim )] else : ticks = opts [ \"ticks\" ] # Prepare diag/upper/lower if type ( opts [ \"diag\" ]) is not list : opts [ \"diag\" ] = [ opts [ \"diag\" ] for _ in range ( len ( samples ))] if type ( opts [ \"upper\" ]) is not list : opts [ \"upper\" ] = [ opts [ \"upper\" ] for _ in range ( len ( samples ))] # if type(opts['lower']) is not list: # opts['lower'] = [opts['lower'] for _ in range(len(samples))] opts [ \"lower\" ] = None # Figure out if we subset the plot subset = opts [ \"subset\" ] if subset is None : rows = cols = dim subset = [ i for i in range ( dim )] else : if type ( subset ) == int : subset = [ subset ] elif type ( subset ) == list : pass else : raise NotImplementedError rows = cols = len ( subset ) fig , axes = plt . subplots ( rows , cols , figsize = opts [ \"fig_size\" ], ** opts [ \"subplots\" ]) # Cast to ndarray in case of 1D subplots. axes = np . array ( axes ) . reshape ( rows , cols ) # Style figure fig . subplots_adjust ( ** opts [ \"fig_subplots_adjust\" ]) fig . suptitle ( opts [ \"title\" ], ** opts [ \"title_format\" ]) # Style axes row_idx = - 1 for row in range ( dim ): if row not in subset : continue else : row_idx += 1 col_idx = - 1 for col in range ( dim ): if col not in subset : continue else : col_idx += 1 if row == col : current = \"diag\" elif row < col : current = \"upper\" else : current = \"lower\" ax = axes [ row_idx , col_idx ] plt . sca ( ax ) # Background color if ( current in opts [ \"fig_bg_colors\" ] and opts [ \"fig_bg_colors\" ][ current ] is not None ): ax . set_facecolor ( opts [ \"fig_bg_colors\" ][ current ]) # Axes if opts [ current ] is None : ax . axis ( \"off\" ) continue # Limits if limits is not None : ax . set_xlim (( limits [ col ][ 0 ], limits [ col ][ 1 ])) if current != \"diag\" : ax . set_ylim (( limits [ row ][ 0 ], limits [ row ][ 1 ])) xmin , xmax = ax . get_xlim () ymin , ymax = ax . get_ylim () # Ticks if ticks is not None : ax . set_xticks (( ticks [ col ][ 0 ], ticks [ col ][ 1 ])) if current != \"diag\" : ax . set_yticks (( ticks [ row ][ 0 ], ticks [ row ][ 1 ])) # Despine ax . spines [ \"right\" ] . set_visible ( False ) ax . spines [ \"top\" ] . set_visible ( False ) ax . spines [ \"bottom\" ] . set_position (( \"outward\" , despine [ \"offset\" ])) # Formatting axes if current == \"diag\" : # off-diagnoals if opts [ \"lower\" ] is None or col == dim - 1 : _format_axis ( ax , xhide = False , xlabel = labels_dim [ col ], yhide = True , tickformatter = opts [ \"tickformatter\" ], ) else : _format_axis ( ax , xhide = True , yhide = True ) else : # off-diagnoals if row == dim - 1 : _format_axis ( ax , xhide = False , xlabel = labels_dim [ col ], yhide = True , tickformatter = opts [ \"tickformatter\" ], ) else : _format_axis ( ax , xhide = True , yhide = True ) if opts [ \"tick_labels\" ] is not None : ax . set_xticklabels ( ( str ( opts [ \"tick_labels\" ][ col ][ 0 ]), str ( opts [ \"tick_labels\" ][ col ][ 1 ]), ) ) # Diagonals if current == \"diag\" : if len ( samples ) > 0 : for n , v in enumerate ( samples ): if opts [ \"diag\" ][ n ] == \"hist\" : h = plt . hist ( v [:, row ], color = opts [ \"samples_colors\" ][ n ], ** opts [ \"hist_diag\" ] ) elif opts [ \"diag\" ][ n ] == \"kde\" : density = gaussian_kde ( v [:, row ], bw_method = opts [ \"kde_diag\" ][ \"bw_method\" ] ) xs = np . linspace ( xmin , xmax , opts [ \"kde_diag\" ][ \"bins\" ]) ys = density ( xs ) h = plt . plot ( xs , ys , color = opts [ \"samples_colors\" ][ n ],) else : pass if len ( points ) > 0 : extent = ax . get_ylim () for n , v in enumerate ( points ): h = plt . plot ( [ v [:, row ], v [:, row ]], extent , color = opts [ \"points_colors\" ][ n ], ** opts [ \"points_diag\" ] ) # Off-diagonals else : if len ( samples ) > 0 : for n , v in enumerate ( samples ): if opts [ \"upper\" ][ n ] == \"hist\" or opts [ \"upper\" ][ n ] == \"hist2d\" : # h = plt.hist2d( # v[:, col], v[:, row], # range=( # [limits[col][0], limits[col][1]], # [limits[row][0], limits[row][1]]), # **opts['hist_offdiag'] # ) hist , xedges , yedges = np . histogram2d ( v [:, col ], v [:, row ], range = [ [ limits [ col ][ 0 ], limits [ col ][ 1 ]], [ limits [ row ][ 0 ], limits [ row ][ 1 ]], ], ** opts [ \"hist_offdiag\" ] ) h = plt . imshow ( hist . T , origin = \"lower\" , extent = [ xedges [ 0 ], xedges [ - 1 ], yedges [ 0 ], yedges [ - 1 ],], aspect = \"auto\" , ) elif opts [ \"upper\" ][ n ] in [ \"kde\" , \"kde2d\" , \"contour\" , \"contourf\" , ]: density = gaussian_kde ( v [:, [ col , row ]] . T , bw_method = opts [ \"kde_offdiag\" ][ \"bw_method\" ], ) X , Y = np . meshgrid ( np . linspace ( limits [ col ][ 0 ], limits [ col ][ 1 ], opts [ \"kde_offdiag\" ][ \"bins\" ], ), np . linspace ( limits [ row ][ 0 ], limits [ row ][ 1 ], opts [ \"kde_offdiag\" ][ \"bins\" ], ), ) positions = np . vstack ([ X . ravel (), Y . ravel ()]) Z = np . reshape ( density ( positions ) . T , X . shape ) if opts [ \"upper\" ][ n ] == \"kde\" or opts [ \"upper\" ][ n ] == \"kde2d\" : h = plt . imshow ( Z , extent = [ limits [ col ][ 0 ], limits [ col ][ 1 ], limits [ row ][ 0 ], limits [ row ][ 1 ], ], origin = \"lower\" , aspect = \"auto\" , ) elif opts [ \"upper\" ][ n ] == \"contour\" : if opts [ \"contour_offdiag\" ][ \"percentile\" ]: Z = probs2contours ( Z , opts [ \"contour_offdiag\" ][ \"levels\" ] ) else : Z = ( Z - Z . min ()) / ( Z . max () - Z . min ()) h = plt . contour ( X , Y , Z , origin = \"lower\" , extent = [ limits [ col ][ 0 ], limits [ col ][ 1 ], limits [ row ][ 0 ], limits [ row ][ 1 ], ], colors = opts [ \"samples_colors\" ][ n ], levels = opts [ \"contour_offdiag\" ][ \"levels\" ], ) else : pass elif opts [ \"upper\" ][ n ] == \"scatter\" : h = plt . scatter ( v [:, col ], v [:, row ], color = opts [ \"samples_colors\" ][ n ], ** opts [ \"scatter_offdiag\" ] ) elif opts [ \"upper\" ][ n ] == \"plot\" : h = plt . plot ( v [:, col ], v [:, row ], color = opts [ \"samples_colors\" ][ n ], ** opts [ \"plot_offdiag\" ] ) else : pass if len ( points ) > 0 : for n , v in enumerate ( points ): h = plt . plot ( v [:, col ], v [:, row ], color = opts [ \"points_colors\" ][ n ], ** opts [ \"points_offdiag\" ] ) if len ( subset ) < dim : for row in range ( len ( subset )): ax = axes [ row , len ( subset ) - 1 ] x0 , x1 = ax . get_xlim () y0 , y1 = ax . get_ylim () text_kwargs = { \"fontsize\" : plt . rcParams [ \"font.size\" ] * 2.0 } ax . text ( x1 + ( x1 - x0 ) / 8.0 , ( y0 + y1 ) / 2.0 , \"...\" , ** text_kwargs ) if row == len ( subset ) - 1 : ax . text ( x1 + ( x1 - x0 ) / 12.0 , y0 - ( y1 - y0 ) / 1.5 , \"...\" , rotation =- 45 , ** text_kwargs ) return fig , axes","title":"API Reference"},{"location":"reference/#api-reference","text":"","title":"API Reference"},{"location":"reference/#inference","text":"","title":"Inference"},{"location":"reference/#sbi.inference.base.infer","text":"Return posterior distribution by running simulation-based inference. This function provides a simple interface to run sbi. Inference is run for a single round and hence the returned posterior p(\\theta|x) p(\\theta|x) can be sampled and evaluated for any x x (i.e. it is amortized). The scope of this function is limited to the most essential features of sbi. For more flexibility (e.g. multi-round inference, different density estimators) please use the flexible interface described here: https://www.mackelab.org/sbi/tutorial/03_flexible_interface/ Parameters: Name Type Description Default simulator Callable A function that takes parameters \\theta \\theta and maps them to simulations, or observations, x , \\mathrm{sim}(\\theta)\\to x \\mathrm{sim}(\\theta)\\to x . Any regular Python callable (i.e. function or class with __call__ method) can be used. required prior A probability distribution that expresses prior knowledge about the parameters, e.g. which ranges are meaningful for them. Any object with .log_prob() and .sample() (for example, a PyTorch distribution) can be used. required method str What inference method to use. Either of SNPE, SNLE or SNRE. required num_simulations int Number of simulation calls. More simulations means a longer runtime, but a better posterior estimate. required num_workers int Number of parallel workers to use for simulations. 1 Returns: Posterior over parameters conditional on observations (amortized). Source code in sbi/inference/base.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 def infer ( simulator : Callable , prior , method : str , num_simulations : int , num_workers : int = 1 ) -> NeuralPosterior : r \"\"\" Return posterior distribution by running simulation-based inference. This function provides a simple interface to run sbi. Inference is run for a single round and hence the returned posterior $p(\\theta|x)$ can be sampled and evaluated for any $x$ (i.e. it is amortized). The scope of this function is limited to the most essential features of sbi. For more flexibility (e.g. multi-round inference, different density estimators) please use the flexible interface described here: https://www.mackelab.org/sbi/tutorial/03_flexible_interface/ Args: simulator: A function that takes parameters $\\theta$ and maps them to simulations, or observations, `x`, $\\mathrm{sim}(\\theta)\\to x$. Any regular Python callable (i.e. function or class with `__call__` method) can be used. prior: A probability distribution that expresses prior knowledge about the parameters, e.g. which ranges are meaningful for them. Any object with `.log_prob()`and `.sample()` (for example, a PyTorch distribution) can be used. method: What inference method to use. Either of SNPE, SNLE or SNRE. num_simulations: Number of simulation calls. More simulations means a longer runtime, but a better posterior estimate. num_workers: Number of parallel workers to use for simulations. Returns: Posterior over parameters conditional on observations (amortized). \"\"\" try : method_fun : Callable = getattr ( sbi . inference , method . upper ()) except AttributeError : raise NameError ( \"Method not available. `method` must be one of 'SNPE', 'SNLE', 'SNRE'.\" ) simulator , prior = prepare_for_sbi ( simulator , prior ) infer_ = method_fun ( simulator , prior , num_workers = num_workers ) posterior = infer_ ( num_simulations = num_simulations ) return posterior","title":"infer()"},{"location":"reference/#sbi.user_input.user_input_checks.prepare_for_sbi","text":"Prepare simulator, prior and for usage in sbi. One of the goals is to allow you to use sbi with inputs computed in numpy. Attempts to meet the following requirements by reshaping and type-casting: - the simulator function receives as input and returns a Tensor. - the simulator can simulate batches of parameters and return batches of data. - the prior does not produce batches and samples and evaluates to Tensor. - the output shape is a torch.Size((1,N)) (i.e, has a leading batch dimension 1). If this is not possible, a suitable exception will be raised. Parameters: Name Type Description Default simulator Callable Simulator as provided by the user. required prior Prior as provided by the user. required Returns: Type Description Tuple[Callable, torch.distributions.distribution.Distribution] Tuple (simulator, prior, x_shape) checked and matching the requirements of sbi. Source code in sbi/user_input/user_input_checks.py 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 def prepare_for_sbi ( simulator : Callable , prior ,) -> Tuple [ Callable , Distribution ]: \"\"\"Prepare simulator, prior and for usage in sbi. One of the goals is to allow you to use sbi with inputs computed in numpy. Attempts to meet the following requirements by reshaping and type-casting: - the simulator function receives as input and returns a Tensor. - the simulator can simulate batches of parameters and return batches of data. - the prior does not produce batches and samples and evaluates to Tensor. - the output shape is a `torch.Size((1,N))` (i.e, has a leading batch dimension 1). If this is not possible, a suitable exception will be raised. Args: simulator: Simulator as provided by the user. prior: Prior as provided by the user. Returns: Tuple (simulator, prior, x_shape) checked and matching the requirements of sbi. \"\"\" # Check prior, return PyTorch prior. prior , _ , prior_returns_numpy = process_prior ( prior ) # Check simulator, returns PyTorch simulator able to simulate batches. simulator = process_simulator ( simulator , prior , prior_returns_numpy ) # Consistency check after making ready for sbi. check_sbi_inputs ( simulator , prior ) return simulator , prior","title":"prepare_for_sbi()"},{"location":"reference/#sbi.inference.snpe.snpe_c.SNPE_C","text":"","title":"SNPE_C"},{"location":"reference/#sbi.inference.snle.snle_a.SNLE_A","text":"","title":"SNLE_A"},{"location":"reference/#sbi.inference.snre.snre_a.SNRE_A","text":"","title":"SNRE_A"},{"location":"reference/#sbi.inference.snre.snre_b.SNRE_B","text":"","title":"SNRE_B"},{"location":"reference/#sbi.inference.abc.mcabc.MCABC","text":"","title":"MCABC"},{"location":"reference/#sbi.inference.abc.smcabc.SMCABC","text":"","title":"SMCABC"},{"location":"reference/#posteriors","text":"","title":"Posteriors"},{"location":"reference/#sbi.inference.posteriors.direct_posterior.DirectPosterior","text":"Posterior p(\\theta|x) p(\\theta|x) with log_prob() and sample() methods, obtained with SNPE. SNPE trains a neural network to directly approximate the posterior distribution. However, for bounded priors, the neural network can have leakage: it puts non-zero mass in regions where the prior is zero. The SnpePosterior class wraps the trained network to deal with these cases. Specifically, this class offers the following functionality: - correct the calculation of the log probability such that it compensates for the leakage. - reject samples that lie outside of the prior bounds. - alternatively, if leakage is very high (which can happen for multi-round SNPE), sample from the posterior with MCMC. The neural network itself can be accessed via the .net attribute.","title":"DirectPosterior"},{"location":"reference/#sbi.inference.posteriors.likelihood_based_posterior.LikelihoodBasedPosterior","text":"Posterior p(\\theta|x) p(\\theta|x) with log_prob() and sample() methods, obtained with SNLE. SNLE trains a neural network to approximate the likelihood p(x|\\theta) p(x|\\theta) . The SNLE_Posterior class wraps the trained network such that one can directly evaluate the unnormalized posterior log probability p(\\theta|x) \\propto p(x|\\theta) \\cdot p(\\theta) p(\\theta|x) \\propto p(x|\\theta) \\cdot p(\\theta) and draw samples from the posterior with MCMC. The neural network itself can be accessed via the .net attribute.","title":"LikelihoodBasedPosterior"},{"location":"reference/#sbi.inference.posteriors.ratio_based_posterior.RatioBasedPosterior","text":"Posterior p(\\theta|x) p(\\theta|x) with log_prob() and sample() methods, obtained with SNRE. SNRE trains a neural network to approximate likelihood ratios, which in turn can be used obtain an unnormalized posterior p(\\theta|x) \\propto p(x|\\theta) \\cdot p(\\theta) p(\\theta|x) \\propto p(x|\\theta) \\cdot p(\\theta) . The SNRE_Posterior class wraps the trained network such that one can directly evaluate the unnormalized posterior log-probability p(\\theta|x) \\propto p(x|\\theta) \\cdot p(\\theta) p(\\theta|x) \\propto p(x|\\theta) \\cdot p(\\theta) and draw samples from the posterior with MCMC. Note that, in the case of single-round SNRE_A / AALR, it is possible to evaluate the log-probability of the normalized posterior, but sampling still requires MCMC. The neural network itself can be accessed via the .net attribute.","title":"RatioBasedPosterior"},{"location":"reference/#models","text":"","title":"Models"},{"location":"reference/#sbi.utils.get_nn_models.posterior_nn","text":"Source code in sbi/utils/get_nn_models.py 48 49 50 51 52 53 54 55 56 57 58 59 60 61 def posterior_nn ( model : str , ** kwargs : Any ) -> Callable : def build_fn ( batch_theta , batch_x ): if model == \"mdn\" : return build_mdn ( batch_x = batch_theta , batch_y = batch_x , ** kwargs ) if model == \"made\" : return build_made ( batch_x = batch_theta , batch_y = batch_x , ** kwargs ) if model == \"maf\" : return build_maf ( batch_x = batch_theta , batch_y = batch_x , ** kwargs ) elif model == \"nsf\" : return build_nsf ( batch_x = batch_theta , batch_y = batch_x , ** kwargs ) else : raise NotImplementedError return build_fn","title":"posterior_nn()"},{"location":"reference/#sbi.utils.get_nn_models.likelihood_nn","text":"Source code in sbi/utils/get_nn_models.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 def likelihood_nn ( model : str , ** kwargs : Any ) -> Callable : def build_fn ( batch_theta , batch_x ): if model == \"mdn\" : return build_mdn ( batch_x = batch_x , batch_y = batch_theta , ** kwargs ) if model == \"made\" : return build_made ( batch_x = batch_x , batch_y = batch_theta , ** kwargs ) if model == \"maf\" : return build_maf ( batch_x = batch_x , batch_y = batch_theta , ** kwargs ) elif model == \"nsf\" : return build_nsf ( batch_x = batch_x , batch_y = batch_theta , ** kwargs ) else : raise NotImplementedError return build_fn","title":"likelihood_nn()"},{"location":"reference/#sbi.utils.get_nn_models.classifier_nn","text":"Source code in sbi/utils/get_nn_models.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 def classifier_nn ( model : str , ** kwargs : Any ) -> Callable : def build_fn ( batch_theta , batch_x ): if model == \"linear\" : return build_linear_classifier ( batch_x = batch_x , batch_y = batch_theta , ** kwargs ) if model == \"mlp\" : return build_mlp_classifier ( batch_x = batch_x , batch_y = batch_theta , ** kwargs ) if model == \"resnet\" : return build_resnet_classifier ( batch_x = batch_x , batch_y = batch_theta , ** kwargs ) raise NotImplementedError return build_fn","title":"classifier_nn()"},{"location":"reference/#utils","text":"","title":"Utils"},{"location":"reference/#sbi.utils.plot.pairplot","text":"Plot samples and points. For developers: if you add arguments that expect dictionaries, make sure to access them via the opts dictionary instantiated below. E.g. if you want to access the dict stored in the input variable hist_diag, use opts[ hist_diag ]. Parameters: Name Type Description Default samples Union[List[numpy.ndarray], List[torch.Tensor], numpy.ndarray, torch.Tensor] posterior samples used to build the histogram required points Optional[Union[List[numpy.ndarray], List[torch.Tensor], numpy.ndarray, torch.Tensor]] list of additional points to scatter None upper Optional[str] plotting style for upper diagonal, {hist, scatter, contour, None} 'hist' diag Optional[str] plotting style for diagonal, {hist, None} 'hist' title Optional[str] title string None legend Optional[bool] whether to plot a legend for the points False labels np.ndarray of strings specifying the names of the parameters None labels_points np.ndarray of strings specifying the names of the passed points None labels_samples np.ndarray of strings specifying the names of the passed samples None samples_colors colors of the samples ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf'] points_colors colors of the points ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf'] subset List containing the dimensions to plot. E.g. subset=[1,3] will plot plot only the 1 st and 3 rd dimension but will discard the 0 th and 2 nd (and, if they exist, the 4 th , 5 th and so on) None limits array containing the plot xlim for each parameter dimension. If None, just use the min and max of the passed samples None ticks location of the ticks for each parameter. If None, just use the min and max along each parameter dimension None tickformatter passed to _format_axis() <matplotlib.ticker.FormatStrFormatter object at 0x7fe482235e20> tick_labels np.ndarray containing the ticklabels. None hist_diag dictionary passed to plt.hist() for diagonal plots {'alpha': 1.0, 'bins': 50, 'density': False, 'histtype': 'step'} hist_offdiag dictionary passed to np.histogram2d() for off diagonal plots {'bins': 50} kde_diag dictionary passed to gaussian_kde() for diagonal plots {'bw_method': 'scott', 'bins': 50, 'color': 'black'} kde_offdiag dictionary passed to gaussian_kde() for off diagonal plots {'bw_method': 'scott', 'bins': 50} contour_offdiag dictionary that should contain percentile and levels keys. percentile : bool. If percentile ==True, the levels are made with respect to the max probability of the posterior If percentile ==False, the levels are drawn at absolute positions levels : list or np.ndarray: specifies the location where the contours are drawn. {'levels': [0.68], 'percentile': True} scatter_offdiag dictionary for plt.scatter() on off diagonal {'alpha': 0.5, 'edgecolor': 'none', 'rasterized': False} plot_offdiag dictionary for plt.plot() on off diagonal {} points_diag dictionary for plt.plot() used for plotting points on diagonal {} points_offdiag dictionary for plt.plot() used for plotting points on off diagonal {'marker': '.', 'markersize': 20} fig_size Tuple size of the entire figure (10, 10) fig_bg_colors Dictionary that contains upper , diag , lower , and specifies the respective background colors. Passed to ax.set_facecolor() {'upper': None, 'diag': None, 'lower': None} fig_subplots_adjust dictionary passed to fig.subplots_adjust() {'top': 0.9} subplots dictionary passed to plt.subplots() {} despine dictionary passed to set_position() for axis position {'offset': 5} title_format dictionary passed to plt.title() {'fontsize': 16} Returns: figure and axis of posterior distribution plot Source code in sbi/utils/plot.py 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 def pairplot ( samples : Union [ List [ np . ndarray ], List [ torch . Tensor ], np . ndarray , torch . Tensor ], points : Optional [ Union [ List [ np . ndarray ], List [ torch . Tensor ], np . ndarray , torch . Tensor ] ] = None , upper : Optional [ str ] = \"hist\" , diag : Optional [ str ] = \"hist\" , title : Optional [ str ] = None , legend : Optional [ bool ] = False , labels = None , labels_points = None , labels_samples = None , samples_colors = plt . rcParams [ \"axes.prop_cycle\" ] . by_key ()[ \"color\" ], points_colors = plt . rcParams [ \"axes.prop_cycle\" ] . by_key ()[ \"color\" ], subset = None , limits = None , ticks = None , tickformatter = mpl . ticker . FormatStrFormatter ( \" %g \" ), tick_labels = None , hist_diag = { \"alpha\" : 1.0 , \"bins\" : 50 , \"density\" : False , \"histtype\" : \"step\" }, hist_offdiag = { \"bins\" : 50 ,}, kde_diag = { \"bw_method\" : \"scott\" , \"bins\" : 50 , \"color\" : \"black\" }, kde_offdiag = { \"bw_method\" : \"scott\" , \"bins\" : 50 }, contour_offdiag = { \"levels\" : [ 0.68 ], \"percentile\" : True }, scatter_offdiag = { \"alpha\" : 0.5 , \"edgecolor\" : \"none\" , \"rasterized\" : False }, plot_offdiag = {}, points_diag = {}, points_offdiag = { \"marker\" : \".\" , \"markersize\" : 20 }, fig_size : Tuple = ( 10 , 10 ), fig_bg_colors = { \"upper\" : None , \"diag\" : None , \"lower\" : None }, fig_subplots_adjust = { \"top\" : 0.9 }, subplots = {}, despine = { \"offset\" : 5 }, title_format = { \"fontsize\" : 16 }, ): \"\"\" Plot samples and points. For developers: if you add arguments that expect dictionaries, make sure to access them via the opts dictionary instantiated below. E.g. if you want to access the dict stored in the input variable hist_diag, use opts[`hist_diag`]. Args: samples: posterior samples used to build the histogram points: list of additional points to scatter upper: plotting style for upper diagonal, {hist, scatter, contour, None} diag: plotting style for diagonal, {hist, None} title: title string legend: whether to plot a legend for the points labels: np.ndarray of strings specifying the names of the parameters labels_points: np.ndarray of strings specifying the names of the passed points labels_samples: np.ndarray of strings specifying the names of the passed samples samples_colors: colors of the samples points_colors: colors of the points subset: List containing the dimensions to plot. E.g. subset=[1,3] will plot plot only the 1st and 3rd dimension but will discard the 0th and 2nd (and, if they exist, the 4th, 5th and so on) limits: array containing the plot xlim for each parameter dimension. If None, just use the min and max of the passed samples ticks: location of the ticks for each parameter. If None, just use the min and max along each parameter dimension tickformatter: passed to _format_axis() tick_labels: np.ndarray containing the ticklabels. hist_diag: dictionary passed to plt.hist() for diagonal plots hist_offdiag: dictionary passed to np.histogram2d() for off diagonal plots kde_diag: dictionary passed to gaussian_kde() for diagonal plots kde_offdiag: dictionary passed to gaussian_kde() for off diagonal plots contour_offdiag: dictionary that should contain `percentile` and `levels` keys. `percentile`: bool. If `percentile`==True, the levels are made with respect to the max probability of the posterior If `percentile`==False, the levels are drawn at absolute positions `levels`: list or np.ndarray: specifies the location where the contours are drawn. scatter_offdiag: dictionary for plt.scatter() on off diagonal plot_offdiag: dictionary for plt.plot() on off diagonal points_diag: dictionary for plt.plot() used for plotting points on diagonal points_offdiag: dictionary for plt.plot() used for plotting points on off diagonal fig_size: size of the entire figure fig_bg_colors: Dictionary that contains `upper`, `diag`, `lower`, and specifies the respective background colors. Passed to ax.set_facecolor() fig_subplots_adjust: dictionary passed to fig.subplots_adjust() subplots: dictionary passed to plt.subplots() despine: dictionary passed to set_position() for axis position title_format: dictionary passed to plt.title() Returns: figure and axis of posterior distribution plot \"\"\" # TODO: add color map support # TODO: automatically determine good bin sizes for histograms # TODO: add legend (if legend is True) # get default values of function arguments # https://stackoverflow.com/questions/12627118/get-a-function-arguments-default-value spec = inspect . getfullargspec ( pairplot ) # build a dict for the defaults # https://stackoverflow.com/questions/12627118/get-a-function-arguments-default-value # answer by gnr default_val_dict = dict ( zip ( spec . args [:: - 1 ], ( spec . defaults or ())[:: - 1 ])) # update the defaults dictionary by the current values of the variables (passed by # the user) opts = _update ( default_val_dict , locals ()) # Prepare samples if type ( samples ) != list : samples = ensure_numpy ( samples ) samples = [ samples ] else : for i , sample_pack in enumerate ( samples ): samples [ i ] = ensure_numpy ( samples [ i ]) # Prepare points if points is None : points = [] if type ( points ) != list : points = ensure_numpy ( points ) points = [ points ] points = [ np . atleast_2d ( p ) for p in points ] points = [ np . atleast_2d ( ensure_numpy ( p )) for p in points ] # Dimensions dim = samples [ 0 ] . shape [ 1 ] num_samples = samples [ 0 ] . shape [ 0 ] # TODO: add asserts checking compatibility of dimensions # Prepare labels if opts [ \"labels\" ] == [] or opts [ \"labels\" ] is None : labels_dim = [ \"dim {} \" . format ( i + 1 ) for i in range ( dim )] else : labels_dim = opts [ \"labels\" ] # Prepare limits if opts [ \"limits\" ] == [] or opts [ \"limits\" ] is None : limits = [] for d in range ( dim ): min = + np . inf max = - np . inf for sample in samples : min_ = sample [:, d ] . min () min = min_ if min_ < min else min max_ = sample [:, d ] . max () max = max_ if max_ > max else max limits . append ([ min , max ]) else : if len ( opts [ \"limits\" ]) == 1 : limits = [ opts [ \"limits\" ][ 0 ] for _ in range ( dim )] else : limits = opts [ \"limits\" ] # Prepare ticks if opts [ \"ticks\" ] == [] or opts [ \"ticks\" ] is None : ticks = None else : if len ( opts [ \"ticks\" ]) == 1 : ticks = [ opts [ \"ticks\" ][ 0 ] for _ in range ( dim )] else : ticks = opts [ \"ticks\" ] # Prepare diag/upper/lower if type ( opts [ \"diag\" ]) is not list : opts [ \"diag\" ] = [ opts [ \"diag\" ] for _ in range ( len ( samples ))] if type ( opts [ \"upper\" ]) is not list : opts [ \"upper\" ] = [ opts [ \"upper\" ] for _ in range ( len ( samples ))] # if type(opts['lower']) is not list: # opts['lower'] = [opts['lower'] for _ in range(len(samples))] opts [ \"lower\" ] = None # Figure out if we subset the plot subset = opts [ \"subset\" ] if subset is None : rows = cols = dim subset = [ i for i in range ( dim )] else : if type ( subset ) == int : subset = [ subset ] elif type ( subset ) == list : pass else : raise NotImplementedError rows = cols = len ( subset ) fig , axes = plt . subplots ( rows , cols , figsize = opts [ \"fig_size\" ], ** opts [ \"subplots\" ]) # Cast to ndarray in case of 1D subplots. axes = np . array ( axes ) . reshape ( rows , cols ) # Style figure fig . subplots_adjust ( ** opts [ \"fig_subplots_adjust\" ]) fig . suptitle ( opts [ \"title\" ], ** opts [ \"title_format\" ]) # Style axes row_idx = - 1 for row in range ( dim ): if row not in subset : continue else : row_idx += 1 col_idx = - 1 for col in range ( dim ): if col not in subset : continue else : col_idx += 1 if row == col : current = \"diag\" elif row < col : current = \"upper\" else : current = \"lower\" ax = axes [ row_idx , col_idx ] plt . sca ( ax ) # Background color if ( current in opts [ \"fig_bg_colors\" ] and opts [ \"fig_bg_colors\" ][ current ] is not None ): ax . set_facecolor ( opts [ \"fig_bg_colors\" ][ current ]) # Axes if opts [ current ] is None : ax . axis ( \"off\" ) continue # Limits if limits is not None : ax . set_xlim (( limits [ col ][ 0 ], limits [ col ][ 1 ])) if current != \"diag\" : ax . set_ylim (( limits [ row ][ 0 ], limits [ row ][ 1 ])) xmin , xmax = ax . get_xlim () ymin , ymax = ax . get_ylim () # Ticks if ticks is not None : ax . set_xticks (( ticks [ col ][ 0 ], ticks [ col ][ 1 ])) if current != \"diag\" : ax . set_yticks (( ticks [ row ][ 0 ], ticks [ row ][ 1 ])) # Despine ax . spines [ \"right\" ] . set_visible ( False ) ax . spines [ \"top\" ] . set_visible ( False ) ax . spines [ \"bottom\" ] . set_position (( \"outward\" , despine [ \"offset\" ])) # Formatting axes if current == \"diag\" : # off-diagnoals if opts [ \"lower\" ] is None or col == dim - 1 : _format_axis ( ax , xhide = False , xlabel = labels_dim [ col ], yhide = True , tickformatter = opts [ \"tickformatter\" ], ) else : _format_axis ( ax , xhide = True , yhide = True ) else : # off-diagnoals if row == dim - 1 : _format_axis ( ax , xhide = False , xlabel = labels_dim [ col ], yhide = True , tickformatter = opts [ \"tickformatter\" ], ) else : _format_axis ( ax , xhide = True , yhide = True ) if opts [ \"tick_labels\" ] is not None : ax . set_xticklabels ( ( str ( opts [ \"tick_labels\" ][ col ][ 0 ]), str ( opts [ \"tick_labels\" ][ col ][ 1 ]), ) ) # Diagonals if current == \"diag\" : if len ( samples ) > 0 : for n , v in enumerate ( samples ): if opts [ \"diag\" ][ n ] == \"hist\" : h = plt . hist ( v [:, row ], color = opts [ \"samples_colors\" ][ n ], ** opts [ \"hist_diag\" ] ) elif opts [ \"diag\" ][ n ] == \"kde\" : density = gaussian_kde ( v [:, row ], bw_method = opts [ \"kde_diag\" ][ \"bw_method\" ] ) xs = np . linspace ( xmin , xmax , opts [ \"kde_diag\" ][ \"bins\" ]) ys = density ( xs ) h = plt . plot ( xs , ys , color = opts [ \"samples_colors\" ][ n ],) else : pass if len ( points ) > 0 : extent = ax . get_ylim () for n , v in enumerate ( points ): h = plt . plot ( [ v [:, row ], v [:, row ]], extent , color = opts [ \"points_colors\" ][ n ], ** opts [ \"points_diag\" ] ) # Off-diagonals else : if len ( samples ) > 0 : for n , v in enumerate ( samples ): if opts [ \"upper\" ][ n ] == \"hist\" or opts [ \"upper\" ][ n ] == \"hist2d\" : # h = plt.hist2d( # v[:, col], v[:, row], # range=( # [limits[col][0], limits[col][1]], # [limits[row][0], limits[row][1]]), # **opts['hist_offdiag'] # ) hist , xedges , yedges = np . histogram2d ( v [:, col ], v [:, row ], range = [ [ limits [ col ][ 0 ], limits [ col ][ 1 ]], [ limits [ row ][ 0 ], limits [ row ][ 1 ]], ], ** opts [ \"hist_offdiag\" ] ) h = plt . imshow ( hist . T , origin = \"lower\" , extent = [ xedges [ 0 ], xedges [ - 1 ], yedges [ 0 ], yedges [ - 1 ],], aspect = \"auto\" , ) elif opts [ \"upper\" ][ n ] in [ \"kde\" , \"kde2d\" , \"contour\" , \"contourf\" , ]: density = gaussian_kde ( v [:, [ col , row ]] . T , bw_method = opts [ \"kde_offdiag\" ][ \"bw_method\" ], ) X , Y = np . meshgrid ( np . linspace ( limits [ col ][ 0 ], limits [ col ][ 1 ], opts [ \"kde_offdiag\" ][ \"bins\" ], ), np . linspace ( limits [ row ][ 0 ], limits [ row ][ 1 ], opts [ \"kde_offdiag\" ][ \"bins\" ], ), ) positions = np . vstack ([ X . ravel (), Y . ravel ()]) Z = np . reshape ( density ( positions ) . T , X . shape ) if opts [ \"upper\" ][ n ] == \"kde\" or opts [ \"upper\" ][ n ] == \"kde2d\" : h = plt . imshow ( Z , extent = [ limits [ col ][ 0 ], limits [ col ][ 1 ], limits [ row ][ 0 ], limits [ row ][ 1 ], ], origin = \"lower\" , aspect = \"auto\" , ) elif opts [ \"upper\" ][ n ] == \"contour\" : if opts [ \"contour_offdiag\" ][ \"percentile\" ]: Z = probs2contours ( Z , opts [ \"contour_offdiag\" ][ \"levels\" ] ) else : Z = ( Z - Z . min ()) / ( Z . max () - Z . min ()) h = plt . contour ( X , Y , Z , origin = \"lower\" , extent = [ limits [ col ][ 0 ], limits [ col ][ 1 ], limits [ row ][ 0 ], limits [ row ][ 1 ], ], colors = opts [ \"samples_colors\" ][ n ], levels = opts [ \"contour_offdiag\" ][ \"levels\" ], ) else : pass elif opts [ \"upper\" ][ n ] == \"scatter\" : h = plt . scatter ( v [:, col ], v [:, row ], color = opts [ \"samples_colors\" ][ n ], ** opts [ \"scatter_offdiag\" ] ) elif opts [ \"upper\" ][ n ] == \"plot\" : h = plt . plot ( v [:, col ], v [:, row ], color = opts [ \"samples_colors\" ][ n ], ** opts [ \"plot_offdiag\" ] ) else : pass if len ( points ) > 0 : for n , v in enumerate ( points ): h = plt . plot ( v [:, col ], v [:, row ], color = opts [ \"points_colors\" ][ n ], ** opts [ \"points_offdiag\" ] ) if len ( subset ) < dim : for row in range ( len ( subset )): ax = axes [ row , len ( subset ) - 1 ] x0 , x1 = ax . get_xlim () y0 , y1 = ax . get_ylim () text_kwargs = { \"fontsize\" : plt . rcParams [ \"font.size\" ] * 2.0 } ax . text ( x1 + ( x1 - x0 ) / 8.0 , ( y0 + y1 ) / 2.0 , \"...\" , ** text_kwargs ) if row == len ( subset ) - 1 : ax . text ( x1 + ( x1 - x0 ) / 12.0 , y0 - ( y1 - y0 ) / 1.5 , \"...\" , rotation =- 45 , ** text_kwargs ) return fig , axes","title":"pairplot()"},{"location":"tutorial/00_getting_started/","text":"Getting started with sbi \u00b6 import torch import sbi.utils as utils from sbi.inference.base import infer Running the inference procedure \u00b6 sbi provides a simple interface to run state-of-the-art algorithms for simulation-based inference. For inference, you need to provide two ingredients: 1) a prior distribution that allows to sample parameter sets. 2) a simulator that takes parameter sets and produces simulation outputs. For example, we can have a 3-dimensional parameter space with a uniform prior between [-1,1] and a simple simulator that for the sake of example adds 1.0 and some Gaussian noise to the parameter set: num_dim = 3 prior = utils . BoxUniform ( low =- 2 * torch . ones ( num_dim ), high = 2 * torch . ones ( num_dim )) def simulator ( parameter_set ): return 1.0 + parameter_set + torch . randn ( parameter_set . shape ) * 0.1 sbi can then run inference: posterior = infer ( simulator , prior , method = 'SNPE' , num_simulations = 1000 ) HBox(children=(FloatProgress(value=0.0, description='Running 1000 simulations.', max=1000.0, style=ProgressSty\u2026 Neural network successfully converged after 73 epochs. Let\u2019s say we have made some observation \\(x\\) : observation = torch . zeros ( 3 ) Given this observation, we can then sample from the posterior \\(p(\\theta|x)\\) , evaluate its log-probability, or plot it. samples = posterior . sample (( 10000 ,), x = observation ) log_probability = posterior . log_prob ( samples , x = observation ) _ = utils . pairplot ( samples , limits = [[ - 2 , 2 ],[ - 2 , 2 ],[ - 2 , 2 ]], fig_size = ( 6 , 6 )) HBox(children=(FloatProgress(value=0.0, description='Drawing 10000 posterior samples', max=10000.0, style=Prog\u2026 Requirements for the simulator, prior, and observation \u00b6 Regardless of the algorithm you need to provide a prior and a simulator for training. Let\u2019s talk about what requirements they need to satisfy. Prior \u00b6 A prior is a distribution object that allows to sample parameter sets. Any class for the prior is allowed as long as it allows to call prior.sample() and prior.log_prob() . Simulator \u00b6 The simulator is a Python callable that takes in a parameter set and outputs data with some (even if very small) stochasticity. Allowed data types and shapes for input and output: the input parameter set and the output have to be either a np.ndarray or a torch.Tensor . the input parameter set should have either shape (1,N) or (N) , and the output must have shape (1,M) or (M) . You can call simulators not written in Python as long as you wrap them in a Python function. Observation \u00b6 Once you have a trained posterior, you will want to evaluate or sample the posterior \\(p(\\theta|x_o)\\) at certain observed values \\(x_o\\) : The allowable data types are either Numpy np.ndarray or a torch torch.Tensor . The shape must be either (1,M) or just (M) . Running different algorithms \u00b6 sbi implements three classes of algorithms that can be used to obtain the posterior distribution: SNPE, SNLE, and SNRE. You can try the different algorithms by simply swapping out the method : posterior = infer ( simulator , prior , method = 'SNPE' , num_simulations = 1000 ) posterior = infer ( simulator , prior , method = 'SNLE' , num_simulations = 1000 ) posterior = infer ( simulator , prior , method = 'SNRE' , num_simulations = 1000 ) HBox(children=(FloatProgress(value=0.0, description='Running 1000 simulations.', max=1000.0, style=ProgressSty\u2026 Neural network successfully converged after 99 epochs. HBox(children=(FloatProgress(value=0.0, description='Running 1000 simulations.', max=1000.0, style=ProgressSty\u2026 Neural network successfully converged after 104 epochs. HBox(children=(FloatProgress(value=0.0, description='Running 1000 simulations.', max=1000.0, style=ProgressSty\u2026 Neural network successfully converged after 66 epochs. You can then infer, sample, evaluate, and plot the posterior as described above.","title":"Getting started"},{"location":"tutorial/00_getting_started/#getting-started-with-sbi","text":"import torch import sbi.utils as utils from sbi.inference.base import infer","title":"Getting started with sbi"},{"location":"tutorial/00_getting_started/#running-the-inference-procedure","text":"sbi provides a simple interface to run state-of-the-art algorithms for simulation-based inference. For inference, you need to provide two ingredients: 1) a prior distribution that allows to sample parameter sets. 2) a simulator that takes parameter sets and produces simulation outputs. For example, we can have a 3-dimensional parameter space with a uniform prior between [-1,1] and a simple simulator that for the sake of example adds 1.0 and some Gaussian noise to the parameter set: num_dim = 3 prior = utils . BoxUniform ( low =- 2 * torch . ones ( num_dim ), high = 2 * torch . ones ( num_dim )) def simulator ( parameter_set ): return 1.0 + parameter_set + torch . randn ( parameter_set . shape ) * 0.1 sbi can then run inference: posterior = infer ( simulator , prior , method = 'SNPE' , num_simulations = 1000 ) HBox(children=(FloatProgress(value=0.0, description='Running 1000 simulations.', max=1000.0, style=ProgressSty\u2026 Neural network successfully converged after 73 epochs. Let\u2019s say we have made some observation \\(x\\) : observation = torch . zeros ( 3 ) Given this observation, we can then sample from the posterior \\(p(\\theta|x)\\) , evaluate its log-probability, or plot it. samples = posterior . sample (( 10000 ,), x = observation ) log_probability = posterior . log_prob ( samples , x = observation ) _ = utils . pairplot ( samples , limits = [[ - 2 , 2 ],[ - 2 , 2 ],[ - 2 , 2 ]], fig_size = ( 6 , 6 )) HBox(children=(FloatProgress(value=0.0, description='Drawing 10000 posterior samples', max=10000.0, style=Prog\u2026","title":"Running the inference procedure"},{"location":"tutorial/00_getting_started/#requirements-for-the-simulator-prior-and-observation","text":"Regardless of the algorithm you need to provide a prior and a simulator for training. Let\u2019s talk about what requirements they need to satisfy.","title":"Requirements for the simulator, prior, and observation"},{"location":"tutorial/00_getting_started/#prior","text":"A prior is a distribution object that allows to sample parameter sets. Any class for the prior is allowed as long as it allows to call prior.sample() and prior.log_prob() .","title":"Prior"},{"location":"tutorial/00_getting_started/#simulator","text":"The simulator is a Python callable that takes in a parameter set and outputs data with some (even if very small) stochasticity. Allowed data types and shapes for input and output: the input parameter set and the output have to be either a np.ndarray or a torch.Tensor . the input parameter set should have either shape (1,N) or (N) , and the output must have shape (1,M) or (M) . You can call simulators not written in Python as long as you wrap them in a Python function.","title":"Simulator"},{"location":"tutorial/00_getting_started/#observation","text":"Once you have a trained posterior, you will want to evaluate or sample the posterior \\(p(\\theta|x_o)\\) at certain observed values \\(x_o\\) : The allowable data types are either Numpy np.ndarray or a torch torch.Tensor . The shape must be either (1,M) or just (M) .","title":"Observation"},{"location":"tutorial/00_getting_started/#running-different-algorithms","text":"sbi implements three classes of algorithms that can be used to obtain the posterior distribution: SNPE, SNLE, and SNRE. You can try the different algorithms by simply swapping out the method : posterior = infer ( simulator , prior , method = 'SNPE' , num_simulations = 1000 ) posterior = infer ( simulator , prior , method = 'SNLE' , num_simulations = 1000 ) posterior = infer ( simulator , prior , method = 'SNRE' , num_simulations = 1000 ) HBox(children=(FloatProgress(value=0.0, description='Running 1000 simulations.', max=1000.0, style=ProgressSty\u2026 Neural network successfully converged after 99 epochs. HBox(children=(FloatProgress(value=0.0, description='Running 1000 simulations.', max=1000.0, style=ProgressSty\u2026 Neural network successfully converged after 104 epochs. HBox(children=(FloatProgress(value=0.0, description='Running 1000 simulations.', max=1000.0, style=ProgressSty\u2026 Neural network successfully converged after 66 epochs. You can then infer, sample, evaluate, and plot the posterior as described above.","title":"Running different algorithms"},{"location":"tutorial/01_gaussian_amortized/","text":"Amortized posterior inference on Gaussian example \u00b6 In this tutorial, we will demonstrate how sbi can infer an amortized posterior for a simple toy model with a uniform prior and Gaussian likelihood. import torch import numpy as np import sbi.utils as utils from sbi.inference.base import infer Defining prior, simulator, and running inference \u00b6 Say we have 3-dimensional parameter space, and the prior is uniformly distributed between -2 and 2 in each dimension, i.e. \\(x\\in [-2,2], y\\in [-2,2], z \\in [-2,2]\\) . num_dim = 3 prior = utils . BoxUniform ( low =- 2 * torch . ones ( num_dim ), high = 2 * torch . ones ( num_dim )) Our simulator takes the input parameters, adds 1.0 in each dimension, and then adds some Gaussian noise: def linear_gaussian ( theta ): return theta + 1.0 + torch . randn_like ( theta ) * 0.1 We can then run inference: posterior = infer ( linear_gaussian , prior , 'SNPE' , num_simulations = 1000 ) HBox(children=(FloatProgress(value=0.0, description='Running 1000 simulations.', max=1000.0, style=ProgressSty\u2026 Neural network successfully converged after 187 epochs. Amortized inference \u00b6 Note that we have not yet provided an observation to the inference procedure. In fact, we can evaluate the posterior for different observations without having to re-run inference. This is called amortization. An amortized posterior is one that is not focused on any particular observation. Naturally, if the diversity of observations is large, any of the inference methods will need to run a sufficient number of simulations for the resulting posterior to perform well across these diverse observations. Let\u2019s say we have two observations x_o_1 = [0,0,0] and x_o_2 = [2,2,2] : x_o_1 = torch . zeros ( 3 ,) x_o_2 = 2.0 * torch . ones ( 3 ,) We can draw samples from the posterior given x_o_1 and then plot them: posterior_samples_1 = posterior . sample (( 10000 ,), x = x_o_1 ) # plot posterior samples _ = utils . pairplot ( posterior_samples_1 , limits = [[ - 2 , 2 ],[ - 2 , 2 ],[ - 2 , 2 ]], fig_size = ( 5 , 5 )) HBox(children=(FloatProgress(value=0.0, description='Drawing 10000 posterior samples', max=10000.0, style=Prog\u2026 As it can be seen, the posterior samples are centered around [-1,-1,-1] in each dimension. This makes sense because the simulator always adds 1.0 in each dimension and we have observed x_o_1 = [0,0,0] . Since the learned posterior is amortized, we can also draw samples from the posterior given the second observation without having to re-run inference: posterior_samples_2 = posterior . sample (( 10000 ,), x = x_o_2 ) # plot posterior samples _ = utils . pairplot ( posterior_samples_2 , limits = [[ - 2 , 2 ],[ - 2 , 2 ],[ - 2 , 2 ]], fig_size = ( 5 , 5 )) HBox(children=(FloatProgress(value=0.0, description='Drawing 10000 posterior samples', max=10000.0, style=Prog\u2026 So, if we have observed x_o_2 = [2,2,2] , the posterior is centered around [1,1,1] \u2013 again, this makes sense because the simulator adds 1.0 in each dimension.","title":"Amortized inference"},{"location":"tutorial/01_gaussian_amortized/#amortized-posterior-inference-on-gaussian-example","text":"In this tutorial, we will demonstrate how sbi can infer an amortized posterior for a simple toy model with a uniform prior and Gaussian likelihood. import torch import numpy as np import sbi.utils as utils from sbi.inference.base import infer","title":"Amortized posterior inference on Gaussian example"},{"location":"tutorial/01_gaussian_amortized/#defining-prior-simulator-and-running-inference","text":"Say we have 3-dimensional parameter space, and the prior is uniformly distributed between -2 and 2 in each dimension, i.e. \\(x\\in [-2,2], y\\in [-2,2], z \\in [-2,2]\\) . num_dim = 3 prior = utils . BoxUniform ( low =- 2 * torch . ones ( num_dim ), high = 2 * torch . ones ( num_dim )) Our simulator takes the input parameters, adds 1.0 in each dimension, and then adds some Gaussian noise: def linear_gaussian ( theta ): return theta + 1.0 + torch . randn_like ( theta ) * 0.1 We can then run inference: posterior = infer ( linear_gaussian , prior , 'SNPE' , num_simulations = 1000 ) HBox(children=(FloatProgress(value=0.0, description='Running 1000 simulations.', max=1000.0, style=ProgressSty\u2026 Neural network successfully converged after 187 epochs.","title":"Defining prior, simulator, and running inference"},{"location":"tutorial/01_gaussian_amortized/#amortized-inference","text":"Note that we have not yet provided an observation to the inference procedure. In fact, we can evaluate the posterior for different observations without having to re-run inference. This is called amortization. An amortized posterior is one that is not focused on any particular observation. Naturally, if the diversity of observations is large, any of the inference methods will need to run a sufficient number of simulations for the resulting posterior to perform well across these diverse observations. Let\u2019s say we have two observations x_o_1 = [0,0,0] and x_o_2 = [2,2,2] : x_o_1 = torch . zeros ( 3 ,) x_o_2 = 2.0 * torch . ones ( 3 ,) We can draw samples from the posterior given x_o_1 and then plot them: posterior_samples_1 = posterior . sample (( 10000 ,), x = x_o_1 ) # plot posterior samples _ = utils . pairplot ( posterior_samples_1 , limits = [[ - 2 , 2 ],[ - 2 , 2 ],[ - 2 , 2 ]], fig_size = ( 5 , 5 )) HBox(children=(FloatProgress(value=0.0, description='Drawing 10000 posterior samples', max=10000.0, style=Prog\u2026 As it can be seen, the posterior samples are centered around [-1,-1,-1] in each dimension. This makes sense because the simulator always adds 1.0 in each dimension and we have observed x_o_1 = [0,0,0] . Since the learned posterior is amortized, we can also draw samples from the posterior given the second observation without having to re-run inference: posterior_samples_2 = posterior . sample (( 10000 ,), x = x_o_2 ) # plot posterior samples _ = utils . pairplot ( posterior_samples_2 , limits = [[ - 2 , 2 ],[ - 2 , 2 ],[ - 2 , 2 ]], fig_size = ( 5 , 5 )) HBox(children=(FloatProgress(value=0.0, description='Drawing 10000 posterior samples', max=10000.0, style=Prog\u2026 So, if we have observed x_o_2 = [2,2,2] , the posterior is centered around [1,1,1] \u2013 again, this makes sense because the simulator adds 1.0 in each dimension.","title":"Amortized inference"},{"location":"tutorial/02_HH_simulator/","text":"Inference on Hodgkin-Huxley model: tutorial \u00b6 In this tutorial, we use sbi to do inference on a Hodgkin-Huxley model from neuroscience (Hodgkin and Huxley, 1952). We will learn two parameters ( \\(\\bar g_{Na}\\) , \\(\\bar g_K\\) ) based on a current-clamp recording, that we generate synthetically (in practice, this would be an experimental observation). First we are going to import basic packages. import numpy as np import torch # visualization import matplotlib as mpl import matplotlib.pyplot as plt # sbi import sbi.utils as utils from sbi.inference.base import infer # remove top and right axis from plots mpl . rcParams [ 'axes.spines.right' ] = False mpl . rcParams [ 'axes.spines.top' ] = False Different required components \u00b6 Before running inference, let us define the different required components: observed data prior over model parameters simulator 1. Observed data \u00b6 Let us assume we current-clamped a neuron and recorded the following voltage trace: In fact, this voltage trace was not measured experimentally but synthetically generated by simulating a Hodgkin-Huxley model with particular parameters ( \\(\\bar g_{Na}\\) , \\(\\bar g_K\\) ). We will come back to this point later in the tutorial. 2. Simulator \u00b6 We would like to infer the posterior over the two parameters ( \\(\\bar g_{Na}\\) , \\(\\bar g_K\\) ) of a Hodgkin-Huxley model, given the observed electrophysiological recording above. The model has channel kinetics as in Pospischil et al. 2008 , and is defined by the following set of differential equations (parameters of interest highlighted in orange): \\[ \\scriptsize \\begin{align} C_m\\frac{dV}{dt}&=g_1\\left(E_1-V\\right)+ \\color{orange}{\\bar{g}_{Na}}m^3h\\left(E_{Na}-V\\right)+ \\color{orange}{\\bar{g}_{K}}n^4\\left(E_K-V\\right)+ \\bar{g}_Mp\\left(E_K-V\\right)+ I_{inj}+ \\sigma\\eta\\left(t\\right)\\\\ \\frac{dq}{dt}&=\\frac{q_\\infty\\left(V\\right)-q}{\\tau_q\\left(V\\right)},\\;q\\in\\{m,h,n,p\\} \\end{align} \\] Above, \\(V\\) represents the membrane potential, \\(C_m\\) is the membrane capacitance, \\(g_{\\text{l}}\\) is the leak conductance, \\(E_{\\text{l}}\\) is the membrane reversal potential, \\(\\bar{g}_c\\) is the density of channels of type \\(c\\) ( \\(\\text{Na}^+\\) , \\(\\text{K}^+\\) , M), \\(E_c\\) is the reversal potential of \\(c\\) , ( \\(m\\) , \\(h\\) , \\(n\\) , \\(p\\) ) are the respective channel gating kinetic variables, and \\(\\sigma \\eta(t)\\) is the intrinsic neural noise. The right hand side of the voltage dynamics is composed of a leak current, a voltage-dependent \\(\\text{Na}^+\\) current, a delayed-rectifier \\(\\text{K}^+\\) current, a slow voltage-dependent \\(\\text{K}^+\\) current responsible for spike-frequency adaptation, and an injected current \\(I_{\\text{inj}}\\) . Channel gating variables \\(q\\) have dynamics fully characterized by the neuron membrane potential \\(V\\) , given the respective steady-state \\(q_{\\infty}(V)\\) and time constant \\(\\tau_{q}(V)\\) (details in Pospischil et al. 2008). The input current \\(I_{\\text{inj}}\\) is defined as from HH_helper_functions import syn_current I , t_on , t_off , dt , t , A_soma = syn_current () The Hodgkin-Huxley simulator is given by: from HH_helper_functions import HHsimulator Putting the input current and the simulator together: def run_HH_model ( params ): params = np . asarray ( params ) # input current, time step I , t_on , t_off , dt , t , A_soma = syn_current () t = np . arange ( 0 , len ( I ), 1 ) * dt # initial voltage V0 = - 70 states = HHsimulator ( V0 , params . reshape ( 1 , - 1 ), dt , t , I , seed = 0 ) return dict ( data = states . reshape ( - 1 ), time = t , dt = dt , I = I . reshape ( - 1 )) To get an idea of the output of the Hodgkin-Huxley model, let us generate some voltage traces for different parameters ( \\(\\bar g_{Na}\\) , \\(\\bar g_K\\) ), given the input current \\(I_{\\text{inj}}\\) : # three sets of (g_Na, g_K) params = np . array ([[ 50. , 1. ],[ 4. , 1.5 ],[ 20. , 15. ]]) num_samples = len ( params [:, 0 ]) sim_samples = np . zeros (( num_samples , len ( I ))) for i in range ( num_samples ): sim_samples [ i ,:] = run_HH_model ( params = params [ i ,:])[ 'data' ] # colors for traces col_min = 2 num_colors = num_samples + col_min cm1 = mpl . cm . Blues col1 = [ cm1 ( 1. * i / num_colors ) for i in range ( col_min , num_colors )] fig = plt . figure ( figsize = ( 7 , 5 )) gs = mpl . gridspec . GridSpec ( 2 , 1 , height_ratios = [ 4 , 1 ]) ax = plt . subplot ( gs [ 0 ]) for i in range ( num_samples ): plt . plot ( t , sim_samples [ i ,:], color = col1 [ i ], lw = 2 ) plt . ylabel ( 'voltage (mV)' ) ax . set_xticks ([]) ax . set_yticks ([ - 80 , - 20 , 40 ]) ax = plt . subplot ( gs [ 1 ]) plt . plot ( t , I * A_soma * 1e3 , 'k' , lw = 2 ) plt . xlabel ( 'time (ms)' ) plt . ylabel ( 'input (nA)' ) ax . set_xticks ([ 0 , max ( t ) / 2 , max ( t )]) ax . set_yticks ([ 0 , 1.1 * np . max ( I * A_soma * 1e3 )]) ax . yaxis . set_major_formatter ( mpl . ticker . FormatStrFormatter ( ' %.2f ' )) plt . show () As can be seen, the voltage traces can be quite diverse for different parameter values. Often, we are not interested in matching the exact trace, but only in matching certain features thereof. In this example of the Hodgkin-Huxley model, the summary features are the number of spikes, the mean resting potential, the standard deviation of the resting potential, and the first four voltage moments: mean, standard deviation, skewness and kurtosis. Using the function calculate_summary_statistics() imported below, we obtain these statistics from the output of the Hodgkin Huxley simulator. from HH_helper_functions import calculate_summary_statistics Lastly, we define a function that performs all of the above steps at once. The function simulation_wrapper takes in conductance values, runs the Hodgkin Huxley model and then returns the summary statistics. def simulation_wrapper ( params ): \"\"\" Returns summary statistics from conductance values in `params`. Summarizes the output of the HH simulator and converts it to `torch.Tensor`. \"\"\" obs = run_HH_model ( params ) summstats = torch . as_tensor ( calculate_summary_statistics ( obs )) return summstats sbi takes any function as simulator. Thus, sbi also has the flexibility to use simulators that utilize external packages, e.g., Brian ( http://briansimulator.org/ ), nest ( https://www.nest-simulator.org/ ), or NEURON ( https://neuron.yale.edu/neuron/ ). External simulators do not even need to be Python-based as long as they store simulation outputs in a format that can be read from Python. All that is necessary is to wrap your external simulator of choice into a Python callable that takes a parameter set and outputs a set of summary statistics we want to fit the parameters to. 3. Prior over model parameters \u00b6 Now that we have the simulator, we need to define a function with the prior over the model parameters ( \\(\\bar g_{Na}\\) , \\(\\bar g_K\\) ), which in this case is chosen to be a Uniform distribution: prior_min = [ . 5 , 1e-4 ] prior_max = [ 80. , 15. ] prior = utils . torchutils . BoxUniform ( low = torch . as_tensor ( prior_min ), high = torch . as_tensor ( prior_max )) Inference \u00b6 Now that we have all the required components, we can run inference with SNPE to identify parameters whose activity matches this trace. posterior = infer ( simulation_wrapper , prior , method = 'SNPE' , num_simulations = 300 , num_workers = 4 ) HBox(children=(FloatProgress(value=0.0, description='Running 300 simulations in 300 batches.', max=300.0, styl\u2026 /Users/deismic/anaconda3/envs/sbi/lib/python3.8/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning) Neural network successfully converged after 202 epochs. Note sbi can parallelize your simulator. If you experience problems with parallelization, try setting num_workers=1 and please give us an error report as a GitHub issue . Coming back to the observed data \u00b6 As mentioned at the beginning of the tutorial, the observed data are generated by the Hodgkin-Huxley model with a set of known parameters ( \\(\\bar g_{Na}\\) , \\(\\bar g_K\\) ). To illustrate how to compute the summary statistics of the observed data, let us regenerate the observed data: # true parameters and respective labels true_params = np . array ([ 50. , 5. ]) labels_params = [ r '$g_ {Na} $' , r '$g_ {K} $' ] observation_trace = run_HH_model ( true_params ) observation_summary_statistics = calculate_summary_statistics ( observation_trace ) As we already shown above, the observed voltage traces look as follows: fig = plt . figure ( figsize = ( 7 , 5 )) gs = mpl . gridspec . GridSpec ( 2 , 1 , height_ratios = [ 4 , 1 ]) ax = plt . subplot ( gs [ 0 ]) plt . plot ( observation_trace [ 'time' ], observation_trace [ 'data' ]) plt . ylabel ( 'voltage (mV)' ) plt . title ( 'observed data' ) plt . setp ( ax , xticks = [], yticks = [ - 80 , - 20 , 40 ]) ax = plt . subplot ( gs [ 1 ]) plt . plot ( observation_trace [ 'time' ], I * A_soma * 1e3 , 'k' , lw = 2 ) plt . xlabel ( 'time (ms)' ) plt . ylabel ( 'input (nA)' ) ax . set_xticks ([ 0 , max ( observation_trace [ 'time' ]) / 2 , max ( observation_trace [ 'time' ])]) ax . set_yticks ([ 0 , 1.1 * np . max ( I * A_soma * 1e3 )]) ax . yaxis . set_major_formatter ( mpl . ticker . FormatStrFormatter ( ' %.2f ' )) Analysis of the posterior given the observed data \u00b6 After running the inference algorithm, let us inspect the inferred posterior distribution over the parameters ( \\(\\bar g_{Na}\\) , \\(\\bar g_K\\) ), given the observed trace. To do so, we first draw samples (i.e. consistent parameter sets) from the posterior: samples = posterior . sample (( 10000 ,), x = observation_summary_statistics ) HBox(children=(FloatProgress(value=0.0, description='Drawing 10000 posterior samples', max=10000.0, style=Prog\u2026 fig , axes = utils . pairplot ( samples , limits = [[ . 5 , 80 ], [ 1e-4 , 15. ]], ticks = [[ . 5 , 80 ], [ 1e-4 , 15. ]], fig_size = ( 5 , 5 ), points = true_params , points_offdiag = { 'markersize' : 6 }, points_colors = 'r' ); As can be seen, the inferred posterior contains the ground-truth parameters (red) in a high-probability region. Now, let us sample parameters from the posterior distribution, simulate the Hodgkin-Huxley model for this parameter set and compare the simulations with the observed data: # Draw a sample from the posterior and convert to numpy for plotting. posterior_sample = posterior . sample (( 1 ,), x = observation_summary_statistics ) . numpy () HBox(children=(FloatProgress(value=0.0, description='Drawing 1 posterior samples', max=1.0, style=ProgressStyl\u2026 fig = plt . figure ( figsize = ( 7 , 5 )) # plot observation t = observation_trace [ 'time' ] y_obs = observation_trace [ 'data' ] plt . plot ( t , y_obs , lw = 2 , label = 'observation' ) # simulate and plot samples x = run_HH_model ( posterior_sample ) plt . plot ( t , x [ 'data' ], '--' , lw = 2 , label = 'posterior sample' ) plt . xlabel ( 'time (ms)' ) plt . ylabel ( 'voltage (mV)' ) ax = plt . gca () handles , labels = ax . get_legend_handles_labels () ax . legend ( handles [:: - 1 ], labels [:: - 1 ], bbox_to_anchor = ( 1.3 , 1 ), loc = 'upper right' ) ax . set_xticks ([ 0 , 60 , 120 ]) ax . set_yticks ([ - 80 , - 20 , 40 ]); As can be seen, the sample from the inferred posterior leads to simulations that closely resemble the observed data, confirming that SNPE did a good job at capturing the observed data in this simple case. References \u00b6 A. L. Hodgkin and A. F. Huxley. A quantitative description of membrane current and its application to conduction and excitation in nerve. The Journal of Physiology, 117(4):500\u2013544, 1952. M. Pospischil, M. Toledo-Rodriguez, C. Monier, Z. Piwkowska, T. Bal, Y. Fr\u00e9gnac, H. Markram, and A. Destexhe. Minimal Hodgkin-Huxley type models for different classes of cortical and thalamic neurons. Biological Cybernetics, 99(4-5), 2008.","title":"Hodgkin-Huxley example"},{"location":"tutorial/02_HH_simulator/#inference-on-hodgkin-huxley-model-tutorial","text":"In this tutorial, we use sbi to do inference on a Hodgkin-Huxley model from neuroscience (Hodgkin and Huxley, 1952). We will learn two parameters ( \\(\\bar g_{Na}\\) , \\(\\bar g_K\\) ) based on a current-clamp recording, that we generate synthetically (in practice, this would be an experimental observation). First we are going to import basic packages. import numpy as np import torch # visualization import matplotlib as mpl import matplotlib.pyplot as plt # sbi import sbi.utils as utils from sbi.inference.base import infer # remove top and right axis from plots mpl . rcParams [ 'axes.spines.right' ] = False mpl . rcParams [ 'axes.spines.top' ] = False","title":"Inference on Hodgkin-Huxley model: tutorial"},{"location":"tutorial/02_HH_simulator/#different-required-components","text":"Before running inference, let us define the different required components: observed data prior over model parameters simulator","title":"Different required components"},{"location":"tutorial/02_HH_simulator/#1-observed-data","text":"Let us assume we current-clamped a neuron and recorded the following voltage trace: In fact, this voltage trace was not measured experimentally but synthetically generated by simulating a Hodgkin-Huxley model with particular parameters ( \\(\\bar g_{Na}\\) , \\(\\bar g_K\\) ). We will come back to this point later in the tutorial.","title":"1. Observed data"},{"location":"tutorial/02_HH_simulator/#2-simulator","text":"We would like to infer the posterior over the two parameters ( \\(\\bar g_{Na}\\) , \\(\\bar g_K\\) ) of a Hodgkin-Huxley model, given the observed electrophysiological recording above. The model has channel kinetics as in Pospischil et al. 2008 , and is defined by the following set of differential equations (parameters of interest highlighted in orange): \\[ \\scriptsize \\begin{align} C_m\\frac{dV}{dt}&=g_1\\left(E_1-V\\right)+ \\color{orange}{\\bar{g}_{Na}}m^3h\\left(E_{Na}-V\\right)+ \\color{orange}{\\bar{g}_{K}}n^4\\left(E_K-V\\right)+ \\bar{g}_Mp\\left(E_K-V\\right)+ I_{inj}+ \\sigma\\eta\\left(t\\right)\\\\ \\frac{dq}{dt}&=\\frac{q_\\infty\\left(V\\right)-q}{\\tau_q\\left(V\\right)},\\;q\\in\\{m,h,n,p\\} \\end{align} \\] Above, \\(V\\) represents the membrane potential, \\(C_m\\) is the membrane capacitance, \\(g_{\\text{l}}\\) is the leak conductance, \\(E_{\\text{l}}\\) is the membrane reversal potential, \\(\\bar{g}_c\\) is the density of channels of type \\(c\\) ( \\(\\text{Na}^+\\) , \\(\\text{K}^+\\) , M), \\(E_c\\) is the reversal potential of \\(c\\) , ( \\(m\\) , \\(h\\) , \\(n\\) , \\(p\\) ) are the respective channel gating kinetic variables, and \\(\\sigma \\eta(t)\\) is the intrinsic neural noise. The right hand side of the voltage dynamics is composed of a leak current, a voltage-dependent \\(\\text{Na}^+\\) current, a delayed-rectifier \\(\\text{K}^+\\) current, a slow voltage-dependent \\(\\text{K}^+\\) current responsible for spike-frequency adaptation, and an injected current \\(I_{\\text{inj}}\\) . Channel gating variables \\(q\\) have dynamics fully characterized by the neuron membrane potential \\(V\\) , given the respective steady-state \\(q_{\\infty}(V)\\) and time constant \\(\\tau_{q}(V)\\) (details in Pospischil et al. 2008). The input current \\(I_{\\text{inj}}\\) is defined as from HH_helper_functions import syn_current I , t_on , t_off , dt , t , A_soma = syn_current () The Hodgkin-Huxley simulator is given by: from HH_helper_functions import HHsimulator Putting the input current and the simulator together: def run_HH_model ( params ): params = np . asarray ( params ) # input current, time step I , t_on , t_off , dt , t , A_soma = syn_current () t = np . arange ( 0 , len ( I ), 1 ) * dt # initial voltage V0 = - 70 states = HHsimulator ( V0 , params . reshape ( 1 , - 1 ), dt , t , I , seed = 0 ) return dict ( data = states . reshape ( - 1 ), time = t , dt = dt , I = I . reshape ( - 1 )) To get an idea of the output of the Hodgkin-Huxley model, let us generate some voltage traces for different parameters ( \\(\\bar g_{Na}\\) , \\(\\bar g_K\\) ), given the input current \\(I_{\\text{inj}}\\) : # three sets of (g_Na, g_K) params = np . array ([[ 50. , 1. ],[ 4. , 1.5 ],[ 20. , 15. ]]) num_samples = len ( params [:, 0 ]) sim_samples = np . zeros (( num_samples , len ( I ))) for i in range ( num_samples ): sim_samples [ i ,:] = run_HH_model ( params = params [ i ,:])[ 'data' ] # colors for traces col_min = 2 num_colors = num_samples + col_min cm1 = mpl . cm . Blues col1 = [ cm1 ( 1. * i / num_colors ) for i in range ( col_min , num_colors )] fig = plt . figure ( figsize = ( 7 , 5 )) gs = mpl . gridspec . GridSpec ( 2 , 1 , height_ratios = [ 4 , 1 ]) ax = plt . subplot ( gs [ 0 ]) for i in range ( num_samples ): plt . plot ( t , sim_samples [ i ,:], color = col1 [ i ], lw = 2 ) plt . ylabel ( 'voltage (mV)' ) ax . set_xticks ([]) ax . set_yticks ([ - 80 , - 20 , 40 ]) ax = plt . subplot ( gs [ 1 ]) plt . plot ( t , I * A_soma * 1e3 , 'k' , lw = 2 ) plt . xlabel ( 'time (ms)' ) plt . ylabel ( 'input (nA)' ) ax . set_xticks ([ 0 , max ( t ) / 2 , max ( t )]) ax . set_yticks ([ 0 , 1.1 * np . max ( I * A_soma * 1e3 )]) ax . yaxis . set_major_formatter ( mpl . ticker . FormatStrFormatter ( ' %.2f ' )) plt . show () As can be seen, the voltage traces can be quite diverse for different parameter values. Often, we are not interested in matching the exact trace, but only in matching certain features thereof. In this example of the Hodgkin-Huxley model, the summary features are the number of spikes, the mean resting potential, the standard deviation of the resting potential, and the first four voltage moments: mean, standard deviation, skewness and kurtosis. Using the function calculate_summary_statistics() imported below, we obtain these statistics from the output of the Hodgkin Huxley simulator. from HH_helper_functions import calculate_summary_statistics Lastly, we define a function that performs all of the above steps at once. The function simulation_wrapper takes in conductance values, runs the Hodgkin Huxley model and then returns the summary statistics. def simulation_wrapper ( params ): \"\"\" Returns summary statistics from conductance values in `params`. Summarizes the output of the HH simulator and converts it to `torch.Tensor`. \"\"\" obs = run_HH_model ( params ) summstats = torch . as_tensor ( calculate_summary_statistics ( obs )) return summstats sbi takes any function as simulator. Thus, sbi also has the flexibility to use simulators that utilize external packages, e.g., Brian ( http://briansimulator.org/ ), nest ( https://www.nest-simulator.org/ ), or NEURON ( https://neuron.yale.edu/neuron/ ). External simulators do not even need to be Python-based as long as they store simulation outputs in a format that can be read from Python. All that is necessary is to wrap your external simulator of choice into a Python callable that takes a parameter set and outputs a set of summary statistics we want to fit the parameters to.","title":"2. Simulator"},{"location":"tutorial/02_HH_simulator/#3-prior-over-model-parameters","text":"Now that we have the simulator, we need to define a function with the prior over the model parameters ( \\(\\bar g_{Na}\\) , \\(\\bar g_K\\) ), which in this case is chosen to be a Uniform distribution: prior_min = [ . 5 , 1e-4 ] prior_max = [ 80. , 15. ] prior = utils . torchutils . BoxUniform ( low = torch . as_tensor ( prior_min ), high = torch . as_tensor ( prior_max ))","title":"3. Prior over model parameters"},{"location":"tutorial/02_HH_simulator/#inference","text":"Now that we have all the required components, we can run inference with SNPE to identify parameters whose activity matches this trace. posterior = infer ( simulation_wrapper , prior , method = 'SNPE' , num_simulations = 300 , num_workers = 4 ) HBox(children=(FloatProgress(value=0.0, description='Running 300 simulations in 300 batches.', max=300.0, styl\u2026 /Users/deismic/anaconda3/envs/sbi/lib/python3.8/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning) Neural network successfully converged after 202 epochs. Note sbi can parallelize your simulator. If you experience problems with parallelization, try setting num_workers=1 and please give us an error report as a GitHub issue .","title":"Inference"},{"location":"tutorial/02_HH_simulator/#coming-back-to-the-observed-data","text":"As mentioned at the beginning of the tutorial, the observed data are generated by the Hodgkin-Huxley model with a set of known parameters ( \\(\\bar g_{Na}\\) , \\(\\bar g_K\\) ). To illustrate how to compute the summary statistics of the observed data, let us regenerate the observed data: # true parameters and respective labels true_params = np . array ([ 50. , 5. ]) labels_params = [ r '$g_ {Na} $' , r '$g_ {K} $' ] observation_trace = run_HH_model ( true_params ) observation_summary_statistics = calculate_summary_statistics ( observation_trace ) As we already shown above, the observed voltage traces look as follows: fig = plt . figure ( figsize = ( 7 , 5 )) gs = mpl . gridspec . GridSpec ( 2 , 1 , height_ratios = [ 4 , 1 ]) ax = plt . subplot ( gs [ 0 ]) plt . plot ( observation_trace [ 'time' ], observation_trace [ 'data' ]) plt . ylabel ( 'voltage (mV)' ) plt . title ( 'observed data' ) plt . setp ( ax , xticks = [], yticks = [ - 80 , - 20 , 40 ]) ax = plt . subplot ( gs [ 1 ]) plt . plot ( observation_trace [ 'time' ], I * A_soma * 1e3 , 'k' , lw = 2 ) plt . xlabel ( 'time (ms)' ) plt . ylabel ( 'input (nA)' ) ax . set_xticks ([ 0 , max ( observation_trace [ 'time' ]) / 2 , max ( observation_trace [ 'time' ])]) ax . set_yticks ([ 0 , 1.1 * np . max ( I * A_soma * 1e3 )]) ax . yaxis . set_major_formatter ( mpl . ticker . FormatStrFormatter ( ' %.2f ' ))","title":"Coming back to the observed data"},{"location":"tutorial/02_HH_simulator/#analysis-of-the-posterior-given-the-observed-data","text":"After running the inference algorithm, let us inspect the inferred posterior distribution over the parameters ( \\(\\bar g_{Na}\\) , \\(\\bar g_K\\) ), given the observed trace. To do so, we first draw samples (i.e. consistent parameter sets) from the posterior: samples = posterior . sample (( 10000 ,), x = observation_summary_statistics ) HBox(children=(FloatProgress(value=0.0, description='Drawing 10000 posterior samples', max=10000.0, style=Prog\u2026 fig , axes = utils . pairplot ( samples , limits = [[ . 5 , 80 ], [ 1e-4 , 15. ]], ticks = [[ . 5 , 80 ], [ 1e-4 , 15. ]], fig_size = ( 5 , 5 ), points = true_params , points_offdiag = { 'markersize' : 6 }, points_colors = 'r' ); As can be seen, the inferred posterior contains the ground-truth parameters (red) in a high-probability region. Now, let us sample parameters from the posterior distribution, simulate the Hodgkin-Huxley model for this parameter set and compare the simulations with the observed data: # Draw a sample from the posterior and convert to numpy for plotting. posterior_sample = posterior . sample (( 1 ,), x = observation_summary_statistics ) . numpy () HBox(children=(FloatProgress(value=0.0, description='Drawing 1 posterior samples', max=1.0, style=ProgressStyl\u2026 fig = plt . figure ( figsize = ( 7 , 5 )) # plot observation t = observation_trace [ 'time' ] y_obs = observation_trace [ 'data' ] plt . plot ( t , y_obs , lw = 2 , label = 'observation' ) # simulate and plot samples x = run_HH_model ( posterior_sample ) plt . plot ( t , x [ 'data' ], '--' , lw = 2 , label = 'posterior sample' ) plt . xlabel ( 'time (ms)' ) plt . ylabel ( 'voltage (mV)' ) ax = plt . gca () handles , labels = ax . get_legend_handles_labels () ax . legend ( handles [:: - 1 ], labels [:: - 1 ], bbox_to_anchor = ( 1.3 , 1 ), loc = 'upper right' ) ax . set_xticks ([ 0 , 60 , 120 ]) ax . set_yticks ([ - 80 , - 20 , 40 ]); As can be seen, the sample from the inferred posterior leads to simulations that closely resemble the observed data, confirming that SNPE did a good job at capturing the observed data in this simple case.","title":"Analysis of the posterior given the observed data"},{"location":"tutorial/02_HH_simulator/#references","text":"A. L. Hodgkin and A. F. Huxley. A quantitative description of membrane current and its application to conduction and excitation in nerve. The Journal of Physiology, 117(4):500\u2013544, 1952. M. Pospischil, M. Toledo-Rodriguez, C. Monier, Z. Piwkowska, T. Bal, Y. Fr\u00e9gnac, H. Markram, and A. Destexhe. Minimal Hodgkin-Huxley type models for different classes of cortical and thalamic neurons. Biological Cybernetics, 99(4-5), 2008.","title":"References"},{"location":"tutorial/03_flexible_interface/","text":"The flexible interface \u00b6 In the previous tutorial, we have demonstrated how sbi can be used to run simulation-based inference with just a single line of code. In addition to this simple interface, sbi also provides a flexible interface which unlocks several additional features implemented in sbi . Features \u00b6 The flexible interface allows you to customize the following: performing sequential posterior estimation by using multiple rounds. This can decrease the number of simulations one has to run, but the inference procedure is no longer amortized. specify your own density estimator, or change hyperparameters of existing ones (e.g. number of hidden units for NSF ). run simulations in batches, which can speed up simulations. when it makes sense, choose between different methods to sample from the posterior. use calibration kernels as proposed by Lueckmann, Goncalves et al. 2017 . Linear Gaussian example \u00b6 import torch from sbi.inference import SNPE , prepare_for_sbi from sbi.utils.get_nn_models import posterior_nn import sbi.utils as utils We will show an example of how we can use the flexible interface to infer the posterior for an example with a Gaussian likelihood (same example as before). After importing the inference method we want, SNPE , a function to prepare the input prepare_for_sbi and a factory for density estimators posterior_nn above, we define the simulator and prior: num_dim = 3 prior = utils . BoxUniform ( low =- 2 * torch . ones ( num_dim ), high = 2 * torch . ones ( num_dim )) def linear_gaussian ( theta ): return theta + 1.0 + torch . randn_like ( theta ) * 0.1 In the flexible interface, you have to ensure that your simulator and prior adhere the requirements of sbi . You can do so with the prepare_for_sbi() function. simulator , prior = prepare_for_sbi ( linear_gaussian , prior ) It is possible to specify a custom density estimator. One option is to use one of set of preconfigured density estimators by passing a string in the density_estimator keyword argument, e.g., \u201cmaf\u201d to use a Masked Autoregressive Flow, of \u201cnsf\u201d to use a Neural Spline Flow with default hyperparameters. Alternatively, you can use a set of utils functions to configure a density estimator yourself, e.g., use a MAF with hyperparameters chosen for your problem at hand. Finally, it is also possible to implement your own density estimator from scratch, e.g., including embedding nets to preprocess data, or to a density estimator architecture of your choice. For the last two options the density_estimator argument needs to be a function that takes theta and x batches as arguments to then construct the density estimator after the first set of simulations was generated. Our utils functions in sbi/utils/get_nn_models return such a function. Here, because we want to use S*N*PE, we specifiy a neural network targeting the posterior (using the utils function posterior_nn ). In this example, we will create a neural spline flow ( 'nsf' ) with 60 hidden units and 3 transform layers: density_estimator_build_fun = posterior_nn ( model = 'nsf' , hidden_features = 60 , num_transforms = 3 ) We will use SNPE with a simulation_batch_size=10 , i.e. 10 simulations will be passed to the simulator which will then handle the simulations in a vectorized way (note that your simulator has to support this in order to use this feature): inference = SNPE ( simulator , prior , density_estimator = density_estimator_build_fun , show_progress_bars = False ) And we can run inference. In this example, we will run inference over 2 rounds, potentially leading to a more focused posterior around the observation x_o . num_rounds = 2 x_o = torch . zeros ( 3 ,) posteriors = [] proposal = None for _ in range ( num_rounds ): posterior = inference ( num_simulations = 500 , proposal = proposal ) posteriors . append ( posterior ) proposal = posterior . set_default_x ( x_o ) Neural network successfully converged after 85 epochs. HBox(children=(FloatProgress(value=0.0, description='Drawing 500 posterior samples', max=500.0, style=Progress\u2026 Neural network successfully converged after 22 epochs. Note that, for num_rounds>1 , the posterior is no longer amortized: it will give good results when sampled around x=observation , but possibly bad results for other x . Once we have obtained the posterior, we can .sample() , .log_prob() , or .pairplot() in the same way as for the easy interface. posterior_samples = posterior . sample (( 10000 ,), x = x_o ) # plot posterior samples _ = utils . pairplot ( posterior_samples , limits = [[ - 2 , 2 ],[ - 2 , 2 ],[ - 2 , 2 ]], fig_size = ( 5 , 5 )) HBox(children=(FloatProgress(value=0.0, description='Drawing 10000 posterior samples', max=10000.0, style=Prog\u2026 We can always print the posterior to know how it was trained: print ( posterior ) Posterior conditional density p(\u03b8|x) (multi-round). Evaluates and samples by default at x=[[0.0, 0.0, 0.0]]. This neural posterior was obtained with a SNPE-class method using a flow.","title":"Flexible interface"},{"location":"tutorial/03_flexible_interface/#the-flexible-interface","text":"In the previous tutorial, we have demonstrated how sbi can be used to run simulation-based inference with just a single line of code. In addition to this simple interface, sbi also provides a flexible interface which unlocks several additional features implemented in sbi .","title":"The flexible interface"},{"location":"tutorial/03_flexible_interface/#features","text":"The flexible interface allows you to customize the following: performing sequential posterior estimation by using multiple rounds. This can decrease the number of simulations one has to run, but the inference procedure is no longer amortized. specify your own density estimator, or change hyperparameters of existing ones (e.g. number of hidden units for NSF ). run simulations in batches, which can speed up simulations. when it makes sense, choose between different methods to sample from the posterior. use calibration kernels as proposed by Lueckmann, Goncalves et al. 2017 .","title":"Features"},{"location":"tutorial/03_flexible_interface/#linear-gaussian-example","text":"import torch from sbi.inference import SNPE , prepare_for_sbi from sbi.utils.get_nn_models import posterior_nn import sbi.utils as utils We will show an example of how we can use the flexible interface to infer the posterior for an example with a Gaussian likelihood (same example as before). After importing the inference method we want, SNPE , a function to prepare the input prepare_for_sbi and a factory for density estimators posterior_nn above, we define the simulator and prior: num_dim = 3 prior = utils . BoxUniform ( low =- 2 * torch . ones ( num_dim ), high = 2 * torch . ones ( num_dim )) def linear_gaussian ( theta ): return theta + 1.0 + torch . randn_like ( theta ) * 0.1 In the flexible interface, you have to ensure that your simulator and prior adhere the requirements of sbi . You can do so with the prepare_for_sbi() function. simulator , prior = prepare_for_sbi ( linear_gaussian , prior ) It is possible to specify a custom density estimator. One option is to use one of set of preconfigured density estimators by passing a string in the density_estimator keyword argument, e.g., \u201cmaf\u201d to use a Masked Autoregressive Flow, of \u201cnsf\u201d to use a Neural Spline Flow with default hyperparameters. Alternatively, you can use a set of utils functions to configure a density estimator yourself, e.g., use a MAF with hyperparameters chosen for your problem at hand. Finally, it is also possible to implement your own density estimator from scratch, e.g., including embedding nets to preprocess data, or to a density estimator architecture of your choice. For the last two options the density_estimator argument needs to be a function that takes theta and x batches as arguments to then construct the density estimator after the first set of simulations was generated. Our utils functions in sbi/utils/get_nn_models return such a function. Here, because we want to use S*N*PE, we specifiy a neural network targeting the posterior (using the utils function posterior_nn ). In this example, we will create a neural spline flow ( 'nsf' ) with 60 hidden units and 3 transform layers: density_estimator_build_fun = posterior_nn ( model = 'nsf' , hidden_features = 60 , num_transforms = 3 ) We will use SNPE with a simulation_batch_size=10 , i.e. 10 simulations will be passed to the simulator which will then handle the simulations in a vectorized way (note that your simulator has to support this in order to use this feature): inference = SNPE ( simulator , prior , density_estimator = density_estimator_build_fun , show_progress_bars = False ) And we can run inference. In this example, we will run inference over 2 rounds, potentially leading to a more focused posterior around the observation x_o . num_rounds = 2 x_o = torch . zeros ( 3 ,) posteriors = [] proposal = None for _ in range ( num_rounds ): posterior = inference ( num_simulations = 500 , proposal = proposal ) posteriors . append ( posterior ) proposal = posterior . set_default_x ( x_o ) Neural network successfully converged after 85 epochs. HBox(children=(FloatProgress(value=0.0, description='Drawing 500 posterior samples', max=500.0, style=Progress\u2026 Neural network successfully converged after 22 epochs. Note that, for num_rounds>1 , the posterior is no longer amortized: it will give good results when sampled around x=observation , but possibly bad results for other x . Once we have obtained the posterior, we can .sample() , .log_prob() , or .pairplot() in the same way as for the easy interface. posterior_samples = posterior . sample (( 10000 ,), x = x_o ) # plot posterior samples _ = utils . pairplot ( posterior_samples , limits = [[ - 2 , 2 ],[ - 2 , 2 ],[ - 2 , 2 ]], fig_size = ( 5 , 5 )) HBox(children=(FloatProgress(value=0.0, description='Drawing 10000 posterior samples', max=10000.0, style=Prog\u2026 We can always print the posterior to know how it was trained: print ( posterior ) Posterior conditional density p(\u03b8|x) (multi-round). Evaluates and samples by default at x=[[0.0, 0.0, 0.0]]. This neural posterior was obtained with a SNPE-class method using a flow.","title":"Linear Gaussian example"}]}