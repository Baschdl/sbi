{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"sbi : simulation-based inference \u00b6 sbi : A Python toolbox for simulation-based inference. Inference can be run in a single line of code: posterior = infer ( simulator , prior , method = 'SNPE' , num_simulations = 1000 ) To learn about the general motivation behind simulation-based inference, and the inference methods included in sbi , read on below. For example applications to canonical problems in neuroscience, browse the recent research article Training deep neural density estimators to identify mechanistic models of neural dynamics . If you want to get started using sbi on your own problem, jump to installation and then check out the tutorial . Motivation and approach \u00b6 Many areas of science and engineering make extensive use of complex, stochastic, numerical simulations to describe the structure and dynamics of the processes being investigated. A key challenge in simulation-based science is constraining these simulation models\u2019 parameters, which are intepretable quantities, with observational data. Bayesian inference provides a general and powerful framework to invert the simulators, i.e. describe the parameters which are consistent both with empirical data and prior knowledge. In the case of simulators, a key quantity required for statistical inference, the likelihood of observed data given parameters, \\(\\mathcal{L}(\\theta) = p(x_o|\\theta)\\) , is typically intractable, rendering conventional statistical approaches inapplicable. sbi implements three powerful machine-learning methods that address this problem: Sequential Neural Posterior Estimation (SNPE), Sequential Neural Likelihood Estimation (SNLE), and Sequential Neural Ratio Estimation (SNRE). Depending on the characteristics of the problem, e.g. the dimensionalities of the parameter space and the observation space, one of the methods will be more suitable. Goal: Algorithmically identify mechanistic models which are consistent with data. Each of the methods above needs three inputs: A candidate mechanistic model, prior knowledge or constraints on model parameters, and observational data (or summary statistics thereof). The methods then proceed by sampling parameters from the prior followed by simulating synthetic data from these parameters, learning the (probabilistic) association between data (or data features) and underlying parameters, i.e. to learn statistical inference from simulated data. The way in which this association is learned differs between the above methods, but all use deep neural networks. This learned neural network is then applied to empirical data to derive the full space of parameters consistent with the data and the prior, i.e. the posterior distribution. High posterior probability is assigned to parameters which are consistent with both the data and the prior, low probability to inconsistent parameters. While SNPE directly learns the posterior distribution, SNLE and SNRE need an extra MCMC sampling step to construct a posterior. If needed, an initial estimate of the posterior can be used to adaptively generate additional informative simulations. Publications \u00b6 See Cranmer, Brehmer, Louppe (2020) for a recent review on simulation-based inference. The following papers offer additional details on the inference methods included in sbi : SNPE \u00b6 Fast \u03b5-free Inference of Simulation Models with Bayesian Conditional Density Estimation by Papamakarios & Murray (NeurIPS 2016) [PDF] [BibTeX] Flexible statistical inference for mechanistic models of neural dynamics by Lueckmann, Goncalves, Bassetto, \u00d6cal, Nonnenmacher & Macke (NeurIPS 2017) [PDF] [BibTeX] Automatic posterior transformation for likelihood-free inference by Greenberg, Nonnenmacher & Macke (ICML 2019) [PDF] [BibTeX] SNLE \u00b6 Sequential neural likelihood: Fast likelihood-free inference with autoregressive flows by Papamakarios, Sterratt & Murray (AISTATS 2019) [PDF] [BibTeX] SNRE \u00b6 Likelihood-free MCMC with Amortized Approximate Likelihood Ratios by Hermans, Begy & Louppe (ICML 2020) [PDF] On Contrastive Learning for Likelihood-free Inference Durkan, Murray & Papamakarios (ICML 2020) [PDF] . Support \u00b6 sbi has been developed in the context of the ADIMEM grant , project A. ADIMEM is a BMBF grant awarded to groups at the Technical University of Munich, University of T\u00fcbingen and Research Center caesar of the Max Planck Gesellschaft.","title":"Home"},{"location":"#sbi-simulation-based-inference","text":"sbi : A Python toolbox for simulation-based inference. Inference can be run in a single line of code: posterior = infer ( simulator , prior , method = 'SNPE' , num_simulations = 1000 ) To learn about the general motivation behind simulation-based inference, and the inference methods included in sbi , read on below. For example applications to canonical problems in neuroscience, browse the recent research article Training deep neural density estimators to identify mechanistic models of neural dynamics . If you want to get started using sbi on your own problem, jump to installation and then check out the tutorial .","title":"sbi: simulation-based inference"},{"location":"#motivation-and-approach","text":"Many areas of science and engineering make extensive use of complex, stochastic, numerical simulations to describe the structure and dynamics of the processes being investigated. A key challenge in simulation-based science is constraining these simulation models\u2019 parameters, which are intepretable quantities, with observational data. Bayesian inference provides a general and powerful framework to invert the simulators, i.e. describe the parameters which are consistent both with empirical data and prior knowledge. In the case of simulators, a key quantity required for statistical inference, the likelihood of observed data given parameters, \\(\\mathcal{L}(\\theta) = p(x_o|\\theta)\\) , is typically intractable, rendering conventional statistical approaches inapplicable. sbi implements three powerful machine-learning methods that address this problem: Sequential Neural Posterior Estimation (SNPE), Sequential Neural Likelihood Estimation (SNLE), and Sequential Neural Ratio Estimation (SNRE). Depending on the characteristics of the problem, e.g. the dimensionalities of the parameter space and the observation space, one of the methods will be more suitable. Goal: Algorithmically identify mechanistic models which are consistent with data. Each of the methods above needs three inputs: A candidate mechanistic model, prior knowledge or constraints on model parameters, and observational data (or summary statistics thereof). The methods then proceed by sampling parameters from the prior followed by simulating synthetic data from these parameters, learning the (probabilistic) association between data (or data features) and underlying parameters, i.e. to learn statistical inference from simulated data. The way in which this association is learned differs between the above methods, but all use deep neural networks. This learned neural network is then applied to empirical data to derive the full space of parameters consistent with the data and the prior, i.e. the posterior distribution. High posterior probability is assigned to parameters which are consistent with both the data and the prior, low probability to inconsistent parameters. While SNPE directly learns the posterior distribution, SNLE and SNRE need an extra MCMC sampling step to construct a posterior. If needed, an initial estimate of the posterior can be used to adaptively generate additional informative simulations.","title":"Motivation and approach"},{"location":"#publications","text":"See Cranmer, Brehmer, Louppe (2020) for a recent review on simulation-based inference. The following papers offer additional details on the inference methods included in sbi :","title":"Publications"},{"location":"#snpe","text":"Fast \u03b5-free Inference of Simulation Models with Bayesian Conditional Density Estimation by Papamakarios & Murray (NeurIPS 2016) [PDF] [BibTeX] Flexible statistical inference for mechanistic models of neural dynamics by Lueckmann, Goncalves, Bassetto, \u00d6cal, Nonnenmacher & Macke (NeurIPS 2017) [PDF] [BibTeX] Automatic posterior transformation for likelihood-free inference by Greenberg, Nonnenmacher & Macke (ICML 2019) [PDF] [BibTeX]","title":"SNPE"},{"location":"#snle","text":"Sequential neural likelihood: Fast likelihood-free inference with autoregressive flows by Papamakarios, Sterratt & Murray (AISTATS 2019) [PDF] [BibTeX]","title":"SNLE"},{"location":"#snre","text":"Likelihood-free MCMC with Amortized Approximate Likelihood Ratios by Hermans, Begy & Louppe (ICML 2020) [PDF] On Contrastive Learning for Likelihood-free Inference Durkan, Murray & Papamakarios (ICML 2020) [PDF] .","title":"SNRE"},{"location":"#support","text":"sbi has been developed in the context of the ADIMEM grant , project A. ADIMEM is a BMBF grant awarded to groups at the Technical University of Munich, University of T\u00fcbingen and Research Center caesar of the Max Planck Gesellschaft.","title":"Support"},{"location":"contribute/","text":"User experiences, bugs, and feature requests \u00b6 If you are using sbi to infer the parameters of a simulator, we would be delighted to know how it worked for you. If it didn\u2019t work according to plan, please open up an issue and tell us more about your use case: the dimensionality of the input parameters and of the output, as well as the setup you used to run inference (i.e. number of simulations, number of rounds,\u2026). To report bugs and suggest features (including better documentation), please equally head over to issues on GitHub . Code contributions \u00b6 In general, we use pull requests to make changes to sbi . Development environment \u00b6 Clone the repo and install all the dependencies using the environment.yml file to create a conda environment: conda env create -f environment.yml . If you already have an sbi environment and want to refresh dependencies, just run conda env update -f environment.yml --prune . Alternatively, you can install via setup.py using pip install -e \".[dev]\" (the dev flag installs development and testing dependencies). Contributing inference algorithms \u00b6 sbi was developed to be extensible and we welcome implementations of additional inference algorithms. Your new inference algorithm should be a class in sbi/inference/your_type_of_algorithm/your_algorithm.py . The class should have a __call__() function which runs inference and returns a posterior object. The posterior object itself should have a .sample() function following the signature of sbi/inference/NeuralPosterior , allowing to draw samples from the posterior. Currently, SNPE , SNLE , and SNRE all share the NeuralPosterior class in sbi/inference/posterior.py , but future versions of sbi will refactor them into separate classes. Style conventions \u00b6 For docstrings and comments, we use Google Style . Code needs to pass through the following tools, which are installed alongside sbi : black : Automatic code formatting for Python. You can run black manually from the console using black . in the top directory of the repository, which will format all files. isort : Used to consistently order imports. You can run isort manually from the console using isort in the top directory. Online documentation \u00b6 Most of the documentation is written in markdown ( basic markdown guide ). You can directly fix mistakes and suggest clearer formulations in markdown files simply by initiating a PR on through GitHub. Click on documentation file and look for the little pencil at top right.","title":"Contribute"},{"location":"contribute/#user-experiences-bugs-and-feature-requests","text":"If you are using sbi to infer the parameters of a simulator, we would be delighted to know how it worked for you. If it didn\u2019t work according to plan, please open up an issue and tell us more about your use case: the dimensionality of the input parameters and of the output, as well as the setup you used to run inference (i.e. number of simulations, number of rounds,\u2026). To report bugs and suggest features (including better documentation), please equally head over to issues on GitHub .","title":"User experiences, bugs, and feature requests"},{"location":"contribute/#code-contributions","text":"In general, we use pull requests to make changes to sbi .","title":"Code contributions"},{"location":"contribute/#development-environment","text":"Clone the repo and install all the dependencies using the environment.yml file to create a conda environment: conda env create -f environment.yml . If you already have an sbi environment and want to refresh dependencies, just run conda env update -f environment.yml --prune . Alternatively, you can install via setup.py using pip install -e \".[dev]\" (the dev flag installs development and testing dependencies).","title":"Development environment"},{"location":"contribute/#contributing-inference-algorithms","text":"sbi was developed to be extensible and we welcome implementations of additional inference algorithms. Your new inference algorithm should be a class in sbi/inference/your_type_of_algorithm/your_algorithm.py . The class should have a __call__() function which runs inference and returns a posterior object. The posterior object itself should have a .sample() function following the signature of sbi/inference/NeuralPosterior , allowing to draw samples from the posterior. Currently, SNPE , SNLE , and SNRE all share the NeuralPosterior class in sbi/inference/posterior.py , but future versions of sbi will refactor them into separate classes.","title":"Contributing inference algorithms"},{"location":"contribute/#style-conventions","text":"For docstrings and comments, we use Google Style . Code needs to pass through the following tools, which are installed alongside sbi : black : Automatic code formatting for Python. You can run black manually from the console using black . in the top directory of the repository, which will format all files. isort : Used to consistently order imports. You can run isort manually from the console using isort in the top directory.","title":"Style conventions"},{"location":"contribute/#online-documentation","text":"Most of the documentation is written in markdown ( basic markdown guide ). You can directly fix mistakes and suggest clearer formulations in markdown files simply by initiating a PR on through GitHub. Click on documentation file and look for the little pencil at top right.","title":"Online documentation"},{"location":"credits/","text":"Credits \u00b6 sbi is licensed under the Affero General Public License version 3 (AGPLv3) and Copyright (C) 2020 \u00c1lvaro Tejero-Cantero, Jakob H. Macke, Jan-Matthis L\u00fcckmann, Michael Deistler, Jan F. B\u00f6lts. Copyright (C) 2020 Conor M. Durkan. Important dependencies and prior art \u00b6 sbi is the successor to delfi , a Theano-based toolbox for sequential neural posterior estimation developed at mackelab . If you were using delfi , we strongly recommend to move your inference over to sbi . Please open issues if you find unexpected behaviour or missing features. We will consider these bugs and give them priority. sbi as a PyTorch-based toolbox started as a fork of conormdurkan/lfi , by Conor M.Durkan . sbi uses density estimators from bayesiains/nflows by Conor M.Durkan , George Papamakarios and Artur Bekasov . These are proxied through pyknos , a package focused on density estimation. sbi uses PyTorch and tries to align with the interfaces (e.g. for probability distributions) adopted by PyTorch . See README.md for a list of publications describing the methods implemented in sbi . Citation \u00b6 If you use sbi consider citing the corresponding paper : @article{tejero-cantero2020sbi, doi = {10.21105/joss.02505}, url = {https://doi.org/10.21105/joss.02505}, year = {2020}, publisher = {The Open Journal}, volume = {5}, number = {52}, pages = {2505}, author = {Alvaro Tejero-Cantero and Jan Boelts and Michael Deistler and Jan-Matthis Lueckmann and Conor Durkan and Pedro J. Gon\u00e7alves and David S. Greenberg and Jakob H. Macke}, title = {sbi: A toolkit for simulation-based inference}, journal = {Journal of Open Source Software} } Support \u00b6 sbi has been developed in the context of the ADIMEM grant , project A. ADIMEM is a BMBF grant awarded to groups at the Technical University of Munich, University of T\u00fcbingen and Research Center caesar of the Max Planck Gesellschaft.","title":"Credits"},{"location":"credits/#credits","text":"sbi is licensed under the Affero General Public License version 3 (AGPLv3) and Copyright (C) 2020 \u00c1lvaro Tejero-Cantero, Jakob H. Macke, Jan-Matthis L\u00fcckmann, Michael Deistler, Jan F. B\u00f6lts. Copyright (C) 2020 Conor M. Durkan.","title":"Credits"},{"location":"credits/#important-dependencies-and-prior-art","text":"sbi is the successor to delfi , a Theano-based toolbox for sequential neural posterior estimation developed at mackelab . If you were using delfi , we strongly recommend to move your inference over to sbi . Please open issues if you find unexpected behaviour or missing features. We will consider these bugs and give them priority. sbi as a PyTorch-based toolbox started as a fork of conormdurkan/lfi , by Conor M.Durkan . sbi uses density estimators from bayesiains/nflows by Conor M.Durkan , George Papamakarios and Artur Bekasov . These are proxied through pyknos , a package focused on density estimation. sbi uses PyTorch and tries to align with the interfaces (e.g. for probability distributions) adopted by PyTorch . See README.md for a list of publications describing the methods implemented in sbi .","title":"Important dependencies and prior art"},{"location":"credits/#citation","text":"If you use sbi consider citing the corresponding paper : @article{tejero-cantero2020sbi, doi = {10.21105/joss.02505}, url = {https://doi.org/10.21105/joss.02505}, year = {2020}, publisher = {The Open Journal}, volume = {5}, number = {52}, pages = {2505}, author = {Alvaro Tejero-Cantero and Jan Boelts and Michael Deistler and Jan-Matthis Lueckmann and Conor Durkan and Pedro J. Gon\u00e7alves and David S. Greenberg and Jakob H. Macke}, title = {sbi: A toolkit for simulation-based inference}, journal = {Journal of Open Source Software} }","title":"Citation"},{"location":"credits/#support","text":"sbi has been developed in the context of the ADIMEM grant , project A. ADIMEM is a BMBF grant awarded to groups at the Technical University of Munich, University of T\u00fcbingen and Research Center caesar of the Max Planck Gesellschaft.","title":"Support"},{"location":"faq/","text":"Frequently asked questions \u00b6 Can the algorithms deal with invalid data, e.g. NaN or inf? What should I do when my \u2018posterior samples are outside of the prior support\u2019 in SNPE? When using multiple workers, I get a pickling error. Can I still use multiprocessing? Can I use the GPU for training the density estimator? How should I save and load objects in sbi ? Can I stop neural network training and resume it later?","title":"FAQ"},{"location":"faq/#frequently-asked-questions","text":"Can the algorithms deal with invalid data, e.g. NaN or inf? What should I do when my \u2018posterior samples are outside of the prior support\u2019 in SNPE? When using multiple workers, I get a pickling error. Can I still use multiprocessing? Can I use the GPU for training the density estimator? How should I save and load objects in sbi ? Can I stop neural network training and resume it later?","title":"Frequently asked questions"},{"location":"install/","text":"Installation \u00b6 sbi requires Python 3.6 or higher. We recommend to use a conda virtual environment ( Miniconda installation instructions ). If conda is installed on the system, an environment for installing sbi can be created as follows: # Create an environment for sbi (indicate Python 3.6 or higher); activate it $ conda create -n sbi_env python=3.7 && conda activate sbi_env Independent of whether you are using conda or not, sbi can be installed using pip : $ pip install sbi To test the installation, drop into a python prompt and run from sbi.examples.minimal import simple posterior = simple () print ( posterior )","title":"Installation"},{"location":"install/#installation","text":"sbi requires Python 3.6 or higher. We recommend to use a conda virtual environment ( Miniconda installation instructions ). If conda is installed on the system, an environment for installing sbi can be created as follows: # Create an environment for sbi (indicate Python 3.6 or higher); activate it $ conda create -n sbi_env python=3.7 && conda activate sbi_env Independent of whether you are using conda or not, sbi can be installed using pip : $ pip install sbi To test the installation, drop into a python prompt and run from sbi.examples.minimal import simple posterior = simple () print ( posterior )","title":"Installation"},{"location":"reference/","text":"API Reference \u00b6 Inference \u00b6 sbi . inference . base . infer ( simulator , prior , method , num_simulations , num_workers = 1 ) \u00b6 Return posterior distribution by running simulation-based inference. This function provides a simple interface to run sbi. Inference is run for a single round and hence the returned posterior \\(p(\\theta|x)\\) can be sampled and evaluated for any \\(x\\) (i.e. it is amortized). The scope of this function is limited to the most essential features of sbi. For more flexibility (e.g. multi-round inference, different density estimators) please use the flexible interface described here: https://www.mackelab.org/sbi/tutorial/02_flexible_interface/ Parameters: Name Type Description Default simulator Callable A function that takes parameters \\(\\theta\\) and maps them to simulations, or observations, x , \\(\\mathrm{sim}(\\theta)\\to x\\) . Any regular Python callable (i.e. function or class with __call__ method) can be used. required prior A probability distribution that expresses prior knowledge about the parameters, e.g. which ranges are meaningful for them. Any object with .log_prob() and .sample() (for example, a PyTorch distribution) can be used. required method str What inference method to use. Either of SNPE, SNLE or SNRE. required num_simulations int Number of simulation calls. More simulations means a longer runtime, but a better posterior estimate. required num_workers int Number of parallel workers to use for simulations. 1 Returns: Posterior over parameters conditional on observations (amortized). Source code in sbi/inference/base.py def infer ( simulator : Callable , prior , method : str , num_simulations : int , num_workers : int = 1 ) -> NeuralPosterior : r \"\"\" Return posterior distribution by running simulation-based inference. This function provides a simple interface to run sbi. Inference is run for a single round and hence the returned posterior $p(\\theta|x)$ can be sampled and evaluated for any $x$ (i.e. it is amortized). The scope of this function is limited to the most essential features of sbi. For more flexibility (e.g. multi-round inference, different density estimators) please use the flexible interface described here: https://www.mackelab.org/sbi/tutorial/02_flexible_interface/ Args: simulator: A function that takes parameters $\\theta$ and maps them to simulations, or observations, `x`, $\\mathrm{sim}(\\theta)\\to x$. Any regular Python callable (i.e. function or class with `__call__` method) can be used. prior: A probability distribution that expresses prior knowledge about the parameters, e.g. which ranges are meaningful for them. Any object with `.log_prob()`and `.sample()` (for example, a PyTorch distribution) can be used. method: What inference method to use. Either of SNPE, SNLE or SNRE. num_simulations: Number of simulation calls. More simulations means a longer runtime, but a better posterior estimate. num_workers: Number of parallel workers to use for simulations. Returns: Posterior over parameters conditional on observations (amortized). \"\"\" try : method_fun : Callable = getattr ( sbi . inference , method . upper ()) except AttributeError : raise NameError ( \"Method not available. `method` must be one of 'SNPE', 'SNLE', 'SNRE'.\" ) simulator , prior = prepare_for_sbi ( simulator , prior ) inference = method_fun ( prior ) theta , x = simulate_for_sbi ( simulator = simulator , proposal = prior , num_simulations = num_simulations , num_workers = num_workers , ) _ = inference . append_simulations ( theta , x ) . train () posterior = inference . build_posterior () return posterior sbi . utils . user_input_checks . prepare_for_sbi ( simulator , prior ) \u00b6 Prepare simulator, prior and for usage in sbi. One of the goals is to allow you to use sbi with inputs computed in numpy. Attempts to meet the following requirements by reshaping and type-casting: the simulator function receives as input and returns a Tensor. the simulator can simulate batches of parameters and return batches of data. the prior does not produce batches and samples and evaluates to Tensor. the output shape is a torch.Size((1,N)) (i.e, has a leading batch dimension 1). If this is not possible, a suitable exception will be raised. Parameters: Name Type Description Default simulator Callable Simulator as provided by the user. required prior Prior as provided by the user. required Returns: Type Description Tuple[Callable, torch.distributions.distribution.Distribution] Tuple (simulator, prior, x_shape) checked and matching the requirements of sbi. Source code in sbi/utils/user_input_checks.py def prepare_for_sbi ( simulator : Callable , prior ) -> Tuple [ Callable , Distribution ]: \"\"\"Prepare simulator, prior and for usage in sbi. One of the goals is to allow you to use sbi with inputs computed in numpy. Attempts to meet the following requirements by reshaping and type-casting: - the simulator function receives as input and returns a Tensor.<br/> - the simulator can simulate batches of parameters and return batches of data.<br/> - the prior does not produce batches and samples and evaluates to Tensor.<br/> - the output shape is a `torch.Size((1,N))` (i.e, has a leading batch dimension 1). If this is not possible, a suitable exception will be raised. Args: simulator: Simulator as provided by the user. prior: Prior as provided by the user. Returns: Tuple (simulator, prior, x_shape) checked and matching the requirements of sbi. \"\"\" # Check prior, return PyTorch prior. prior , _ , prior_returns_numpy = process_prior ( prior ) # Check simulator, returns PyTorch simulator able to simulate batches. simulator = process_simulator ( simulator , prior , prior_returns_numpy ) # Consistency check after making ready for sbi. check_sbi_inputs ( simulator , prior ) return simulator , prior sbi . inference . base . simulate_for_sbi ( simulator , proposal , num_simulations , num_workers = 1 , simulation_batch_size = 1 , show_progress_bar = True ) \u00b6 Returns ( \\(\\theta, x\\) ) pairs obtained from sampling the proposal and simulating. This function performs two steps: Sample parameters \\(\\theta\\) from the proposal . Simulate these parameters to obtain \\(x\\) . Parameters: Name Type Description Default simulator Callable A function that takes parameters \\(\\theta\\) and maps them to simulations, or observations, x , \\(\\text{sim}(\\theta)\\to x\\) . Any regular Python callable (i.e. function or class with __call__ method) can be used. required proposal Any Probability distribution that the parameters \\(\\theta\\) are sampled from. required num_simulations int Number of simulations that are run. required num_workers int Number of parallel workers to use for simulations. 1 simulation_batch_size int Number of parameter sets that the simulator maps to data x at once. If None, we simulate all parameter sets at the same time. If >= 1, the simulator has to process data of shape (simulation_batch_size, parameter_dimension). 1 show_progress_bar bool Whether to show a progress bar for simulating. This will not affect whether there will be a progressbar while drawing samples from the proposal. True Returns: Sampled parameters \\(\\theta\\) and simulation-outputs \\(x\\) . Source code in sbi/inference/base.py def simulate_for_sbi ( simulator : Callable , proposal : Any , num_simulations : int , num_workers : int = 1 , simulation_batch_size : int = 1 , show_progress_bar : bool = True , ) -> Tuple [ Tensor , Tensor ]: r \"\"\" Returns ($\\theta, x$) pairs obtained from sampling the proposal and simulating. This function performs two steps: - Sample parameters $\\theta$ from the `proposal`. - Simulate these parameters to obtain $x$. Args: simulator: A function that takes parameters $\\theta$ and maps them to simulations, or observations, `x`, $\\text{sim}(\\theta)\\to x$. Any regular Python callable (i.e. function or class with `__call__` method) can be used. proposal: Probability distribution that the parameters $\\theta$ are sampled from. num_simulations: Number of simulations that are run. num_workers: Number of parallel workers to use for simulations. simulation_batch_size: Number of parameter sets that the simulator maps to data x at once. If None, we simulate all parameter sets at the same time. If >= 1, the simulator has to process data of shape (simulation_batch_size, parameter_dimension). show_progress_bar: Whether to show a progress bar for simulating. This will not affect whether there will be a progressbar while drawing samples from the proposal. Returns: Sampled parameters $\\theta$ and simulation-outputs $x$. \"\"\" check_if_proposal_has_default_x ( proposal ) theta = proposal . sample (( num_simulations ,)) x = simulate_in_batches ( simulator , theta , simulation_batch_size , num_workers , show_progress_bar ) return theta , x sbi.inference.snpe.snpe_c.SNPE_C ( PosteriorEstimator ) \u00b6 __init__ ( self , prior = None , density_estimator = 'maf' , device = 'cpu' , logging_level = 'WARNING' , summary_writer = None , show_progress_bars = True , ** unused_args ) special \u00b6 SNPE-C / APT [1]. [1] Automatic Posterior Transformation for Likelihood-free Inference , Greenberg et al., ICML 2019, https://arxiv.org/abs/1905.07488 . This class implements two loss variants of SNPE-C: the non-atomic and the atomic version. The atomic loss of SNPE-C can be used for any density estimator, i.e. also for normalizing flows. However, it suffers from leakage issues. On the other hand, the non-atomic loss can only be used only if the proposal distribution is a mixture of Gaussians, the density estimator is a mixture of Gaussians, and the prior is either Gaussian or Uniform. It does not suffer from leakage issues. At the beginning of each round, we print whether the non-atomic or the atomic version is used. In this codebase, we will automatically switch to the non-atomic loss if the following criteria are fulfilled: - proposal is a DirectPosterior with density_estimator mdn , as built with utils.sbi.posterior_nn() . - the density estimator is a mdn , as built with utils.sbi.posterior_nn() . - isinstance(prior, MultivariateNormal) (from torch.distributions ) or isinstance(prior, sbi.utils.BoxUniform) Note that custom implementations of any of these densities (or estimators) will not trigger the non-atomic loss, and the algorithm will fall back onto using the atomic loss. Parameters: Name Type Description Default prior Optional[Any] A probability distribution that expresses prior knowledge about the parameters, e.g. which ranges are meaningful for them. Any object with .log_prob() and .sample() (for example, a PyTorch distribution) can be used. None density_estimator Union[str, Callable] If it is a string, use a pre-configured network of the provided type (one of nsf, maf, mdn, made). Alternatively, a function that builds a custom neural network can be provided. The function will be called with the first batch of simulations (theta, x), which can thus be used for shape inference and potentially for z-scoring. It needs to return a PyTorch nn.Module implementing the density estimator. The density estimator needs to provide the methods .log_prob and .sample() . 'maf' device str Training device, e.g., \u201ccpu\u201d, \u201ccuda\u201d or \u201ccuda:{0, 1, \u2026}\u201d. 'cpu' logging_level Union[int, str] Minimum severity of messages to log. One of the strings INFO, WARNING, DEBUG, ERROR and CRITICAL. 'WARNING' summary_writer Optional[Writer] A tensorboard SummaryWriter to control, among others, log file location (default is <current working directory>/logs .) None show_progress_bars bool Whether to show a progressbar during training. True unused_args Absorbs additional arguments. No entries will be used. If it is not empty, we warn. In future versions, when the new interface of 0.14.0 is more mature, we will remove this argument. {} Source code in sbi/inference/snpe/snpe_c.py def __init__ ( self , prior : Optional [ Any ] = None , density_estimator : Union [ str , Callable ] = \"maf\" , device : str = \"cpu\" , logging_level : Union [ int , str ] = \"WARNING\" , summary_writer : Optional [ TensorboardSummaryWriter ] = None , show_progress_bars : bool = True , ** unused_args , ): r \"\"\"SNPE-C / APT [1]. [1] _Automatic Posterior Transformation for Likelihood-free Inference_, Greenberg et al., ICML 2019, https://arxiv.org/abs/1905.07488. This class implements two loss variants of SNPE-C: the non-atomic and the atomic version. The atomic loss of SNPE-C can be used for any density estimator, i.e. also for normalizing flows. However, it suffers from leakage issues. On the other hand, the non-atomic loss can only be used only if the proposal distribution is a mixture of Gaussians, the density estimator is a mixture of Gaussians, and the prior is either Gaussian or Uniform. It does not suffer from leakage issues. At the beginning of each round, we print whether the non-atomic or the atomic version is used. In this codebase, we will automatically switch to the non-atomic loss if the following criteria are fulfilled:<br/> - proposal is a `DirectPosterior` with density_estimator `mdn`, as built with `utils.sbi.posterior_nn()`.<br/> - the density estimator is a `mdn`, as built with `utils.sbi.posterior_nn()`.<br/> - `isinstance(prior, MultivariateNormal)` (from `torch.distributions`) or `isinstance(prior, sbi.utils.BoxUniform)` Note that custom implementations of any of these densities (or estimators) will not trigger the non-atomic loss, and the algorithm will fall back onto using the atomic loss. Args: prior: A probability distribution that expresses prior knowledge about the parameters, e.g. which ranges are meaningful for them. Any object with `.log_prob()`and `.sample()` (for example, a PyTorch distribution) can be used. density_estimator: If it is a string, use a pre-configured network of the provided type (one of nsf, maf, mdn, made). Alternatively, a function that builds a custom neural network can be provided. The function will be called with the first batch of simulations (theta, x), which can thus be used for shape inference and potentially for z-scoring. It needs to return a PyTorch `nn.Module` implementing the density estimator. The density estimator needs to provide the methods `.log_prob` and `.sample()`. device: Training device, e.g., \"cpu\", \"cuda\" or \"cuda:{0, 1, ...}\". logging_level: Minimum severity of messages to log. One of the strings INFO, WARNING, DEBUG, ERROR and CRITICAL. summary_writer: A tensorboard `SummaryWriter` to control, among others, log file location (default is `<current working directory>/logs`.) show_progress_bars: Whether to show a progressbar during training. unused_args: Absorbs additional arguments. No entries will be used. If it is not empty, we warn. In future versions, when the new interface of 0.14.0 is more mature, we will remove this argument. \"\"\" kwargs = del_entries ( locals (), entries = ( \"self\" , \"__class__\" , \"unused_args\" )) super () . __init__ ( ** kwargs , ** unused_args ) append_simulations ( self , theta , x , proposal = None ) inherited \u00b6 Store parameters and simulation outputs to use them for later training. Data are stored as entries in lists for each type of variable (parameter/data). Stores \\(\\theta\\) , \\(x\\) , prior_masks (indicating if simulations are coming from the prior or not) and an index indicating which round the batch of simulations came from. Parameters: Name Type Description Default theta Tensor Parameter sets. required x Tensor Simulation outputs. required proposal Optional[Any] The distribution that the parameters \\(\\theta\\) were sampled from. Pass None if the parameters were sampled from the prior. If not None , it will trigger a different loss-function. None Returns: Type Description PosteriorEstimator NeuralInference object (returned so that this function is chainable). Source code in sbi/inference/snpe/snpe_c.py def append_simulations ( self , theta : Tensor , x : Tensor , proposal : Optional [ Any ] = None ) -> \"PosteriorEstimator\" : r \"\"\" Store parameters and simulation outputs to use them for later training. Data are stored as entries in lists for each type of variable (parameter/data). Stores $\\theta$, $x$, prior_masks (indicating if simulations are coming from the prior or not) and an index indicating which round the batch of simulations came from. Args: theta: Parameter sets. x: Simulation outputs. proposal: The distribution that the parameters $\\theta$ were sampled from. Pass `None` if the parameters were sampled from the prior. If not `None`, it will trigger a different loss-function. Returns: NeuralInference object (returned so that this function is chainable). \"\"\" theta , x = validate_theta_and_x ( theta , x , training_device = self . _device ) self . _check_proposal ( proposal ) if ( proposal is None or proposal is self . _prior or ( isinstance ( proposal , RestrictedPrior ) and proposal . _prior is self . _prior ) ): # The `_data_round_index` will later be used to infer if one should train # with MLE loss or with atomic loss (see, in `train()`: # self._round = max(self._data_round_index)) self . _data_round_index . append ( 0 ) self . _prior_masks . append ( mask_sims_from_prior ( 0 , theta . size ( 0 ))) else : if not self . _data_round_index : # This catches a pretty specific case: if, in the first round, one # passes data that does not come from the prior. self . _data_round_index . append ( 1 ) else : self . _data_round_index . append ( max ( self . _data_round_index ) + 1 ) self . _prior_masks . append ( mask_sims_from_prior ( 1 , theta . size ( 0 ))) self . _theta_roundwise . append ( theta ) self . _x_roundwise . append ( x ) self . _proposal_roundwise . append ( proposal ) if self . _prior is None or isinstance ( self . _prior , ImproperEmpirical ): if proposal is not None : raise ValueError ( \"You had not passed a prior at initialization, but now you \" \"passed a proposal. If you want to run multi-round SNPE, you have \" \"to specify a prior (set the `.prior` argument or re-initialize \" \"the object with a prior distribution). If the samples you passed \" \"to `append_simulations()` were sampled from the prior, you can \" \"run single-round inference with \" \"`append_simulations(..., proposal=None)`.\" ) theta_prior = self . get_simulations ()[ 0 ] self . _prior = ImproperEmpirical ( theta_prior , ones ( theta_prior . shape [ 0 ])) return self build_posterior ( self , density_estimator = None , sample_with = 'rejection' , mcmc_method = 'slice_np' , mcmc_parameters = None , rejection_sampling_parameters = None , sample_with_mcmc = None ) inherited \u00b6 Build posterior from the neural density estimator. For SNPE, the posterior distribution that is returned here implements the following functionality over the raw neural density estimator: correct the calculation of the log probability such that it compensates for the leakage. reject samples that lie outside of the prior bounds. alternatively, if leakage is very high (which can happen for multi-round SNPE), sample from the posterior with MCMC. Parameters: Name Type Description Default density_estimator Optional[Module] The density estimator that the posterior is based on. If None , use the latest neural density estimator that was trained. None sample_with str Method to use for sampling from the posterior. Must be one of [ rejection | mcmc ]. With default parameters, rejection samples from the posterior estimated by the neural net and rejects only if the samples are outside of the prior support. 'rejection' mcmc_method str Method used for MCMC sampling, one of slice_np , slice , hmc , nuts . Currently defaults to slice_np for a custom numpy implementation of slice sampling; select hmc , nuts or slice for Pyro-based sampling. 'slice_np' mcmc_parameters Optional[Dict[str, Any]] Dictionary overriding the default parameters for MCMC. The following parameters are supported: thin to set the thinning factor for the chain, warmup_steps to set the initial number of samples to discard, num_chains for the number of chains, init_strategy for the initialisation strategy for chains; prior will draw init locations from prior, whereas sir will use Sequential-Importance-Resampling using init_strategy_num_candidates to find init locations. None rejection_sampling_parameters Optional[Dict[str, Any]] Dictionary overriding the default parameters for rejection sampling. The following parameters are supported: proposal as the proposal distribtution (default is the trained neural net). max_sampling_batch_size as the batchsize of samples being drawn from the proposal at every iteration. num_samples_to_find_max as the number of samples that are used to find the maximum of the potential_fn / proposal ratio. num_iter_to_find_max as the number of gradient ascent iterations to find the maximum of that ratio. m as multiplier to that ratio. None sample_with_mcmc Optional[bool] Deprecated since sbi v0.16.0 . Use sample_with=mcmc instead. None Returns: Type Description DirectPosterior Posterior \\(p(\\theta|x)\\) with .sample() and .log_prob() methods. Source code in sbi/inference/snpe/snpe_c.py def build_posterior ( self , density_estimator : Optional [ TorchModule ] = None , sample_with : str = \"rejection\" , mcmc_method : str = \"slice_np\" , mcmc_parameters : Optional [ Dict [ str , Any ]] = None , rejection_sampling_parameters : Optional [ Dict [ str , Any ]] = None , sample_with_mcmc : Optional [ bool ] = None , ) -> DirectPosterior : r \"\"\" Build posterior from the neural density estimator. For SNPE, the posterior distribution that is returned here implements the following functionality over the raw neural density estimator: - correct the calculation of the log probability such that it compensates for the leakage. - reject samples that lie outside of the prior bounds. - alternatively, if leakage is very high (which can happen for multi-round SNPE), sample from the posterior with MCMC. Args: density_estimator: The density estimator that the posterior is based on. If `None`, use the latest neural density estimator that was trained. sample_with: Method to use for sampling from the posterior. Must be one of [`rejection` | `mcmc`]. With default parameters, `rejection` samples from the posterior estimated by the neural net and rejects only if the samples are outside of the prior support. mcmc_method: Method used for MCMC sampling, one of `slice_np`, `slice`, `hmc`, `nuts`. Currently defaults to `slice_np` for a custom numpy implementation of slice sampling; select `hmc`, `nuts` or `slice` for Pyro-based sampling. mcmc_parameters: Dictionary overriding the default parameters for MCMC. The following parameters are supported: `thin` to set the thinning factor for the chain, `warmup_steps` to set the initial number of samples to discard, `num_chains` for the number of chains, `init_strategy` for the initialisation strategy for chains; `prior` will draw init locations from prior, whereas `sir` will use Sequential-Importance-Resampling using `init_strategy_num_candidates` to find init locations. rejection_sampling_parameters: Dictionary overriding the default parameters for rejection sampling. The following parameters are supported: `proposal` as the proposal distribtution (default is the trained neural net). `max_sampling_batch_size` as the batchsize of samples being drawn from the proposal at every iteration. `num_samples_to_find_max` as the number of samples that are used to find the maximum of the `potential_fn / proposal` ratio. `num_iter_to_find_max` as the number of gradient ascent iterations to find the maximum of that ratio. `m` as multiplier to that ratio. sample_with_mcmc: Deprecated since `sbi v0.16.0`. Use `sample_with=mcmc` instead. Returns: Posterior $p(\\theta|x)$ with `.sample()` and `.log_prob()` methods. \"\"\" if sample_with_mcmc is not None : warn ( f \"You set `sample_with_mcmc= { sample_with_mcmc } `. This is deprecated \" \"since `sbi v0.17.0` and will lead to an error in future versions. \" \"Please use `sample_with='mcmc'` instead.\" ) if sample_with_mcmc : sample_with = \"mcmc\" if density_estimator is None : density_estimator = self . _neural_net # If internal net is used device is defined. device = self . _device else : # Otherwise, infer it from the device of the net parameters. device = next ( density_estimator . parameters ()) . device . type self . _posterior = DirectPosterior ( method_family = \"snpe\" , neural_net = density_estimator , prior = self . _prior , x_shape = self . _x_shape , sample_with = sample_with , mcmc_method = mcmc_method , mcmc_parameters = mcmc_parameters , rejection_sampling_parameters = rejection_sampling_parameters , device = device , ) self . _posterior . _num_trained_rounds = self . _round + 1 # Store models at end of each round. self . _model_bank . append ( deepcopy ( self . _posterior )) self . _model_bank [ - 1 ] . net . eval () return deepcopy ( self . _posterior ) get_dataloaders ( self , dataset , training_batch_size = 50 , validation_fraction = 0.1 , resume_training = False , dataloader_kwargs = None ) inherited \u00b6 Return dataloaders for training and validation. Parameters: Name Type Description Default dataset TensorDataset holding all theta and x, optionally masks. required training_batch_size int training arg of inference methods. 50 resume_training bool Whether the current call is resuming training so that no new training and validation indices into the dataset have to be created. False dataloader_kwargs Optional[dict] Additional or updated kwargs to be passed to the training and validation dataloaders (like, e.g., a collate_fn) None Returns: Type Description Tuple[torch.utils.data.dataloader.DataLoader, torch.utils.data.dataloader.DataLoader] Tuple of dataloaders for training and validation. Source code in sbi/inference/snpe/snpe_c.py def get_dataloaders ( self , dataset : data . TensorDataset , training_batch_size : int = 50 , validation_fraction : float = 0.1 , resume_training : bool = False , dataloader_kwargs : Optional [ dict ] = None , ) -> Tuple [ data . DataLoader , data . DataLoader ]: \"\"\"Return dataloaders for training and validation. Args: dataset: holding all theta and x, optionally masks. training_batch_size: training arg of inference methods. resume_training: Whether the current call is resuming training so that no new training and validation indices into the dataset have to be created. dataloader_kwargs: Additional or updated kwargs to be passed to the training and validation dataloaders (like, e.g., a collate_fn) Returns: Tuple of dataloaders for training and validation. \"\"\" # Get total number of training examples. num_examples = len ( dataset ) # Select random train and validation splits from (theta, x) pairs. num_training_examples = int (( 1 - validation_fraction ) * num_examples ) num_validation_examples = num_examples - num_training_examples if not resume_training : permuted_indices = torch . randperm ( num_examples ) self . train_indices , self . val_indices = ( permuted_indices [: num_training_examples ], permuted_indices [ num_training_examples :], ) # Create training and validation loaders using a subset sampler. # Intentionally use dicts to define the default dataloader args # Then, use dataloader_kwargs to override (or add to) any of these defaults # https://stackoverflow.com/questions/44784577/in-method-call-args-how-to-override-keyword-argument-of-unpacked-dict train_loader_kwargs = { \"batch_size\" : min ( training_batch_size , num_training_examples ), \"drop_last\" : True , \"sampler\" : data . sampler . SubsetRandomSampler ( self . train_indices ), } train_loader_kwargs = ( dict ( train_loader_kwargs , ** dataloader_kwargs ) if dataloader_kwargs is not None else train_loader_kwargs ) val_loader_kwargs = { \"batch_size\" : min ( training_batch_size , num_validation_examples ), \"shuffle\" : False , \"drop_last\" : True , \"sampler\" : data . sampler . SubsetRandomSampler ( self . val_indices ), } val_loader_kwargs = ( dict ( val_loader_kwargs , ** dataloader_kwargs ) if dataloader_kwargs is not None else val_loader_kwargs ) train_loader = data . DataLoader ( dataset , ** train_loader_kwargs ) val_loader = data . DataLoader ( dataset , ** val_loader_kwargs ) return train_loader , val_loader get_simulations ( self , starting_round = 0 , exclude_invalid_x = True , warn_on_invalid = True ) inherited \u00b6 Returns all \\(\\theta\\) , \\(x\\) , and prior_masks from rounds >= starting_round . If requested, do not return invalid data. Parameters: Name Type Description Default starting_round int The earliest round to return samples from (we start counting from zero). 0 exclude_invalid_x bool Whether to exclude simulation outputs x=NaN or x=\u00b1\u221e during training. True warn_on_invalid bool Whether to give out a warning if invalid simulations were found. True Returns: Parameters, simulation outputs, prior masks. Source code in sbi/inference/snpe/snpe_c.py def get_simulations ( self , starting_round : int = 0 , exclude_invalid_x : bool = True , warn_on_invalid : bool = True , ) -> Tuple [ Tensor , Tensor , Tensor ]: r \"\"\" Returns all $\\theta$, $x$, and prior_masks from rounds >= `starting_round`. If requested, do not return invalid data. Args: starting_round: The earliest round to return samples from (we start counting from zero). exclude_invalid_x: Whether to exclude simulation outputs `x=NaN` or `x=\u00b1\u221e` during training. warn_on_invalid: Whether to give out a warning if invalid simulations were found. Returns: Parameters, simulation outputs, prior masks. \"\"\" theta = get_simulations_since_round ( self . _theta_roundwise , self . _data_round_index , starting_round ) x = get_simulations_since_round ( self . _x_roundwise , self . _data_round_index , starting_round ) prior_masks = get_simulations_since_round ( self . _prior_masks , self . _data_round_index , starting_round ) # Check for NaNs in simulations. is_valid_x , num_nans , num_infs = handle_invalid_x ( x , exclude_invalid_x ) # Check for problematic z-scoring warn_if_zscoring_changes_data ( x ) if warn_on_invalid : warn_on_invalid_x ( num_nans , num_infs , exclude_invalid_x ) warn_on_invalid_x_for_snpec_leakage ( num_nans , num_infs , exclude_invalid_x , type ( self ) . __name__ , self . _round ) return theta [ is_valid_x ], x [ is_valid_x ], prior_masks [ is_valid_x ] provide_presimulated ( self , theta , x , from_round = 0 ) inherited \u00b6 Deprecated since sbi 0.14.0. Instead of using this, please use .append_simulations() . Please consult release notes to see how you can update your code: https://github.com/mackelab/sbi/releases/tag/v0.14.0 More information can be found under the corresponding pull request on github: https://github.com/mackelab/sbi/pull/378 and tutorials: https://www.mackelab.org/sbi/tutorial/02_flexible_interface/ Provide external \\(\\theta\\) and \\(x\\) to be used for training later on. Parameters: Name Type Description Default theta Tensor Parameter sets used to generate presimulated data. required x Tensor Simulation outputs of presimulated data. required from_round int Which round the data was simulated from. from_round=0 means that the data came from the first round, i.e. the prior. 0 Source code in sbi/inference/snpe/snpe_c.py def provide_presimulated ( self , theta : Tensor , x : Tensor , from_round : int = 0 ) -> None : r \"\"\" Deprecated since sbi 0.14.0. Instead of using this, please use `.append_simulations()`. Please consult release notes to see how you can update your code: https://github.com/mackelab/sbi/releases/tag/v0.14.0 More information can be found under the corresponding pull request on github: https://github.com/mackelab/sbi/pull/378 and tutorials: https://www.mackelab.org/sbi/tutorial/02_flexible_interface/ Provide external $\\theta$ and $x$ to be used for training later on. Args: theta: Parameter sets used to generate presimulated data. x: Simulation outputs of presimulated data. from_round: Which round the data was simulated from. `from_round=0` means that the data came from the first round, i.e. the prior. \"\"\" raise NameError ( f \"Deprecated since sbi 0.14.0. \" f \"Instead of using this, please use `.append_simulations()`. Please \" f \"consult release notes to see how you can update your code: \" f \"https://github.com/mackelab/sbi/releases/tag/v0.14.0\" f \"More information can be found under the corresponding pull request on \" f \"github: \" f \"https://github.com/mackelab/sbi/pull/378\" f \"and tutorials: \" f \"https://www.mackelab.org/sbi/tutorial/02_flexible_interface/\" , ) train ( self , num_atoms = 10 , training_batch_size = 50 , learning_rate = 0.0005 , validation_fraction = 0.1 , stop_after_epochs = 20 , max_num_epochs = None , clip_max_norm = 5.0 , calibration_kernel = None , exclude_invalid_x = True , resume_training = False , discard_prior_samples = False , use_combined_loss = False , retrain_from_scratch_each_round = False , show_train_summary = False , dataloader_kwargs = None ) \u00b6 Return density estimator that approximates the distribution \\(p(\\theta|x)\\) . Parameters: Name Type Description Default num_atoms int Number of atoms to use for classification. 10 training_batch_size int Training batch size. 50 learning_rate float Learning rate for Adam optimizer. 0.0005 validation_fraction float The fraction of data to use for validation. 0.1 stop_after_epochs int The number of epochs to wait for improvement on the validation set before terminating training. 20 max_num_epochs Optional[int] Maximum number of epochs to run. If reached, we stop training even when the validation loss is still decreasing. If None, we train until validation loss increases (see also stop_after_epochs ). None clip_max_norm Optional[float] Value at which to clip the total gradient norm in order to prevent exploding gradients. Use None for no clipping. 5.0 calibration_kernel Optional[Callable] A function to calibrate the loss with respect to the simulations x . See Lueckmann, Gon\u00e7alves et al., NeurIPS 2017. None exclude_invalid_x bool Whether to exclude simulation outputs x=NaN or x=\u00b1\u221e during training. Expect errors, silent or explicit, when False . True resume_training bool Can be used in case training time is limited, e.g. on a cluster. If True , the split between train and validation set, the optimizer, the number of epochs, and the best validation log-prob will be restored from the last time .train() was called. False discard_prior_samples bool Whether to discard samples simulated in round 1, i.e. from the prior. Training may be sped up by ignoring such less targeted samples. False use_combined_loss bool Whether to train the neural net also on prior samples using maximum likelihood in addition to training it on all samples using atomic loss. The extra MLE loss helps prevent density leaking with bounded priors. False retrain_from_scratch_each_round bool Whether to retrain the conditional density estimator for the posterior from scratch each round. False show_train_summary bool Whether to print the number of epochs and validation loss and leakage after the training. False dataloader_kwargs Optional[Dict] Additional or updated kwargs to be passed to the training and validation dataloaders (like, e.g., a collate_fn) None Returns: Type Description DirectPosterior Density estimator that approximates the distribution \\(p(\\theta|x)\\) . Source code in sbi/inference/snpe/snpe_c.py def train ( self , num_atoms : int = 10 , training_batch_size : int = 50 , learning_rate : float = 5e-4 , validation_fraction : float = 0.1 , stop_after_epochs : int = 20 , max_num_epochs : Optional [ int ] = None , clip_max_norm : Optional [ float ] = 5.0 , calibration_kernel : Optional [ Callable ] = None , exclude_invalid_x : bool = True , resume_training : bool = False , discard_prior_samples : bool = False , use_combined_loss : bool = False , retrain_from_scratch_each_round : bool = False , show_train_summary : bool = False , dataloader_kwargs : Optional [ Dict ] = None , ) -> DirectPosterior : r \"\"\" Return density estimator that approximates the distribution $p(\\theta|x)$. Args: num_atoms: Number of atoms to use for classification. training_batch_size: Training batch size. learning_rate: Learning rate for Adam optimizer. validation_fraction: The fraction of data to use for validation. stop_after_epochs: The number of epochs to wait for improvement on the validation set before terminating training. max_num_epochs: Maximum number of epochs to run. If reached, we stop training even when the validation loss is still decreasing. If None, we train until validation loss increases (see also `stop_after_epochs`). clip_max_norm: Value at which to clip the total gradient norm in order to prevent exploding gradients. Use None for no clipping. calibration_kernel: A function to calibrate the loss with respect to the simulations `x`. See Lueckmann, Gon\u00e7alves et al., NeurIPS 2017. exclude_invalid_x: Whether to exclude simulation outputs `x=NaN` or `x=\u00b1\u221e` during training. Expect errors, silent or explicit, when `False`. resume_training: Can be used in case training time is limited, e.g. on a cluster. If `True`, the split between train and validation set, the optimizer, the number of epochs, and the best validation log-prob will be restored from the last time `.train()` was called. discard_prior_samples: Whether to discard samples simulated in round 1, i.e. from the prior. Training may be sped up by ignoring such less targeted samples. use_combined_loss: Whether to train the neural net also on prior samples using maximum likelihood in addition to training it on all samples using atomic loss. The extra MLE loss helps prevent density leaking with bounded priors. retrain_from_scratch_each_round: Whether to retrain the conditional density estimator for the posterior from scratch each round. show_train_summary: Whether to print the number of epochs and validation loss and leakage after the training. dataloader_kwargs: Additional or updated kwargs to be passed to the training and validation dataloaders (like, e.g., a collate_fn) Returns: Density estimator that approximates the distribution $p(\\theta|x)$. \"\"\" # WARNING: sneaky trick ahead. We proxy the parent's `train` here, # requiring the signature to have `num_atoms`, save it for use below, and # continue. It's sneaky because we are using the object (self) as a namespace # to pass arguments between functions, and that's implicit state management. self . _num_atoms = num_atoms self . _use_combined_loss = use_combined_loss kwargs = del_entries ( locals (), entries = ( \"self\" , \"__class__\" , \"num_atoms\" , \"use_combined_loss\" ) ) self . _round = max ( self . _data_round_index ) if self . _round > 0 : # Set the proposal to the last proposal that was passed by the user. For # atomic SNPE, it does not matter what the proposal is. For non-atomic # SNPE, we only use the latest data that was passed, i.e. the one from the # last proposal. proposal = self . _proposal_roundwise [ - 1 ] if hasattr ( proposal , \"net\" ): self . use_non_atomic_loss = ( isinstance ( proposal . net . _distribution , mdn ) and isinstance ( self . _neural_net . _distribution , mdn ) and check_dist_class ( self . _prior , class_to_check = ( Uniform , MultivariateNormal ) )[ 0 ] ) else : self . use_non_atomic_loss = False algorithm = \"non-atomic\" if self . use_non_atomic_loss else \"atomic\" print ( f \"Using SNPE-C with { algorithm } loss\" ) if self . use_non_atomic_loss : # Take care of z-scoring, pre-compute and store prior terms. self . _set_state_for_mog_proposal () return super () . train ( ** kwargs ) sbi.inference.snle.snle_a.SNLE_A ( LikelihoodEstimator ) \u00b6 __init__ ( self , prior , density_estimator = 'maf' , device = 'cpu' , logging_level = 'WARNING' , summary_writer = None , show_progress_bars = True , ** unused_args ) special \u00b6 Sequential Neural Likelihood [1]. [1] Sequential Neural Likelihood: Fast Likelihood-free Inference with Autoregressive Flows_, Papamakarios et al., AISTATS 2019, https://arxiv.org/abs/1805.07226 Parameters: Name Type Description Default prior A probability distribution that expresses prior knowledge about the parameters, e.g. which ranges are meaningful for them. Any object with .log_prob() and .sample() (for example, a PyTorch distribution) can be used. required density_estimator Union[str, Callable] If it is a string, use a pre-configured network of the provided type (one of nsf, maf, mdn, made). Alternatively, a function that builds a custom neural network can be provided. The function will be called with the first batch of simulations (theta, x), which can thus be used for shape inference and potentially for z-scoring. It needs to return a PyTorch nn.Module implementing the density estimator. The density estimator needs to provide the methods .log_prob and .sample() . 'maf' device str Training device, e.g., \u201ccpu\u201d, \u201ccuda\u201d or \u201ccuda:{0, 1, \u2026}\u201d. 'cpu' logging_level Union[int, str] Minimum severity of messages to log. One of the strings INFO, WARNING, DEBUG, ERROR and CRITICAL. 'WARNING' summary_writer Optional[Writer] A tensorboard SummaryWriter to control, among others, log file location (default is <current working directory>/logs .) None show_progress_bars bool Whether to show a progressbar during simulation and sampling. True unused_args Absorbs additional arguments. No entries will be used. If it is not empty, we warn. In future versions, when the new interface of 0.14.0 is more mature, we will remove this argument. {} Source code in sbi/inference/snle/snle_a.py def __init__ ( self , prior , density_estimator : Union [ str , Callable ] = \"maf\" , device : str = \"cpu\" , logging_level : Union [ int , str ] = \"WARNING\" , summary_writer : Optional [ TensorboardSummaryWriter ] = None , show_progress_bars : bool = True , ** unused_args , ): r \"\"\"Sequential Neural Likelihood [1]. [1] Sequential Neural Likelihood: Fast Likelihood-free Inference with Autoregressive Flows_, Papamakarios et al., AISTATS 2019, https://arxiv.org/abs/1805.07226 Args: prior: A probability distribution that expresses prior knowledge about the parameters, e.g. which ranges are meaningful for them. Any object with `.log_prob()`and `.sample()` (for example, a PyTorch distribution) can be used. density_estimator: If it is a string, use a pre-configured network of the provided type (one of nsf, maf, mdn, made). Alternatively, a function that builds a custom neural network can be provided. The function will be called with the first batch of simulations (theta, x), which can thus be used for shape inference and potentially for z-scoring. It needs to return a PyTorch `nn.Module` implementing the density estimator. The density estimator needs to provide the methods `.log_prob` and `.sample()`. device: Training device, e.g., \"cpu\", \"cuda\" or \"cuda:{0, 1, ...}\". logging_level: Minimum severity of messages to log. One of the strings INFO, WARNING, DEBUG, ERROR and CRITICAL. summary_writer: A tensorboard `SummaryWriter` to control, among others, log file location (default is `<current working directory>/logs`.) show_progress_bars: Whether to show a progressbar during simulation and sampling. unused_args: Absorbs additional arguments. No entries will be used. If it is not empty, we warn. In future versions, when the new interface of 0.14.0 is more mature, we will remove this argument. \"\"\" kwargs = del_entries ( locals (), entries = ( \"self\" , \"__class__\" , \"unused_args\" )) super () . __init__ ( ** kwargs , ** unused_args ) append_simulations ( self , theta , x , from_round = 0 ) inherited \u00b6 Store parameters and simulation outputs to use them for later training. Data are stored as entries in lists for each type of variable (parameter/data). Stores \\(\\theta\\) , \\(x\\) , prior_masks (indicating if simulations are coming from the prior or not) and an index indicating which round the batch of simulations came from. Parameters: Name Type Description Default theta Tensor Parameter sets. required x Tensor Simulation outputs. required from_round int Which round the data stemmed from. Round 0 means from the prior. With default settings, this is not used at all for SNLE . Only when the user later on requests .train(discard_prior_samples=True) , we use these indices to find which training data stemmed from the prior. 0 Returns: Type Description LikelihoodEstimator NeuralInference object (returned so that this function is chainable). Source code in sbi/inference/snle/snle_a.py def append_simulations ( self , theta : Tensor , x : Tensor , from_round : int = 0 , ) -> \"LikelihoodEstimator\" : r \"\"\" Store parameters and simulation outputs to use them for later training. Data are stored as entries in lists for each type of variable (parameter/data). Stores $\\theta$, $x$, prior_masks (indicating if simulations are coming from the prior or not) and an index indicating which round the batch of simulations came from. Args: theta: Parameter sets. x: Simulation outputs. from_round: Which round the data stemmed from. Round 0 means from the prior. With default settings, this is not used at all for `SNLE`. Only when the user later on requests `.train(discard_prior_samples=True)`, we use these indices to find which training data stemmed from the prior. Returns: NeuralInference object (returned so that this function is chainable). \"\"\" theta , x = validate_theta_and_x ( theta , x , training_device = self . _device ) self . _theta_roundwise . append ( theta ) self . _x_roundwise . append ( x ) self . _prior_masks . append ( mask_sims_from_prior ( int ( from_round ), theta . size ( 0 ))) self . _data_round_index . append ( int ( from_round )) return self build_posterior ( self , density_estimator = None , sample_with = 'mcmc' , mcmc_method = 'slice_np' , mcmc_parameters = None , rejection_sampling_parameters = None ) inherited \u00b6 Build posterior from the neural density estimator. SNLE trains a neural network to approximate the likelihood \\(p(x|\\theta)\\) . The LikelihoodBasedPosterior class wraps the trained network such that one can directly evaluate the unnormalized posterior log probability \\(p(\\theta|x) \\propto p(x|\\theta) \\cdot p(\\theta)\\) and draw samples from the posterior with MCMC. Parameters: Name Type Description Default density_estimator Optional[Module] The density estimator that the posterior is based on. If None , use the latest neural density estimator that was trained. None sample_with str Method to use for sampling from the posterior. Must be one of [ mcmc | rejection ]. 'mcmc' mcmc_method str Method used for MCMC sampling, one of slice_np , slice , hmc , nuts . Currently defaults to slice_np for a custom numpy implementation of slice sampling; select hmc , nuts or slice for Pyro-based sampling. 'slice_np' mcmc_parameters Optional[Dict[str, Any]] Dictionary overriding the default parameters for MCMC. The following parameters are supported: thin to set the thinning factor for the chain, warmup_steps to set the initial number of samples to discard, num_chains for the number of chains, init_strategy for the initialisation strategy for chains; prior will draw init locations from prior, whereas sir will use Sequential-Importance-Resampling using init_strategy_num_candidates to find init locations. None rejection_sampling_parameters Optional[Dict[str, Any]] Dictionary overriding the default parameters for rejection sampling. The following parameters are supported: proposal as the proposal distribtution (default is the prior). max_sampling_batch_size as the batchsize of samples being drawn from the proposal at every iteration. num_samples_to_find_max as the number of samples that are used to find the maximum of the potential_fn / proposal ratio. num_iter_to_find_max as the number of gradient ascent iterations to find the maximum of that ratio. m as multiplier to that ratio. None Returns: Type Description LikelihoodBasedPosterior Posterior \\(p(\\theta|x)\\) with .sample() and .log_prob() methods (the returned log-probability is unnormalized). Source code in sbi/inference/snle/snle_a.py def build_posterior ( self , density_estimator : Optional [ TorchModule ] = None , sample_with : str = \"mcmc\" , mcmc_method : str = \"slice_np\" , mcmc_parameters : Optional [ Dict [ str , Any ]] = None , rejection_sampling_parameters : Optional [ Dict [ str , Any ]] = None , ) -> LikelihoodBasedPosterior : r \"\"\" Build posterior from the neural density estimator. SNLE trains a neural network to approximate the likelihood $p(x|\\theta)$. The `LikelihoodBasedPosterior` class wraps the trained network such that one can directly evaluate the unnormalized posterior log probability $p(\\theta|x) \\propto p(x|\\theta) \\cdot p(\\theta)$ and draw samples from the posterior with MCMC. Args: density_estimator: The density estimator that the posterior is based on. If `None`, use the latest neural density estimator that was trained. sample_with: Method to use for sampling from the posterior. Must be one of [`mcmc` | `rejection`]. mcmc_method: Method used for MCMC sampling, one of `slice_np`, `slice`, `hmc`, `nuts`. Currently defaults to `slice_np` for a custom numpy implementation of slice sampling; select `hmc`, `nuts` or `slice` for Pyro-based sampling. mcmc_parameters: Dictionary overriding the default parameters for MCMC. The following parameters are supported: `thin` to set the thinning factor for the chain, `warmup_steps` to set the initial number of samples to discard, `num_chains` for the number of chains, `init_strategy` for the initialisation strategy for chains; `prior` will draw init locations from prior, whereas `sir` will use Sequential-Importance-Resampling using `init_strategy_num_candidates` to find init locations. rejection_sampling_parameters: Dictionary overriding the default parameters for rejection sampling. The following parameters are supported: `proposal` as the proposal distribtution (default is the prior). `max_sampling_batch_size` as the batchsize of samples being drawn from the proposal at every iteration. `num_samples_to_find_max` as the number of samples that are used to find the maximum of the `potential_fn / proposal` ratio. `num_iter_to_find_max` as the number of gradient ascent iterations to find the maximum of that ratio. `m` as multiplier to that ratio. Returns: Posterior $p(\\theta|x)$ with `.sample()` and `.log_prob()` methods (the returned log-probability is unnormalized). \"\"\" if density_estimator is None : density_estimator = self . _neural_net # If internal net is used device is defined. device = self . _device else : # Otherwise, infer it from the device of the net parameters. device = next ( density_estimator . parameters ()) . device . type self . _posterior = LikelihoodBasedPosterior ( method_family = \"snle\" , neural_net = density_estimator , prior = self . _prior , x_shape = self . _x_shape , sample_with = sample_with , mcmc_method = mcmc_method , mcmc_parameters = mcmc_parameters , rejection_sampling_parameters = rejection_sampling_parameters , device = device , ) self . _posterior . _num_trained_rounds = self . _round + 1 # Store models at end of each round. self . _model_bank . append ( deepcopy ( self . _posterior )) self . _model_bank [ - 1 ] . net . eval () return deepcopy ( self . _posterior ) get_dataloaders ( self , dataset , training_batch_size = 50 , validation_fraction = 0.1 , resume_training = False , dataloader_kwargs = None ) inherited \u00b6 Return dataloaders for training and validation. Parameters: Name Type Description Default dataset TensorDataset holding all theta and x, optionally masks. required training_batch_size int training arg of inference methods. 50 resume_training bool Whether the current call is resuming training so that no new training and validation indices into the dataset have to be created. False dataloader_kwargs Optional[dict] Additional or updated kwargs to be passed to the training and validation dataloaders (like, e.g., a collate_fn) None Returns: Type Description Tuple[torch.utils.data.dataloader.DataLoader, torch.utils.data.dataloader.DataLoader] Tuple of dataloaders for training and validation. Source code in sbi/inference/snle/snle_a.py def get_dataloaders ( self , dataset : data . TensorDataset , training_batch_size : int = 50 , validation_fraction : float = 0.1 , resume_training : bool = False , dataloader_kwargs : Optional [ dict ] = None , ) -> Tuple [ data . DataLoader , data . DataLoader ]: \"\"\"Return dataloaders for training and validation. Args: dataset: holding all theta and x, optionally masks. training_batch_size: training arg of inference methods. resume_training: Whether the current call is resuming training so that no new training and validation indices into the dataset have to be created. dataloader_kwargs: Additional or updated kwargs to be passed to the training and validation dataloaders (like, e.g., a collate_fn) Returns: Tuple of dataloaders for training and validation. \"\"\" # Get total number of training examples. num_examples = len ( dataset ) # Select random train and validation splits from (theta, x) pairs. num_training_examples = int (( 1 - validation_fraction ) * num_examples ) num_validation_examples = num_examples - num_training_examples if not resume_training : permuted_indices = torch . randperm ( num_examples ) self . train_indices , self . val_indices = ( permuted_indices [: num_training_examples ], permuted_indices [ num_training_examples :], ) # Create training and validation loaders using a subset sampler. # Intentionally use dicts to define the default dataloader args # Then, use dataloader_kwargs to override (or add to) any of these defaults # https://stackoverflow.com/questions/44784577/in-method-call-args-how-to-override-keyword-argument-of-unpacked-dict train_loader_kwargs = { \"batch_size\" : min ( training_batch_size , num_training_examples ), \"drop_last\" : True , \"sampler\" : data . sampler . SubsetRandomSampler ( self . train_indices ), } train_loader_kwargs = ( dict ( train_loader_kwargs , ** dataloader_kwargs ) if dataloader_kwargs is not None else train_loader_kwargs ) val_loader_kwargs = { \"batch_size\" : min ( training_batch_size , num_validation_examples ), \"shuffle\" : False , \"drop_last\" : True , \"sampler\" : data . sampler . SubsetRandomSampler ( self . val_indices ), } val_loader_kwargs = ( dict ( val_loader_kwargs , ** dataloader_kwargs ) if dataloader_kwargs is not None else val_loader_kwargs ) train_loader = data . DataLoader ( dataset , ** train_loader_kwargs ) val_loader = data . DataLoader ( dataset , ** val_loader_kwargs ) return train_loader , val_loader get_simulations ( self , starting_round = 0 , exclude_invalid_x = True , warn_on_invalid = True ) inherited \u00b6 Returns all \\(\\theta\\) , \\(x\\) , and prior_masks from rounds >= starting_round . If requested, do not return invalid data. Parameters: Name Type Description Default starting_round int The earliest round to return samples from (we start counting from zero). 0 exclude_invalid_x bool Whether to exclude simulation outputs x=NaN or x=\u00b1\u221e during training. True warn_on_invalid bool Whether to give out a warning if invalid simulations were found. True Returns: Parameters, simulation outputs, prior masks. Source code in sbi/inference/snle/snle_a.py def get_simulations ( self , starting_round : int = 0 , exclude_invalid_x : bool = True , warn_on_invalid : bool = True , ) -> Tuple [ Tensor , Tensor , Tensor ]: r \"\"\" Returns all $\\theta$, $x$, and prior_masks from rounds >= `starting_round`. If requested, do not return invalid data. Args: starting_round: The earliest round to return samples from (we start counting from zero). exclude_invalid_x: Whether to exclude simulation outputs `x=NaN` or `x=\u00b1\u221e` during training. warn_on_invalid: Whether to give out a warning if invalid simulations were found. Returns: Parameters, simulation outputs, prior masks. \"\"\" theta = get_simulations_since_round ( self . _theta_roundwise , self . _data_round_index , starting_round ) x = get_simulations_since_round ( self . _x_roundwise , self . _data_round_index , starting_round ) prior_masks = get_simulations_since_round ( self . _prior_masks , self . _data_round_index , starting_round ) # Check for NaNs in simulations. is_valid_x , num_nans , num_infs = handle_invalid_x ( x , exclude_invalid_x ) # Check for problematic z-scoring warn_if_zscoring_changes_data ( x ) if warn_on_invalid : warn_on_invalid_x ( num_nans , num_infs , exclude_invalid_x ) warn_on_invalid_x_for_snpec_leakage ( num_nans , num_infs , exclude_invalid_x , type ( self ) . __name__ , self . _round ) return theta [ is_valid_x ], x [ is_valid_x ], prior_masks [ is_valid_x ] provide_presimulated ( self , theta , x , from_round = 0 ) inherited \u00b6 Deprecated since sbi 0.14.0. Instead of using this, please use .append_simulations() . Please consult release notes to see how you can update your code: https://github.com/mackelab/sbi/releases/tag/v0.14.0 More information can be found under the corresponding pull request on github: https://github.com/mackelab/sbi/pull/378 and tutorials: https://www.mackelab.org/sbi/tutorial/02_flexible_interface/ Provide external \\(\\theta\\) and \\(x\\) to be used for training later on. Parameters: Name Type Description Default theta Tensor Parameter sets used to generate presimulated data. required x Tensor Simulation outputs of presimulated data. required from_round int Which round the data was simulated from. from_round=0 means that the data came from the first round, i.e. the prior. 0 Source code in sbi/inference/snle/snle_a.py def provide_presimulated ( self , theta : Tensor , x : Tensor , from_round : int = 0 ) -> None : r \"\"\" Deprecated since sbi 0.14.0. Instead of using this, please use `.append_simulations()`. Please consult release notes to see how you can update your code: https://github.com/mackelab/sbi/releases/tag/v0.14.0 More information can be found under the corresponding pull request on github: https://github.com/mackelab/sbi/pull/378 and tutorials: https://www.mackelab.org/sbi/tutorial/02_flexible_interface/ Provide external $\\theta$ and $x$ to be used for training later on. Args: theta: Parameter sets used to generate presimulated data. x: Simulation outputs of presimulated data. from_round: Which round the data was simulated from. `from_round=0` means that the data came from the first round, i.e. the prior. \"\"\" raise NameError ( f \"Deprecated since sbi 0.14.0. \" f \"Instead of using this, please use `.append_simulations()`. Please \" f \"consult release notes to see how you can update your code: \" f \"https://github.com/mackelab/sbi/releases/tag/v0.14.0\" f \"More information can be found under the corresponding pull request on \" f \"github: \" f \"https://github.com/mackelab/sbi/pull/378\" f \"and tutorials: \" f \"https://www.mackelab.org/sbi/tutorial/02_flexible_interface/\" , ) train ( self , training_batch_size = 50 , learning_rate = 0.0005 , validation_fraction = 0.1 , stop_after_epochs = 20 , max_num_epochs = None , clip_max_norm = 5.0 , exclude_invalid_x = True , resume_training = False , discard_prior_samples = False , retrain_from_scratch_each_round = False , show_train_summary = False , dataloader_kwargs = None ) \u00b6 Return density estimator that approximates the distribution \\(p(x|\\theta)\\) . Parameters: Name Type Description Default training_batch_size int Training batch size. 50 learning_rate float Learning rate for Adam optimizer. 0.0005 validation_fraction float The fraction of data to use for validation. 0.1 stop_after_epochs int The number of epochs to wait for improvement on the validation set before terminating training. 20 max_num_epochs Optional[int] Maximum number of epochs to run. If reached, we stop training even when the validation loss is still decreasing. If None, we train until validation loss increases (see also stop_after_epochs ). None clip_max_norm Optional[float] Value at which to clip the total gradient norm in order to prevent exploding gradients. Use None for no clipping. 5.0 exclude_invalid_x bool Whether to exclude simulation outputs x=NaN or x=\u00b1\u221e during training. Expect errors, silent or explicit, when False . True resume_training bool Can be used in case training time is limited, e.g. on a cluster. If True , the split between train and validation set, the optimizer, the number of epochs, and the best validation log-prob will be restored from the last time .train() was called. False discard_prior_samples bool Whether to discard samples simulated in round 1, i.e. from the prior. Training may be sped up by ignoring such less targeted samples. False retrain_from_scratch_each_round bool Whether to retrain the conditional density estimator for the posterior from scratch each round. False show_train_summary bool Whether to print the number of epochs and validation loss and leakage after the training. False dataloader_kwargs Optional[Dict] Additional or updated kwargs to be passed to the training and validation dataloaders (like, e.g., a collate_fn) None Returns: Type Description NeuralPosterior Density estimator that approximates the distribution \\(p(x|\\theta)\\) . Source code in sbi/inference/snle/snle_a.py def train ( self , training_batch_size : int = 50 , learning_rate : float = 5e-4 , validation_fraction : float = 0.1 , stop_after_epochs : int = 20 , max_num_epochs : Optional [ int ] = None , clip_max_norm : Optional [ float ] = 5.0 , exclude_invalid_x : bool = True , resume_training : bool = False , discard_prior_samples : bool = False , retrain_from_scratch_each_round : bool = False , show_train_summary : bool = False , dataloader_kwargs : Optional [ Dict ] = None , ) -> NeuralPosterior : r \"\"\" Return density estimator that approximates the distribution $p(x|\\theta)$. Args: training_batch_size: Training batch size. learning_rate: Learning rate for Adam optimizer. validation_fraction: The fraction of data to use for validation. stop_after_epochs: The number of epochs to wait for improvement on the validation set before terminating training. max_num_epochs: Maximum number of epochs to run. If reached, we stop training even when the validation loss is still decreasing. If None, we train until validation loss increases (see also `stop_after_epochs`). clip_max_norm: Value at which to clip the total gradient norm in order to prevent exploding gradients. Use None for no clipping. exclude_invalid_x: Whether to exclude simulation outputs `x=NaN` or `x=\u00b1\u221e` during training. Expect errors, silent or explicit, when `False`. resume_training: Can be used in case training time is limited, e.g. on a cluster. If `True`, the split between train and validation set, the optimizer, the number of epochs, and the best validation log-prob will be restored from the last time `.train()` was called. discard_prior_samples: Whether to discard samples simulated in round 1, i.e. from the prior. Training may be sped up by ignoring such less targeted samples. retrain_from_scratch_each_round: Whether to retrain the conditional density estimator for the posterior from scratch each round. show_train_summary: Whether to print the number of epochs and validation loss and leakage after the training. dataloader_kwargs: Additional or updated kwargs to be passed to the training and validation dataloaders (like, e.g., a collate_fn) Returns: Density estimator that approximates the distribution $p(x|\\theta)$. \"\"\" kwargs = del_entries ( locals (), entries = ( \"self\" , \"__class__\" )) return super () . train ( ** kwargs ) sbi.inference.snre.snre_a.SNRE_A ( RatioEstimator ) \u00b6 __init__ ( self , prior , classifier = 'resnet' , device = 'cpu' , logging_level = 'warning' , summary_writer = None , show_progress_bars = True , ** unused_args ) special \u00b6 AALR[1], here known as SNRE_A. [1] Likelihood-free MCMC with Amortized Approximate Likelihood Ratios , Hermans et al., ICML 2020, https://arxiv.org/abs/1903.04057 Parameters: Name Type Description Default prior A probability distribution that expresses prior knowledge about the parameters, e.g. which ranges are meaningful for them. Any object with .log_prob() and .sample() (for example, a PyTorch distribution) can be used. required classifier Union[str, Callable] Classifier trained to approximate likelihood ratios. If it is a string, use a pre-configured network of the provided type (one of linear, mlp, resnet). Alternatively, a function that builds a custom neural network can be provided. The function will be called with the first batch of simulations (theta, x), which can thus be used for shape inference and potentially for z-scoring. It needs to return a PyTorch nn.Module implementing the classifier. 'resnet' device str Training device, e.g., \u201ccpu\u201d, \u201ccuda\u201d or \u201ccuda:{0, 1, \u2026}\u201d. 'cpu' logging_level Union[int, str] Minimum severity of messages to log. One of the strings INFO, WARNING, DEBUG, ERROR and CRITICAL. 'warning' summary_writer Optional[Writer] A tensorboard SummaryWriter to control, among others, log file location (default is <current working directory>/logs .) None show_progress_bars bool Whether to show a progressbar during simulation and sampling. True unused_args Absorbs additional arguments. No entries will be used. If it is not empty, we warn. In future versions, when the new interface of 0.14.0 is more mature, we will remove this argument. {} Source code in sbi/inference/snre/snre_a.py def __init__ ( self , prior , classifier : Union [ str , Callable ] = \"resnet\" , device : str = \"cpu\" , logging_level : Union [ int , str ] = \"warning\" , summary_writer : Optional [ TensorboardSummaryWriter ] = None , show_progress_bars : bool = True , ** unused_args ): r \"\"\"AALR[1], here known as SNRE_A. [1] _Likelihood-free MCMC with Amortized Approximate Likelihood Ratios_, Hermans et al., ICML 2020, https://arxiv.org/abs/1903.04057 Args: prior: A probability distribution that expresses prior knowledge about the parameters, e.g. which ranges are meaningful for them. Any object with `.log_prob()`and `.sample()` (for example, a PyTorch distribution) can be used. classifier: Classifier trained to approximate likelihood ratios. If it is a string, use a pre-configured network of the provided type (one of linear, mlp, resnet). Alternatively, a function that builds a custom neural network can be provided. The function will be called with the first batch of simulations (theta, x), which can thus be used for shape inference and potentially for z-scoring. It needs to return a PyTorch `nn.Module` implementing the classifier. device: Training device, e.g., \"cpu\", \"cuda\" or \"cuda:{0, 1, ...}\". logging_level: Minimum severity of messages to log. One of the strings INFO, WARNING, DEBUG, ERROR and CRITICAL. summary_writer: A tensorboard `SummaryWriter` to control, among others, log file location (default is `<current working directory>/logs`.) show_progress_bars: Whether to show a progressbar during simulation and sampling. unused_args: Absorbs additional arguments. No entries will be used. If it is not empty, we warn. In future versions, when the new interface of 0.14.0 is more mature, we will remove this argument. \"\"\" kwargs = del_entries ( locals (), entries = ( \"self\" , \"__class__\" , \"unused_args\" )) super () . __init__ ( ** kwargs , ** unused_args ) append_simulations ( self , theta , x , from_round = 0 ) inherited \u00b6 Store parameters and simulation outputs to use them for later training. Data are stored as entries in lists for each type of variable (parameter/data). Stores \\(\\theta\\) , \\(x\\) , prior_masks (indicating if simulations are coming from the prior or not) and an index indicating which round the batch of simulations came from. Parameters: Name Type Description Default theta Tensor Parameter sets. required x Tensor Simulation outputs. required from_round int Which round the data stemmed from. Round 0 means from the prior. With default settings, this is not used at all for SNRE . Only when the user later on requests .train(discard_prior_samples=True) , we use these indices to find which training data stemmed from the prior. 0 Returns: Type Description RatioEstimator NeuralInference object (returned so that this function is chainable). Source code in sbi/inference/snre/snre_a.py def append_simulations ( self , theta : Tensor , x : Tensor , from_round : int = 0 , ) -> \"RatioEstimator\" : r \"\"\" Store parameters and simulation outputs to use them for later training. Data are stored as entries in lists for each type of variable (parameter/data). Stores $\\theta$, $x$, prior_masks (indicating if simulations are coming from the prior or not) and an index indicating which round the batch of simulations came from. Args: theta: Parameter sets. x: Simulation outputs. from_round: Which round the data stemmed from. Round 0 means from the prior. With default settings, this is not used at all for `SNRE`. Only when the user later on requests `.train(discard_prior_samples=True)`, we use these indices to find which training data stemmed from the prior. Returns: NeuralInference object (returned so that this function is chainable). \"\"\" theta , x = validate_theta_and_x ( theta , x , training_device = self . _device ) self . _theta_roundwise . append ( theta ) self . _x_roundwise . append ( x ) self . _prior_masks . append ( mask_sims_from_prior ( int ( from_round ), theta . size ( 0 ))) self . _data_round_index . append ( int ( from_round )) return self build_posterior ( self , density_estimator = None , sample_with = 'mcmc' , mcmc_method = 'slice_np' , mcmc_parameters = None , rejection_sampling_parameters = None ) inherited \u00b6 Build posterior from the neural density estimator. SNRE trains a neural network to approximate likelihood ratios, which in turn can be used obtain an unnormalized posterior \\(p(\\theta|x) \\propto p(x|\\theta) \\cdot p(\\theta)\\) . The posterior returned here wraps the trained network such that one can directly evaluate the unnormalized posterior log-probability \\(p(\\theta|x) \\propto p(x|\\theta) \\cdot p(\\theta)\\) and draw samples from the posterior with MCMC. Note that, in the case of single-round SNRE_A / AALR, it is possible to evaluate the log-probability of the normalized posterior, but sampling still requires MCMC. Parameters: Name Type Description Default density_estimator Optional[Module] The density estimator that the posterior is based on. If None , use the latest neural density estimator that was trained. None sample_with str Method to use for sampling from the posterior. Must be one of [ mcmc | rejection ]. 'mcmc' mcmc_method str Method used for MCMC sampling, one of slice_np , slice , hmc , nuts . Currently defaults to slice_np for a custom numpy implementation of slice sampling; select hmc , nuts or slice for Pyro-based sampling. 'slice_np' mcmc_parameters Optional[Dict[str, Any]] Dictionary overriding the default parameters for MCMC. The following parameters are supported: thin to set the thinning factor for the chain, warmup_steps to set the initial number of samples to discard, num_chains for the number of chains, init_strategy for the initialisation strategy for chains; prior will draw init locations from prior, whereas sir will use Sequential-Importance-Resampling using init_strategy_num_candidates to find init locations. None rejection_sampling_parameters Optional[Dict[str, Any]] Dictionary overriding the default parameters for rejection sampling. The following parameters are supported: proposal as the proposal distribtution (default is the prior). max_sampling_batch_size as the batchsize of samples being drawn from the proposal at every iteration. num_samples_to_find_max as the number of samples that are used to find the maximum of the potential_fn / proposal ratio. num_iter_to_find_max as the number of gradient ascent iterations to find the maximum of that ratio. m as multiplier to that ratio. None Returns: Type Description RatioBasedPosterior Posterior \\(p(\\theta|x)\\) with .sample() and .log_prob() methods (the returned log-probability is unnormalized). Source code in sbi/inference/snre/snre_a.py def build_posterior ( self , density_estimator : Optional [ TorchModule ] = None , sample_with : str = \"mcmc\" , mcmc_method : str = \"slice_np\" , mcmc_parameters : Optional [ Dict [ str , Any ]] = None , rejection_sampling_parameters : Optional [ Dict [ str , Any ]] = None , ) -> RatioBasedPosterior : r \"\"\" Build posterior from the neural density estimator. SNRE trains a neural network to approximate likelihood ratios, which in turn can be used obtain an unnormalized posterior $p(\\theta|x) \\propto p(x|\\theta) \\cdot p(\\theta)$. The posterior returned here wraps the trained network such that one can directly evaluate the unnormalized posterior log-probability $p(\\theta|x) \\propto p(x|\\theta) \\cdot p(\\theta)$ and draw samples from the posterior with MCMC. Note that, in the case of single-round SNRE_A / AALR, it is possible to evaluate the log-probability of the **normalized** posterior, but sampling still requires MCMC. Args: density_estimator: The density estimator that the posterior is based on. If `None`, use the latest neural density estimator that was trained. sample_with: Method to use for sampling from the posterior. Must be one of [`mcmc` | `rejection`]. mcmc_method: Method used for MCMC sampling, one of `slice_np`, `slice`, `hmc`, `nuts`. Currently defaults to `slice_np` for a custom numpy implementation of slice sampling; select `hmc`, `nuts` or `slice` for Pyro-based sampling. mcmc_parameters: Dictionary overriding the default parameters for MCMC. The following parameters are supported: `thin` to set the thinning factor for the chain, `warmup_steps` to set the initial number of samples to discard, `num_chains` for the number of chains, `init_strategy` for the initialisation strategy for chains; `prior` will draw init locations from prior, whereas `sir` will use Sequential-Importance-Resampling using `init_strategy_num_candidates` to find init locations. rejection_sampling_parameters: Dictionary overriding the default parameters for rejection sampling. The following parameters are supported: `proposal` as the proposal distribtution (default is the prior). `max_sampling_batch_size` as the batchsize of samples being drawn from the proposal at every iteration. `num_samples_to_find_max` as the number of samples that are used to find the maximum of the `potential_fn / proposal` ratio. `num_iter_to_find_max` as the number of gradient ascent iterations to find the maximum of that ratio. `m` as multiplier to that ratio. Returns: Posterior $p(\\theta|x)$ with `.sample()` and `.log_prob()` methods (the returned log-probability is unnormalized). \"\"\" if density_estimator is None : density_estimator = self . _neural_net # If internal net is used device is defined. device = self . _device else : # Otherwise, infer it from the device of the net parameters. device = next ( density_estimator . parameters ()) . device . type self . _posterior = RatioBasedPosterior ( method_family = self . __class__ . __name__ . lower (), neural_net = density_estimator , prior = self . _prior , x_shape = self . _x_shape , sample_with = sample_with , mcmc_method = mcmc_method , mcmc_parameters = mcmc_parameters , rejection_sampling_parameters = rejection_sampling_parameters , device = device , ) self . _posterior . _num_trained_rounds = self . _round + 1 # Store models at end of each round. self . _model_bank . append ( deepcopy ( self . _posterior )) self . _model_bank [ - 1 ] . net . eval () return deepcopy ( self . _posterior ) get_dataloaders ( self , dataset , training_batch_size = 50 , validation_fraction = 0.1 , resume_training = False , dataloader_kwargs = None ) inherited \u00b6 Return dataloaders for training and validation. Parameters: Name Type Description Default dataset TensorDataset holding all theta and x, optionally masks. required training_batch_size int training arg of inference methods. 50 resume_training bool Whether the current call is resuming training so that no new training and validation indices into the dataset have to be created. False dataloader_kwargs Optional[dict] Additional or updated kwargs to be passed to the training and validation dataloaders (like, e.g., a collate_fn) None Returns: Type Description Tuple[torch.utils.data.dataloader.DataLoader, torch.utils.data.dataloader.DataLoader] Tuple of dataloaders for training and validation. Source code in sbi/inference/snre/snre_a.py def get_dataloaders ( self , dataset : data . TensorDataset , training_batch_size : int = 50 , validation_fraction : float = 0.1 , resume_training : bool = False , dataloader_kwargs : Optional [ dict ] = None , ) -> Tuple [ data . DataLoader , data . DataLoader ]: \"\"\"Return dataloaders for training and validation. Args: dataset: holding all theta and x, optionally masks. training_batch_size: training arg of inference methods. resume_training: Whether the current call is resuming training so that no new training and validation indices into the dataset have to be created. dataloader_kwargs: Additional or updated kwargs to be passed to the training and validation dataloaders (like, e.g., a collate_fn) Returns: Tuple of dataloaders for training and validation. \"\"\" # Get total number of training examples. num_examples = len ( dataset ) # Select random train and validation splits from (theta, x) pairs. num_training_examples = int (( 1 - validation_fraction ) * num_examples ) num_validation_examples = num_examples - num_training_examples if not resume_training : permuted_indices = torch . randperm ( num_examples ) self . train_indices , self . val_indices = ( permuted_indices [: num_training_examples ], permuted_indices [ num_training_examples :], ) # Create training and validation loaders using a subset sampler. # Intentionally use dicts to define the default dataloader args # Then, use dataloader_kwargs to override (or add to) any of these defaults # https://stackoverflow.com/questions/44784577/in-method-call-args-how-to-override-keyword-argument-of-unpacked-dict train_loader_kwargs = { \"batch_size\" : min ( training_batch_size , num_training_examples ), \"drop_last\" : True , \"sampler\" : data . sampler . SubsetRandomSampler ( self . train_indices ), } train_loader_kwargs = ( dict ( train_loader_kwargs , ** dataloader_kwargs ) if dataloader_kwargs is not None else train_loader_kwargs ) val_loader_kwargs = { \"batch_size\" : min ( training_batch_size , num_validation_examples ), \"shuffle\" : False , \"drop_last\" : True , \"sampler\" : data . sampler . SubsetRandomSampler ( self . val_indices ), } val_loader_kwargs = ( dict ( val_loader_kwargs , ** dataloader_kwargs ) if dataloader_kwargs is not None else val_loader_kwargs ) train_loader = data . DataLoader ( dataset , ** train_loader_kwargs ) val_loader = data . DataLoader ( dataset , ** val_loader_kwargs ) return train_loader , val_loader get_simulations ( self , starting_round = 0 , exclude_invalid_x = True , warn_on_invalid = True ) inherited \u00b6 Returns all \\(\\theta\\) , \\(x\\) , and prior_masks from rounds >= starting_round . If requested, do not return invalid data. Parameters: Name Type Description Default starting_round int The earliest round to return samples from (we start counting from zero). 0 exclude_invalid_x bool Whether to exclude simulation outputs x=NaN or x=\u00b1\u221e during training. True warn_on_invalid bool Whether to give out a warning if invalid simulations were found. True Returns: Parameters, simulation outputs, prior masks. Source code in sbi/inference/snre/snre_a.py def get_simulations ( self , starting_round : int = 0 , exclude_invalid_x : bool = True , warn_on_invalid : bool = True , ) -> Tuple [ Tensor , Tensor , Tensor ]: r \"\"\" Returns all $\\theta$, $x$, and prior_masks from rounds >= `starting_round`. If requested, do not return invalid data. Args: starting_round: The earliest round to return samples from (we start counting from zero). exclude_invalid_x: Whether to exclude simulation outputs `x=NaN` or `x=\u00b1\u221e` during training. warn_on_invalid: Whether to give out a warning if invalid simulations were found. Returns: Parameters, simulation outputs, prior masks. \"\"\" theta = get_simulations_since_round ( self . _theta_roundwise , self . _data_round_index , starting_round ) x = get_simulations_since_round ( self . _x_roundwise , self . _data_round_index , starting_round ) prior_masks = get_simulations_since_round ( self . _prior_masks , self . _data_round_index , starting_round ) # Check for NaNs in simulations. is_valid_x , num_nans , num_infs = handle_invalid_x ( x , exclude_invalid_x ) # Check for problematic z-scoring warn_if_zscoring_changes_data ( x ) if warn_on_invalid : warn_on_invalid_x ( num_nans , num_infs , exclude_invalid_x ) warn_on_invalid_x_for_snpec_leakage ( num_nans , num_infs , exclude_invalid_x , type ( self ) . __name__ , self . _round ) return theta [ is_valid_x ], x [ is_valid_x ], prior_masks [ is_valid_x ] provide_presimulated ( self , theta , x , from_round = 0 ) inherited \u00b6 Deprecated since sbi 0.14.0. Instead of using this, please use .append_simulations() . Please consult release notes to see how you can update your code: https://github.com/mackelab/sbi/releases/tag/v0.14.0 More information can be found under the corresponding pull request on github: https://github.com/mackelab/sbi/pull/378 and tutorials: https://www.mackelab.org/sbi/tutorial/02_flexible_interface/ Provide external \\(\\theta\\) and \\(x\\) to be used for training later on. Parameters: Name Type Description Default theta Tensor Parameter sets used to generate presimulated data. required x Tensor Simulation outputs of presimulated data. required from_round int Which round the data was simulated from. from_round=0 means that the data came from the first round, i.e. the prior. 0 Source code in sbi/inference/snre/snre_a.py def provide_presimulated ( self , theta : Tensor , x : Tensor , from_round : int = 0 ) -> None : r \"\"\" Deprecated since sbi 0.14.0. Instead of using this, please use `.append_simulations()`. Please consult release notes to see how you can update your code: https://github.com/mackelab/sbi/releases/tag/v0.14.0 More information can be found under the corresponding pull request on github: https://github.com/mackelab/sbi/pull/378 and tutorials: https://www.mackelab.org/sbi/tutorial/02_flexible_interface/ Provide external $\\theta$ and $x$ to be used for training later on. Args: theta: Parameter sets used to generate presimulated data. x: Simulation outputs of presimulated data. from_round: Which round the data was simulated from. `from_round=0` means that the data came from the first round, i.e. the prior. \"\"\" raise NameError ( f \"Deprecated since sbi 0.14.0. \" f \"Instead of using this, please use `.append_simulations()`. Please \" f \"consult release notes to see how you can update your code: \" f \"https://github.com/mackelab/sbi/releases/tag/v0.14.0\" f \"More information can be found under the corresponding pull request on \" f \"github: \" f \"https://github.com/mackelab/sbi/pull/378\" f \"and tutorials: \" f \"https://www.mackelab.org/sbi/tutorial/02_flexible_interface/\" , ) train ( self , training_batch_size = 50 , learning_rate = 0.0005 , validation_fraction = 0.1 , stop_after_epochs = 20 , max_num_epochs = None , clip_max_norm = 5.0 , exclude_invalid_x = True , resume_training = False , discard_prior_samples = False , retrain_from_scratch_each_round = False , show_train_summary = False , dataloader_kwargs = None ) \u00b6 Return classifier that approximates the ratio \\(p(\\theta,x)/p(\\theta)p(x)\\) . Parameters: Name Type Description Default training_batch_size int Training batch size. 50 learning_rate float Learning rate for Adam optimizer. 0.0005 validation_fraction float The fraction of data to use for validation. 0.1 stop_after_epochs int The number of epochs to wait for improvement on the validation set before terminating training. 20 max_num_epochs Optional[int] Maximum number of epochs to run. If reached, we stop training even when the validation loss is still decreasing. If None, we train until validation loss increases (see also stop_after_epochs ). None clip_max_norm Optional[float] Value at which to clip the total gradient norm in order to prevent exploding gradients. Use None for no clipping. 5.0 exclude_invalid_x bool Whether to exclude simulation outputs x=NaN or x=\u00b1\u221e during training. Expect errors, silent or explicit, when False . True resume_training bool Can be used in case training time is limited, e.g. on a cluster. If True , the split between train and validation set, the optimizer, the number of epochs, and the best validation log-prob will be restored from the last time .train() was called. False discard_prior_samples bool Whether to discard samples simulated in round 1, i.e. from the prior. Training may be sped up by ignoring such less targeted samples. False retrain_from_scratch_each_round bool Whether to retrain the conditional density estimator for the posterior from scratch each round. False show_train_summary bool Whether to print the number of epochs and validation loss and leakage after the training. False dataloader_kwargs Optional[Dict] Additional or updated kwargs to be passed to the training and validation dataloaders (like, e.g., a collate_fn) None Returns: Type Description NeuralPosterior Classifier that approximates the ratio \\(p(\\theta,x)/p(\\theta)p(x)\\) . Source code in sbi/inference/snre/snre_a.py def train ( self , training_batch_size : int = 50 , learning_rate : float = 5e-4 , validation_fraction : float = 0.1 , stop_after_epochs : int = 20 , max_num_epochs : Optional [ int ] = None , clip_max_norm : Optional [ float ] = 5.0 , exclude_invalid_x : bool = True , resume_training : bool = False , discard_prior_samples : bool = False , retrain_from_scratch_each_round : bool = False , show_train_summary : bool = False , dataloader_kwargs : Optional [ Dict ] = None , ) -> NeuralPosterior : r \"\"\" Return classifier that approximates the ratio $p(\\theta,x)/p(\\theta)p(x)$. Args: training_batch_size: Training batch size. learning_rate: Learning rate for Adam optimizer. validation_fraction: The fraction of data to use for validation. stop_after_epochs: The number of epochs to wait for improvement on the validation set before terminating training. max_num_epochs: Maximum number of epochs to run. If reached, we stop training even when the validation loss is still decreasing. If None, we train until validation loss increases (see also `stop_after_epochs`). clip_max_norm: Value at which to clip the total gradient norm in order to prevent exploding gradients. Use None for no clipping. exclude_invalid_x: Whether to exclude simulation outputs `x=NaN` or `x=\u00b1\u221e` during training. Expect errors, silent or explicit, when `False`. resume_training: Can be used in case training time is limited, e.g. on a cluster. If `True`, the split between train and validation set, the optimizer, the number of epochs, and the best validation log-prob will be restored from the last time `.train()` was called. discard_prior_samples: Whether to discard samples simulated in round 1, i.e. from the prior. Training may be sped up by ignoring such less targeted samples. retrain_from_scratch_each_round: Whether to retrain the conditional density estimator for the posterior from scratch each round. show_train_summary: Whether to print the number of epochs and validation loss and leakage after the training. dataloader_kwargs: Additional or updated kwargs to be passed to the training and validation dataloaders (like, e.g., a collate_fn) Returns: Classifier that approximates the ratio $p(\\theta,x)/p(\\theta)p(x)$. \"\"\" # AALR is defined for `num_atoms=2`. # Proxy to `super().__call__` to ensure right parameter. kwargs = del_entries ( locals (), entries = ( \"self\" , \"__class__\" )) return super () . train ( ** kwargs , num_atoms = 2 ) sbi.inference.snre.snre_b.SNRE_B ( RatioEstimator ) \u00b6 __init__ ( self , prior , classifier = 'resnet' , device = 'cpu' , logging_level = 'warning' , summary_writer = None , show_progress_bars = True , ** unused_args ) special \u00b6 SRE[1], here known as SNRE_B. [1] On Contrastive Learning for Likelihood-free Inference , Durkan et al., ICML 2020, https://arxiv.org/pdf/2002.03712 Parameters: Name Type Description Default prior A probability distribution that expresses prior knowledge about the parameters, e.g. which ranges are meaningful for them. Any object with .log_prob() and .sample() (for example, a PyTorch distribution) can be used. required classifier Union[str, Callable] Classifier trained to approximate likelihood ratios. If it is a string, use a pre-configured network of the provided type (one of linear, mlp, resnet). Alternatively, a function that builds a custom neural network can be provided. The function will be called with the first batch of simulations (theta, x), which can thus be used for shape inference and potentially for z-scoring. It needs to return a PyTorch nn.Module implementing the classifier. 'resnet' device str Training device, e.g., \u201ccpu\u201d, \u201ccuda\u201d or \u201ccuda:{0, 1, \u2026}\u201d. 'cpu' logging_level Union[int, str] Minimum severity of messages to log. One of the strings INFO, WARNING, DEBUG, ERROR and CRITICAL. 'warning' summary_writer Optional[Writer] A tensorboard SummaryWriter to control, among others, log file location (default is <current working directory>/logs .) None show_progress_bars bool Whether to show a progressbar during simulation and sampling. True unused_args Absorbs additional arguments. No entries will be used. If it is not empty, we warn. In future versions, when the new interface of 0.14.0 is more mature, we will remove this argument. {} Source code in sbi/inference/snre/snre_b.py def __init__ ( self , prior , classifier : Union [ str , Callable ] = \"resnet\" , device : str = \"cpu\" , logging_level : Union [ int , str ] = \"warning\" , summary_writer : Optional [ TensorboardSummaryWriter ] = None , show_progress_bars : bool = True , ** unused_args ): r \"\"\"SRE[1], here known as SNRE_B. [1] _On Contrastive Learning for Likelihood-free Inference_, Durkan et al., ICML 2020, https://arxiv.org/pdf/2002.03712 Args: prior: A probability distribution that expresses prior knowledge about the parameters, e.g. which ranges are meaningful for them. Any object with `.log_prob()`and `.sample()` (for example, a PyTorch distribution) can be used. classifier: Classifier trained to approximate likelihood ratios. If it is a string, use a pre-configured network of the provided type (one of linear, mlp, resnet). Alternatively, a function that builds a custom neural network can be provided. The function will be called with the first batch of simulations (theta, x), which can thus be used for shape inference and potentially for z-scoring. It needs to return a PyTorch `nn.Module` implementing the classifier. device: Training device, e.g., \"cpu\", \"cuda\" or \"cuda:{0, 1, ...}\". logging_level: Minimum severity of messages to log. One of the strings INFO, WARNING, DEBUG, ERROR and CRITICAL. summary_writer: A tensorboard `SummaryWriter` to control, among others, log file location (default is `<current working directory>/logs`.) show_progress_bars: Whether to show a progressbar during simulation and sampling. unused_args: Absorbs additional arguments. No entries will be used. If it is not empty, we warn. In future versions, when the new interface of 0.14.0 is more mature, we will remove this argument. \"\"\" kwargs = del_entries ( locals (), entries = ( \"self\" , \"__class__\" , \"unused_args\" )) super () . __init__ ( ** kwargs , ** unused_args ) append_simulations ( self , theta , x , from_round = 0 ) inherited \u00b6 Store parameters and simulation outputs to use them for later training. Data are stored as entries in lists for each type of variable (parameter/data). Stores \\(\\theta\\) , \\(x\\) , prior_masks (indicating if simulations are coming from the prior or not) and an index indicating which round the batch of simulations came from. Parameters: Name Type Description Default theta Tensor Parameter sets. required x Tensor Simulation outputs. required from_round int Which round the data stemmed from. Round 0 means from the prior. With default settings, this is not used at all for SNRE . Only when the user later on requests .train(discard_prior_samples=True) , we use these indices to find which training data stemmed from the prior. 0 Returns: Type Description RatioEstimator NeuralInference object (returned so that this function is chainable). Source code in sbi/inference/snre/snre_b.py def append_simulations ( self , theta : Tensor , x : Tensor , from_round : int = 0 , ) -> \"RatioEstimator\" : r \"\"\" Store parameters and simulation outputs to use them for later training. Data are stored as entries in lists for each type of variable (parameter/data). Stores $\\theta$, $x$, prior_masks (indicating if simulations are coming from the prior or not) and an index indicating which round the batch of simulations came from. Args: theta: Parameter sets. x: Simulation outputs. from_round: Which round the data stemmed from. Round 0 means from the prior. With default settings, this is not used at all for `SNRE`. Only when the user later on requests `.train(discard_prior_samples=True)`, we use these indices to find which training data stemmed from the prior. Returns: NeuralInference object (returned so that this function is chainable). \"\"\" theta , x = validate_theta_and_x ( theta , x , training_device = self . _device ) self . _theta_roundwise . append ( theta ) self . _x_roundwise . append ( x ) self . _prior_masks . append ( mask_sims_from_prior ( int ( from_round ), theta . size ( 0 ))) self . _data_round_index . append ( int ( from_round )) return self build_posterior ( self , density_estimator = None , sample_with = 'mcmc' , mcmc_method = 'slice_np' , mcmc_parameters = None , rejection_sampling_parameters = None ) inherited \u00b6 Build posterior from the neural density estimator. SNRE trains a neural network to approximate likelihood ratios, which in turn can be used obtain an unnormalized posterior \\(p(\\theta|x) \\propto p(x|\\theta) \\cdot p(\\theta)\\) . The posterior returned here wraps the trained network such that one can directly evaluate the unnormalized posterior log-probability \\(p(\\theta|x) \\propto p(x|\\theta) \\cdot p(\\theta)\\) and draw samples from the posterior with MCMC. Note that, in the case of single-round SNRE_A / AALR, it is possible to evaluate the log-probability of the normalized posterior, but sampling still requires MCMC. Parameters: Name Type Description Default density_estimator Optional[Module] The density estimator that the posterior is based on. If None , use the latest neural density estimator that was trained. None sample_with str Method to use for sampling from the posterior. Must be one of [ mcmc | rejection ]. 'mcmc' mcmc_method str Method used for MCMC sampling, one of slice_np , slice , hmc , nuts . Currently defaults to slice_np for a custom numpy implementation of slice sampling; select hmc , nuts or slice for Pyro-based sampling. 'slice_np' mcmc_parameters Optional[Dict[str, Any]] Dictionary overriding the default parameters for MCMC. The following parameters are supported: thin to set the thinning factor for the chain, warmup_steps to set the initial number of samples to discard, num_chains for the number of chains, init_strategy for the initialisation strategy for chains; prior will draw init locations from prior, whereas sir will use Sequential-Importance-Resampling using init_strategy_num_candidates to find init locations. None rejection_sampling_parameters Optional[Dict[str, Any]] Dictionary overriding the default parameters for rejection sampling. The following parameters are supported: proposal as the proposal distribtution (default is the prior). max_sampling_batch_size as the batchsize of samples being drawn from the proposal at every iteration. num_samples_to_find_max as the number of samples that are used to find the maximum of the potential_fn / proposal ratio. num_iter_to_find_max as the number of gradient ascent iterations to find the maximum of that ratio. m as multiplier to that ratio. None Returns: Type Description RatioBasedPosterior Posterior \\(p(\\theta|x)\\) with .sample() and .log_prob() methods (the returned log-probability is unnormalized). Source code in sbi/inference/snre/snre_b.py def build_posterior ( self , density_estimator : Optional [ TorchModule ] = None , sample_with : str = \"mcmc\" , mcmc_method : str = \"slice_np\" , mcmc_parameters : Optional [ Dict [ str , Any ]] = None , rejection_sampling_parameters : Optional [ Dict [ str , Any ]] = None , ) -> RatioBasedPosterior : r \"\"\" Build posterior from the neural density estimator. SNRE trains a neural network to approximate likelihood ratios, which in turn can be used obtain an unnormalized posterior $p(\\theta|x) \\propto p(x|\\theta) \\cdot p(\\theta)$. The posterior returned here wraps the trained network such that one can directly evaluate the unnormalized posterior log-probability $p(\\theta|x) \\propto p(x|\\theta) \\cdot p(\\theta)$ and draw samples from the posterior with MCMC. Note that, in the case of single-round SNRE_A / AALR, it is possible to evaluate the log-probability of the **normalized** posterior, but sampling still requires MCMC. Args: density_estimator: The density estimator that the posterior is based on. If `None`, use the latest neural density estimator that was trained. sample_with: Method to use for sampling from the posterior. Must be one of [`mcmc` | `rejection`]. mcmc_method: Method used for MCMC sampling, one of `slice_np`, `slice`, `hmc`, `nuts`. Currently defaults to `slice_np` for a custom numpy implementation of slice sampling; select `hmc`, `nuts` or `slice` for Pyro-based sampling. mcmc_parameters: Dictionary overriding the default parameters for MCMC. The following parameters are supported: `thin` to set the thinning factor for the chain, `warmup_steps` to set the initial number of samples to discard, `num_chains` for the number of chains, `init_strategy` for the initialisation strategy for chains; `prior` will draw init locations from prior, whereas `sir` will use Sequential-Importance-Resampling using `init_strategy_num_candidates` to find init locations. rejection_sampling_parameters: Dictionary overriding the default parameters for rejection sampling. The following parameters are supported: `proposal` as the proposal distribtution (default is the prior). `max_sampling_batch_size` as the batchsize of samples being drawn from the proposal at every iteration. `num_samples_to_find_max` as the number of samples that are used to find the maximum of the `potential_fn / proposal` ratio. `num_iter_to_find_max` as the number of gradient ascent iterations to find the maximum of that ratio. `m` as multiplier to that ratio. Returns: Posterior $p(\\theta|x)$ with `.sample()` and `.log_prob()` methods (the returned log-probability is unnormalized). \"\"\" if density_estimator is None : density_estimator = self . _neural_net # If internal net is used device is defined. device = self . _device else : # Otherwise, infer it from the device of the net parameters. device = next ( density_estimator . parameters ()) . device . type self . _posterior = RatioBasedPosterior ( method_family = self . __class__ . __name__ . lower (), neural_net = density_estimator , prior = self . _prior , x_shape = self . _x_shape , sample_with = sample_with , mcmc_method = mcmc_method , mcmc_parameters = mcmc_parameters , rejection_sampling_parameters = rejection_sampling_parameters , device = device , ) self . _posterior . _num_trained_rounds = self . _round + 1 # Store models at end of each round. self . _model_bank . append ( deepcopy ( self . _posterior )) self . _model_bank [ - 1 ] . net . eval () return deepcopy ( self . _posterior ) get_dataloaders ( self , dataset , training_batch_size = 50 , validation_fraction = 0.1 , resume_training = False , dataloader_kwargs = None ) inherited \u00b6 Return dataloaders for training and validation. Parameters: Name Type Description Default dataset TensorDataset holding all theta and x, optionally masks. required training_batch_size int training arg of inference methods. 50 resume_training bool Whether the current call is resuming training so that no new training and validation indices into the dataset have to be created. False dataloader_kwargs Optional[dict] Additional or updated kwargs to be passed to the training and validation dataloaders (like, e.g., a collate_fn) None Returns: Type Description Tuple[torch.utils.data.dataloader.DataLoader, torch.utils.data.dataloader.DataLoader] Tuple of dataloaders for training and validation. Source code in sbi/inference/snre/snre_b.py def get_dataloaders ( self , dataset : data . TensorDataset , training_batch_size : int = 50 , validation_fraction : float = 0.1 , resume_training : bool = False , dataloader_kwargs : Optional [ dict ] = None , ) -> Tuple [ data . DataLoader , data . DataLoader ]: \"\"\"Return dataloaders for training and validation. Args: dataset: holding all theta and x, optionally masks. training_batch_size: training arg of inference methods. resume_training: Whether the current call is resuming training so that no new training and validation indices into the dataset have to be created. dataloader_kwargs: Additional or updated kwargs to be passed to the training and validation dataloaders (like, e.g., a collate_fn) Returns: Tuple of dataloaders for training and validation. \"\"\" # Get total number of training examples. num_examples = len ( dataset ) # Select random train and validation splits from (theta, x) pairs. num_training_examples = int (( 1 - validation_fraction ) * num_examples ) num_validation_examples = num_examples - num_training_examples if not resume_training : permuted_indices = torch . randperm ( num_examples ) self . train_indices , self . val_indices = ( permuted_indices [: num_training_examples ], permuted_indices [ num_training_examples :], ) # Create training and validation loaders using a subset sampler. # Intentionally use dicts to define the default dataloader args # Then, use dataloader_kwargs to override (or add to) any of these defaults # https://stackoverflow.com/questions/44784577/in-method-call-args-how-to-override-keyword-argument-of-unpacked-dict train_loader_kwargs = { \"batch_size\" : min ( training_batch_size , num_training_examples ), \"drop_last\" : True , \"sampler\" : data . sampler . SubsetRandomSampler ( self . train_indices ), } train_loader_kwargs = ( dict ( train_loader_kwargs , ** dataloader_kwargs ) if dataloader_kwargs is not None else train_loader_kwargs ) val_loader_kwargs = { \"batch_size\" : min ( training_batch_size , num_validation_examples ), \"shuffle\" : False , \"drop_last\" : True , \"sampler\" : data . sampler . SubsetRandomSampler ( self . val_indices ), } val_loader_kwargs = ( dict ( val_loader_kwargs , ** dataloader_kwargs ) if dataloader_kwargs is not None else val_loader_kwargs ) train_loader = data . DataLoader ( dataset , ** train_loader_kwargs ) val_loader = data . DataLoader ( dataset , ** val_loader_kwargs ) return train_loader , val_loader get_simulations ( self , starting_round = 0 , exclude_invalid_x = True , warn_on_invalid = True ) inherited \u00b6 Returns all \\(\\theta\\) , \\(x\\) , and prior_masks from rounds >= starting_round . If requested, do not return invalid data. Parameters: Name Type Description Default starting_round int The earliest round to return samples from (we start counting from zero). 0 exclude_invalid_x bool Whether to exclude simulation outputs x=NaN or x=\u00b1\u221e during training. True warn_on_invalid bool Whether to give out a warning if invalid simulations were found. True Returns: Parameters, simulation outputs, prior masks. Source code in sbi/inference/snre/snre_b.py def get_simulations ( self , starting_round : int = 0 , exclude_invalid_x : bool = True , warn_on_invalid : bool = True , ) -> Tuple [ Tensor , Tensor , Tensor ]: r \"\"\" Returns all $\\theta$, $x$, and prior_masks from rounds >= `starting_round`. If requested, do not return invalid data. Args: starting_round: The earliest round to return samples from (we start counting from zero). exclude_invalid_x: Whether to exclude simulation outputs `x=NaN` or `x=\u00b1\u221e` during training. warn_on_invalid: Whether to give out a warning if invalid simulations were found. Returns: Parameters, simulation outputs, prior masks. \"\"\" theta = get_simulations_since_round ( self . _theta_roundwise , self . _data_round_index , starting_round ) x = get_simulations_since_round ( self . _x_roundwise , self . _data_round_index , starting_round ) prior_masks = get_simulations_since_round ( self . _prior_masks , self . _data_round_index , starting_round ) # Check for NaNs in simulations. is_valid_x , num_nans , num_infs = handle_invalid_x ( x , exclude_invalid_x ) # Check for problematic z-scoring warn_if_zscoring_changes_data ( x ) if warn_on_invalid : warn_on_invalid_x ( num_nans , num_infs , exclude_invalid_x ) warn_on_invalid_x_for_snpec_leakage ( num_nans , num_infs , exclude_invalid_x , type ( self ) . __name__ , self . _round ) return theta [ is_valid_x ], x [ is_valid_x ], prior_masks [ is_valid_x ] provide_presimulated ( self , theta , x , from_round = 0 ) inherited \u00b6 Deprecated since sbi 0.14.0. Instead of using this, please use .append_simulations() . Please consult release notes to see how you can update your code: https://github.com/mackelab/sbi/releases/tag/v0.14.0 More information can be found under the corresponding pull request on github: https://github.com/mackelab/sbi/pull/378 and tutorials: https://www.mackelab.org/sbi/tutorial/02_flexible_interface/ Provide external \\(\\theta\\) and \\(x\\) to be used for training later on. Parameters: Name Type Description Default theta Tensor Parameter sets used to generate presimulated data. required x Tensor Simulation outputs of presimulated data. required from_round int Which round the data was simulated from. from_round=0 means that the data came from the first round, i.e. the prior. 0 Source code in sbi/inference/snre/snre_b.py def provide_presimulated ( self , theta : Tensor , x : Tensor , from_round : int = 0 ) -> None : r \"\"\" Deprecated since sbi 0.14.0. Instead of using this, please use `.append_simulations()`. Please consult release notes to see how you can update your code: https://github.com/mackelab/sbi/releases/tag/v0.14.0 More information can be found under the corresponding pull request on github: https://github.com/mackelab/sbi/pull/378 and tutorials: https://www.mackelab.org/sbi/tutorial/02_flexible_interface/ Provide external $\\theta$ and $x$ to be used for training later on. Args: theta: Parameter sets used to generate presimulated data. x: Simulation outputs of presimulated data. from_round: Which round the data was simulated from. `from_round=0` means that the data came from the first round, i.e. the prior. \"\"\" raise NameError ( f \"Deprecated since sbi 0.14.0. \" f \"Instead of using this, please use `.append_simulations()`. Please \" f \"consult release notes to see how you can update your code: \" f \"https://github.com/mackelab/sbi/releases/tag/v0.14.0\" f \"More information can be found under the corresponding pull request on \" f \"github: \" f \"https://github.com/mackelab/sbi/pull/378\" f \"and tutorials: \" f \"https://www.mackelab.org/sbi/tutorial/02_flexible_interface/\" , ) train ( self , num_atoms = 10 , training_batch_size = 50 , learning_rate = 0.0005 , validation_fraction = 0.1 , stop_after_epochs = 20 , max_num_epochs = None , clip_max_norm = 5.0 , exclude_invalid_x = True , resume_training = False , discard_prior_samples = False , retrain_from_scratch_each_round = False , show_train_summary = False , dataloader_kwargs = None ) \u00b6 Return classifier that approximates the ratio \\(p(\\theta,x)/p(\\theta)p(x)\\) . Parameters: Name Type Description Default num_atoms int Number of atoms to use for classification. 10 training_batch_size int Training batch size. 50 learning_rate float Learning rate for Adam optimizer. 0.0005 validation_fraction float The fraction of data to use for validation. 0.1 stop_after_epochs int The number of epochs to wait for improvement on the validation set before terminating training. 20 max_num_epochs Optional[int] Maximum number of epochs to run. If reached, we stop training even when the validation loss is still decreasing. If None, we train until validation loss increases (see also stop_after_epochs ). None clip_max_norm Optional[float] Value at which to clip the total gradient norm in order to prevent exploding gradients. Use None for no clipping. 5.0 exclude_invalid_x bool Whether to exclude simulation outputs x=NaN or x=\u00b1\u221e during training. Expect errors, silent or explicit, when False . True resume_training bool Can be used in case training time is limited, e.g. on a cluster. If True , the split between train and validation set, the optimizer, the number of epochs, and the best validation log-prob will be restored from the last time .train() was called. False discard_prior_samples bool Whether to discard samples simulated in round 1, i.e. from the prior. Training may be sped up by ignoring such less targeted samples. False retrain_from_scratch_each_round bool Whether to retrain the conditional density estimator for the posterior from scratch each round. False show_train_summary bool Whether to print the number of epochs and validation loss and leakage after the training. False dataloader_kwargs Optional[Dict] Additional or updated kwargs to be passed to the training and validation dataloaders (like, e.g., a collate_fn) None Returns: Type Description NeuralPosterior Classifier that approximates the ratio \\(p(\\theta,x)/p(\\theta)p(x)\\) . Source code in sbi/inference/snre/snre_b.py def train ( self , num_atoms : int = 10 , training_batch_size : int = 50 , learning_rate : float = 5e-4 , validation_fraction : float = 0.1 , stop_after_epochs : int = 20 , max_num_epochs : Optional [ int ] = None , clip_max_norm : Optional [ float ] = 5.0 , exclude_invalid_x : bool = True , resume_training : bool = False , discard_prior_samples : bool = False , retrain_from_scratch_each_round : bool = False , show_train_summary : bool = False , dataloader_kwargs : Optional [ Dict ] = None , ) -> NeuralPosterior : r \"\"\" Return classifier that approximates the ratio $p(\\theta,x)/p(\\theta)p(x)$. Args: num_atoms: Number of atoms to use for classification. training_batch_size: Training batch size. learning_rate: Learning rate for Adam optimizer. validation_fraction: The fraction of data to use for validation. stop_after_epochs: The number of epochs to wait for improvement on the validation set before terminating training. max_num_epochs: Maximum number of epochs to run. If reached, we stop training even when the validation loss is still decreasing. If None, we train until validation loss increases (see also `stop_after_epochs`). clip_max_norm: Value at which to clip the total gradient norm in order to prevent exploding gradients. Use None for no clipping. exclude_invalid_x: Whether to exclude simulation outputs `x=NaN` or `x=\u00b1\u221e` during training. Expect errors, silent or explicit, when `False`. resume_training: Can be used in case training time is limited, e.g. on a cluster. If `True`, the split between train and validation set, the optimizer, the number of epochs, and the best validation log-prob will be restored from the last time `.train()` was called. discard_prior_samples: Whether to discard samples simulated in round 1, i.e. from the prior. Training may be sped up by ignoring such less targeted samples. retrain_from_scratch_each_round: Whether to retrain the conditional density estimator for the posterior from scratch each round. show_train_summary: Whether to print the number of epochs and validation loss and leakage after the training. dataloader_kwargs: Additional or updated kwargs to be passed to the training and validation dataloaders (like, e.g., a collate_fn) Returns: Classifier that approximates the ratio $p(\\theta,x)/p(\\theta)p(x)$. \"\"\" kwargs = del_entries ( locals (), entries = ( \"self\" , \"__class__\" )) return super () . train ( ** kwargs ) sbi.inference.abc.mcabc.MCABC ( ABCBASE ) \u00b6 __call__ ( self , x_o , num_simulations , eps = None , quantile = None , lra = False , sass = False , sass_fraction = 0.25 , sass_expansion_degree = 1 , kde = False , kde_kwargs = {}, return_summary = False ) special \u00b6 Run MCABC and return accepted parameters or KDE object fitted on them. Parameters: Name Type Description Default x_o Union[torch.Tensor, numpy.ndarray] Observed data. required num_simulations int Number of simulations to run. required eps Optional[float] Acceptance threshold \\(\\epsilon\\) for distance between observed and simulated data. None quantile Optional[float] Upper quantile of smallest distances for which the corresponding parameters are returned, e.g, q=0.01 will return the top 1%. Exactly one of quantile or eps have to be passed. None lra bool Whether to run linear regression adjustment as in Beaumont et al. 2002 False sass bool Whether to determine semi-automatic summary statistics as in Fearnhead & Prangle 2012. False sass_fraction float Fraction of simulation budget used for the initial sass run. 0.25 sass_expansion_degree int Degree of the polynomial feature expansion for the sass regression, default 1 - no expansion. 1 kde bool Whether to run KDE on the accepted parameters to return a KDE object from which one can sample. False kde_kwargs Optional[Dict[str, Any]] kwargs for performing KDE: \u2018bandwidth=\u2019; either a float, or a string naming a bandwidth heuristics, e.g., \u2018cv\u2019 (cross validation), \u2018silvermann\u2019 or \u2018scott\u2019, default \u2018cv\u2019. \u2018transform\u2019: transform applied to the parameters before doing KDE. \u2018sample_weights\u2019: weights associated with samples. See \u2018get_kde\u2019 for more details {} return_summary bool Whether to return the distances and data corresponding to the accepted parameters. False Returns: Type Description theta (if kde False) accepted parameters kde (if kde True): KDE object based on accepted parameters from which one can .sample() and .log_prob(). summary (if summary True): dictionary containing the accepted paramters (if kde True), distances and simulated data x. Source code in sbi/inference/abc/mcabc.py def __call__ ( self , x_o : Union [ Tensor , ndarray ], num_simulations : int , eps : Optional [ float ] = None , quantile : Optional [ float ] = None , lra : bool = False , sass : bool = False , sass_fraction : float = 0.25 , sass_expansion_degree : int = 1 , kde : bool = False , kde_kwargs : Optional [ Dict [ str , Any ]] = {}, return_summary : bool = False , ) -> Union [ Tuple [ Tensor , dict ], Tuple [ KDEWrapper , dict ], Tensor , KDEWrapper ]: r \"\"\"Run MCABC and return accepted parameters or KDE object fitted on them. Args: x_o: Observed data. num_simulations: Number of simulations to run. eps: Acceptance threshold $\\epsilon$ for distance between observed and simulated data. quantile: Upper quantile of smallest distances for which the corresponding parameters are returned, e.g, q=0.01 will return the top 1%. Exactly one of quantile or `eps` have to be passed. lra: Whether to run linear regression adjustment as in Beaumont et al. 2002 sass: Whether to determine semi-automatic summary statistics as in Fearnhead & Prangle 2012. sass_fraction: Fraction of simulation budget used for the initial sass run. sass_expansion_degree: Degree of the polynomial feature expansion for the sass regression, default 1 - no expansion. kde: Whether to run KDE on the accepted parameters to return a KDE object from which one can sample. kde_kwargs: kwargs for performing KDE: 'bandwidth='; either a float, or a string naming a bandwidth heuristics, e.g., 'cv' (cross validation), 'silvermann' or 'scott', default 'cv'. 'transform': transform applied to the parameters before doing KDE. 'sample_weights': weights associated with samples. See 'get_kde' for more details return_summary: Whether to return the distances and data corresponding to the accepted parameters. Returns: theta (if kde False): accepted parameters kde (if kde True): KDE object based on accepted parameters from which one can .sample() and .log_prob(). summary (if summary True): dictionary containing the accepted paramters (if kde True), distances and simulated data x. \"\"\" # Exactly one of eps or quantile need to be passed. assert ( eps is not None ) ^ ( quantile is not None ), \"Eps or quantile must be passed, but not both.\" # Run SASS and change the simulator and x_o accordingly. if sass : num_pilot_simulations = int ( sass_fraction * num_simulations ) self . logger . info ( f \"Running SASS with { num_pilot_simulations } pilot samples.\" ) num_simulations -= num_pilot_simulations pilot_theta = self . prior . sample (( num_pilot_simulations ,)) pilot_x = self . _batched_simulator ( pilot_theta ) sass_transform = self . get_sass_transform ( pilot_theta , pilot_x , sass_expansion_degree ) simulator = lambda theta : sass_transform ( self . _batched_simulator ( theta )) x_o = sass_transform ( x_o ) else : simulator = self . _batched_simulator # Simulate and calculate distances. theta = self . prior . sample (( num_simulations ,)) x = simulator ( theta ) # Infer shape of x to test and set x_o. self . x_shape = x [ 0 ] . unsqueeze ( 0 ) . shape self . x_o = process_x ( x_o , self . x_shape ) distances = self . distance ( self . x_o , x ) # Select based on acceptance threshold epsilon. if eps is not None : is_accepted = distances < eps num_accepted = is_accepted . sum () . item () assert num_accepted > 0 , f \"No parameters accepted, eps= { eps } too small\" theta_accepted = theta [ is_accepted ] distances_accepted = distances [ is_accepted ] x_accepted = x [ is_accepted ] # Select based on quantile on sorted distances. elif quantile is not None : num_top_samples = int ( num_simulations * quantile ) sort_idx = torch . argsort ( distances ) theta_accepted = theta [ sort_idx ][: num_top_samples ] distances_accepted = distances [ sort_idx ][: num_top_samples ] x_accepted = x [ sort_idx ][: num_top_samples ] else : raise ValueError ( \"One of epsilon or quantile has to be passed.\" ) # Maybe adjust theta with LRA. if lra : self . logger . info ( \"Running Linear regression adjustment.\" ) final_theta = self . run_lra ( theta_accepted , x_accepted , observation = self . x_o ) else : final_theta = theta_accepted if kde : self . logger . info ( f \"\"\"KDE on { final_theta . shape [ 0 ] } samples with bandwidth option { kde_kwargs [ \"bandwidth\" ] if \"bandwidth\" in kde_kwargs else \"cv\" } . Beware that KDE can give unreliable results when used with too few samples and in high dimensions.\"\"\" ) kde_dist = get_kde ( final_theta , ** kde_kwargs ) if return_summary : return kde_dist , dict ( theta = final_theta , distances = distances_accepted , x = x_accepted ) else : return kde_dist elif return_summary : return final_theta , dict ( distances = distances_accepted , x = x_accepted ) else : return final_theta __init__ ( self , simulator , prior , distance = 'l2' , num_workers = 1 , simulation_batch_size = 1 , show_progress_bars = True ) special \u00b6 Monte-Carlo Approximate Bayesian Computation (Rejection ABC) [1]. [1] Pritchard, J. K., Seielstad, M. T., Perez-Lezaun, A., & Feldman, M. W. (1999). Population growth of human Y chromosomes: a study of Y chromosome microsatellites. Molecular biology and evolution, 16(12), 1791-1798. Parameters: Name Type Description Default simulator Callable A function that takes parameters $ heta$ and maps them to simulations, or observations, x , \\(\\mathrm{sim}( heta) o x\\) . Any regular Python callable (i.e. function or class with __call__ method) can be used. required prior A probability distribution that expresses prior knowledge about the parameters, e.g. which ranges are meaningful for them. Any object with .log_prob() and .sample() (for example, a PyTorch distribution) can be used. required distance Union[str, Callable] Distance function to compare observed and simulated data. Can be a custom function or one of l1 , l2 , mse . 'l2' num_workers int Number of parallel workers to use for simulations. 1 simulation_batch_size int Number of parameter sets that the simulator maps to data x at once. If None, we simulate all parameter sets at the same time. If >= 1, the simulator has to process data of shape (simulation_batch_size, parameter_dimension). 1 show_progress_bars bool Whether to show a progressbar during simulation and sampling. True Source code in sbi/inference/abc/mcabc.py def __init__ ( self , simulator : Callable , prior , distance : Union [ str , Callable ] = \"l2\" , num_workers : int = 1 , simulation_batch_size : int = 1 , show_progress_bars : bool = True , ): \"\"\"Monte-Carlo Approximate Bayesian Computation (Rejection ABC) [1]. [1] Pritchard, J. K., Seielstad, M. T., Perez-Lezaun, A., & Feldman, M. W. (1999). Population growth of human Y chromosomes: a study of Y chromosome microsatellites. Molecular biology and evolution, 16(12), 1791-1798. Args: simulator: A function that takes parameters $\\theta$ and maps them to simulations, or observations, `x`, $\\mathrm{sim}(\\theta)\\to x$. Any regular Python callable (i.e. function or class with `__call__` method) can be used. prior: A probability distribution that expresses prior knowledge about the parameters, e.g. which ranges are meaningful for them. Any object with `.log_prob()`and `.sample()` (for example, a PyTorch distribution) can be used. distance: Distance function to compare observed and simulated data. Can be a custom function or one of `l1`, `l2`, `mse`. num_workers: Number of parallel workers to use for simulations. simulation_batch_size: Number of parameter sets that the simulator maps to data x at once. If None, we simulate all parameter sets at the same time. If >= 1, the simulator has to process data of shape (simulation_batch_size, parameter_dimension). show_progress_bars: Whether to show a progressbar during simulation and sampling. \"\"\" super () . __init__ ( simulator = simulator , prior = prior , distance = distance , num_workers = num_workers , simulation_batch_size = simulation_batch_size , show_progress_bars = show_progress_bars , ) choose_distance_function ( distance_type = 'l2' ) inherited \u00b6 Return distance function for given distance type. Source code in sbi/inference/abc/mcabc.py @staticmethod def choose_distance_function ( distance_type : str = \"l2\" ) -> Callable : \"\"\"Return distance function for given distance type.\"\"\" if distance_type == \"mse\" : distance = lambda xo , x : torch . mean (( xo - x ) ** 2 , dim =- 1 ) elif distance_type == \"l2\" : distance = lambda xo , x : torch . norm (( xo - x ), dim =- 1 ) elif distance_type == \"l1\" : distance = lambda xo , x : torch . mean ( abs ( xo - x ), dim =- 1 ) else : raise ValueError ( r \"Distance {distance_type} not supported.\" ) def distance_fun ( observed_data : Tensor , simulated_data : Tensor ) -> Tensor : \"\"\"Return distance over batch dimension. Args: observed_data: Observed data, could be 1D. simulated_data: Batch of simulated data, has batch dimension. Returns: Torch tensor with batch of distances. \"\"\" assert simulated_data . ndim == 2 , \"simulated data needs batch dimension\" return distance ( observed_data , simulated_data ) return distance_fun get_sass_transform ( theta , x , expansion_degree = 1 , sample_weight = None ) inherited \u00b6 Return semi-automatic summary statitics function. Running weighted linear regressin as in Fearnhead & Prandle 2012: https://arxiv.org/abs/1004.1112 Following implementation in https://abcpy.readthedocs.io/en/latest/_modules/abcpy/statistics.html#Identity and https://pythonhosted.org/abcpy/_modules/abcpy/summaryselections.html#Semiautomatic Source code in sbi/inference/abc/mcabc.py @staticmethod def get_sass_transform ( theta : torch . Tensor , x : torch . Tensor , expansion_degree : int = 1 , sample_weight = None , ) -> Callable : \"\"\"Return semi-automatic summary statitics function. Running weighted linear regressin as in Fearnhead & Prandle 2012: https://arxiv.org/abs/1004.1112 Following implementation in https://abcpy.readthedocs.io/en/latest/_modules/abcpy/statistics.html#Identity and https://pythonhosted.org/abcpy/_modules/abcpy/summaryselections.html#Semiautomatic \"\"\" expansion = PolynomialFeatures ( degree = expansion_degree , include_bias = False ) # Transform x, remove intercept. x_expanded = expansion . fit_transform ( x ) sumstats_map = np . zeros (( x_expanded . shape [ 1 ], theta . shape [ 1 ])) for parameter_idx in range ( theta . shape [ 1 ]): regression_model = LinearRegression ( fit_intercept = True ) regression_model . fit ( X = x_expanded , y = theta [:, parameter_idx ], sample_weight = sample_weight ) sumstats_map [:, parameter_idx ] = regression_model . coef_ sumstats_map = torch . tensor ( sumstats_map , dtype = torch . float32 ) def sumstats_transform ( x ): x_expanded = torch . tensor ( expansion . fit_transform ( x ), dtype = torch . float32 ) return x_expanded . mm ( sumstats_map ) return sumstats_transform run_lra ( theta , x , observation , sample_weight = None ) inherited \u00b6 Return parameters adjusted with linear regression adjustment. Implementation as in Beaumont et al. 2002: https://arxiv.org/abs/1707.01254 Source code in sbi/inference/abc/mcabc.py @staticmethod def run_lra ( theta : torch . Tensor , x : torch . Tensor , observation : torch . Tensor , sample_weight = None , ) -> torch . Tensor : \"\"\"Return parameters adjusted with linear regression adjustment. Implementation as in Beaumont et al. 2002: https://arxiv.org/abs/1707.01254 \"\"\" theta_adjusted = theta for parameter_idx in range ( theta . shape [ 1 ]): regression_model = LinearRegression ( fit_intercept = True ) regression_model . fit ( X = x , y = theta [:, parameter_idx ], sample_weight = sample_weight , ) theta_adjusted [:, parameter_idx ] += regression_model . predict ( observation . reshape ( 1 , - 1 ) ) theta_adjusted [:, parameter_idx ] -= regression_model . predict ( x ) return theta_adjusted sbi.inference.abc.smcabc.SMCABC ( ABCBASE ) \u00b6 __call__ ( self , x_o , num_particles , num_initial_pop , num_simulations , epsilon_decay , distance_based_decay = False , ess_min = None , kernel_variance_scale = 1.0 , use_last_pop_samples = True , return_summary = False , kde = False , kde_kwargs = {}, kde_sample_weights = False , lra = False , lra_with_weights = False , sass = False , sass_fraction = 0.25 , sass_expansion_degree = 1 ) special \u00b6 Run SMCABC and return accepted parameters or KDE object fitted on them. Parameters: Name Type Description Default x_o Union[torch.Tensor, numpy.ndarray] Observed data. required num_particles int Number of particles in each population. required num_initial_pop int Number of simulations used for initial population. required num_simulations int Total number of possible simulations. required epsilon_decay float Factor with which the acceptance threshold \\(\\epsilon\\) decays. required distance_based_decay bool Whether the \\(\\epsilon\\) decay is constant over populations or calculated from the previous populations distribution of distances. False ess_min Optional[float] Threshold of effective sampling size for resampling weights. Not used when None (default). None kernel_variance_scale float Factor for scaling the perturbation kernel variance. 1.0 use_last_pop_samples bool Whether to fill up the current population with samples from the previous population when the budget is used up. If False, the current population is discarded and the previous population is returned. True lra bool Whether to run linear regression adjustment as in Beaumont et al. 2002 False lra_with_weights bool Whether to run lra as weighted linear regression with SMC weights False sass bool Whether to determine semi-automatic summary statistics as in Fearnhead & Prangle 2012. False sass_fraction float Fraction of simulation budget used for the initial sass run. 0.25 sass_expansion_degree int Degree of the polynomial feature expansion for the sass regression, default 1 - no expansion. 1 kde bool Whether to run KDE on the accepted parameters to return a KDE object from which one can sample. False kde_kwargs Optional[Dict[str, Any]] kwargs for performing KDE: \u2018bandwidth=\u2019; either a float, or a string naming a bandwidth heuristics, e.g., \u2018cv\u2019 (cross validation), \u2018silvermann\u2019 or \u2018scott\u2019, default \u2018cv\u2019. \u2018transform\u2019: transform applied to the parameters before doing KDE. \u2018sample_weights\u2019: weights associated with samples. See \u2018get_kde\u2019 for more details {} kde_sample_weights bool Whether perform weighted KDE with SMC weights or on raw particles. False return_summary bool Whether to return a dictionary with all accepted particles, weights, etc. at the end. False Returns: Type Description theta (if kde False) accepted parameters of the last population. kde (if kde True): KDE object fitted on accepted parameters, from which one can .sample() and .log_prob(). summary (if return_summary True): dictionary containing the accepted paramters (if kde True), distances and simulated data x of all populations. Source code in sbi/inference/abc/smcabc.py def __call__ ( self , x_o : Union [ Tensor , ndarray ], num_particles : int , num_initial_pop : int , num_simulations : int , epsilon_decay : float , distance_based_decay : bool = False , ess_min : Optional [ float ] = None , kernel_variance_scale : float = 1.0 , use_last_pop_samples : bool = True , return_summary : bool = False , kde : bool = False , kde_kwargs : Optional [ Dict [ str , Any ]] = {}, kde_sample_weights : bool = False , lra : bool = False , lra_with_weights : bool = False , sass : bool = False , sass_fraction : float = 0.25 , sass_expansion_degree : int = 1 , ) -> Union [ Tensor , KDEWrapper , Tuple [ Tensor , dict ], Tuple [ KDEWrapper , dict ]]: r \"\"\"Run SMCABC and return accepted parameters or KDE object fitted on them. Args: x_o: Observed data. num_particles: Number of particles in each population. num_initial_pop: Number of simulations used for initial population. num_simulations: Total number of possible simulations. epsilon_decay: Factor with which the acceptance threshold $\\epsilon$ decays. distance_based_decay: Whether the $\\epsilon$ decay is constant over populations or calculated from the previous populations distribution of distances. ess_min: Threshold of effective sampling size for resampling weights. Not used when None (default). kernel_variance_scale: Factor for scaling the perturbation kernel variance. use_last_pop_samples: Whether to fill up the current population with samples from the previous population when the budget is used up. If False, the current population is discarded and the previous population is returned. lra: Whether to run linear regression adjustment as in Beaumont et al. 2002 lra_with_weights: Whether to run lra as weighted linear regression with SMC weights sass: Whether to determine semi-automatic summary statistics as in Fearnhead & Prangle 2012. sass_fraction: Fraction of simulation budget used for the initial sass run. sass_expansion_degree: Degree of the polynomial feature expansion for the sass regression, default 1 - no expansion. kde: Whether to run KDE on the accepted parameters to return a KDE object from which one can sample. kde_kwargs: kwargs for performing KDE: 'bandwidth='; either a float, or a string naming a bandwidth heuristics, e.g., 'cv' (cross validation), 'silvermann' or 'scott', default 'cv'. 'transform': transform applied to the parameters before doing KDE. 'sample_weights': weights associated with samples. See 'get_kde' for more details kde_sample_weights: Whether perform weighted KDE with SMC weights or on raw particles. return_summary: Whether to return a dictionary with all accepted particles, weights, etc. at the end. Returns: theta (if kde False): accepted parameters of the last population. kde (if kde True): KDE object fitted on accepted parameters, from which one can .sample() and .log_prob(). summary (if return_summary True): dictionary containing the accepted paramters (if kde True), distances and simulated data x of all populations. \"\"\" pop_idx = 0 self . num_simulations = num_simulations # Pilot run for SASS. if sass : num_pilot_simulations = int ( sass_fraction * num_simulations ) self . logger . info ( f \"Running SASS with { num_pilot_simulations } pilot samples.\" ) sass_transform = self . run_sass_set_xo ( num_particles , num_pilot_simulations , x_o , lra , sass_expansion_degree ) # Udpate simulator and xo x_o = sass_transform ( self . x_o ) def sass_simulator ( theta ): self . simulation_counter += theta . shape [ 0 ] return sass_transform ( self . _batched_simulator ( theta )) self . _simulate_with_budget = sass_simulator # run initial population particles , epsilon , distances , x = self . _set_xo_and_sample_initial_population ( x_o , num_particles , num_initial_pop ) log_weights = torch . log ( 1 / num_particles * ones ( num_particles )) self . logger . info ( ( f \"population= { pop_idx } , eps= { epsilon } , ess= { 1.0 } , \" f \"num_sims= { num_initial_pop } \" ) ) all_particles = [ particles ] all_log_weights = [ log_weights ] all_distances = [ distances ] all_epsilons = [ epsilon ] all_x = [ x ] while self . simulation_counter < self . num_simulations : pop_idx += 1 # Decay based on quantile of distances from previous pop. if distance_based_decay : epsilon = self . _get_next_epsilon ( all_distances [ pop_idx - 1 ], epsilon_decay ) # Constant decay. else : epsilon *= epsilon_decay # Get kernel variance from previous pop. self . kernel_variance = self . get_kernel_variance ( all_particles [ pop_idx - 1 ], torch . exp ( all_log_weights [ pop_idx - 1 ]), samples_per_dim = 500 , kernel_variance_scale = kernel_variance_scale , ) particles , log_weights , distances , x = self . _sample_next_population ( particles = all_particles [ pop_idx - 1 ], log_weights = all_log_weights [ pop_idx - 1 ], distances = all_distances [ pop_idx - 1 ], epsilon = epsilon , x = all_x [ pop_idx - 1 ], use_last_pop_samples = use_last_pop_samples , ) # Resample population if effective sampling size is too small. if ess_min is not None : particles , log_weights = self . resample_if_ess_too_small ( particles , log_weights , ess_min , pop_idx ) self . logger . info ( ( f \"population= { pop_idx } done: eps= { epsilon : .6f } ,\" f \" num_sims= { self . simulation_counter } .\" ) ) # collect results all_particles . append ( particles ) all_log_weights . append ( log_weights ) all_distances . append ( distances ) all_epsilons . append ( epsilon ) all_x . append ( x ) # Maybe run LRA and adjust weights. if lra : self . logger . info ( \"Running Linear regression adjustment.\" ) adjusted_particles , adjusted_weights = self . run_lra_update_weights ( particles = all_particles [ - 1 ], xs = all_x [ - 1 ], observation = x_o , log_weights = all_log_weights [ - 1 ], lra_with_weights = lra_with_weights , ) final_particles = adjusted_particles else : final_particles = all_particles [ - 1 ] if kde : self . logger . info ( f \"\"\"KDE on { final_particles . shape [ 0 ] } samples with bandwidth option { kde_kwargs [ \"bandwidth\" ] if \"bandwidth\" in kde_kwargs else \"cv\" } . Beware that KDE can give unreliable results when used with too few samples and in high dimensions.\"\"\" ) # Maybe get particles weights from last population for weighted KDE. if kde_sample_weights : kde_kwargs [ \"sample_weights\" ] = all_log_weights [ - 1 ] . exp () kde_dist = get_kde ( final_particles , ** kde_kwargs ) if return_summary : return ( kde_dist , dict ( particles = all_particles , weights = all_log_weights , epsilons = all_epsilons , distances = all_distances , xs = all_x , ), ) else : return kde_dist if return_summary : return ( final_particles , dict ( particles = all_particles , weights = all_log_weights , epsilons = all_epsilons , distances = all_distances , xs = all_x , ), ) else : return final_particles __init__ ( self , simulator , prior , distance = 'l2' , num_workers = 1 , simulation_batch_size = 1 , show_progress_bars = True , kernel = 'gaussian' , algorithm_variant = 'C' ) special \u00b6 Sequential Monte Carlo Approximate Bayesian Computation. We distinguish between three different SMC methods here: - A: Toni et al. 2010 (Phd Thesis) - B: Sisson et al. 2007 (with correction from 2009) - C: Beaumont et al. 2009 In Toni et al. 2010 we find an overview of the differences on page 34: - B: same as A except for resampling of weights if the effective sampling size is too small. - C: same as A except for calculation of the covariance of the perturbation kernel: the kernel covariance is a scaled version of the covariance of the previous population. Parameters: Name Type Description Default simulator Callable A function that takes parameters \\(\\theta\\) and maps them to simulations, or observations, x , \\(\\mathrm{sim}(\\theta)\\to x\\) . Any regular Python callable (i.e. function or class with __call__ method) can be used. required prior Distribution A probability distribution that expresses prior knowledge about the parameters, e.g. which ranges are meaningful for them. Any object with .log_prob() and .sample() (for example, a PyTorch distribution) can be used. required distance Union[str, Callable] Distance function to compare observed and simulated data. Can be a custom function or one of l1 , l2 , mse . 'l2' num_workers int Number of parallel workers to use for simulations. 1 simulation_batch_size int Number of parameter sets that the simulator maps to data x at once. If None, we simulate all parameter sets at the same time. If >= 1, the simulator has to process data of shape (simulation_batch_size, parameter_dimension). 1 show_progress_bars bool Whether to show a progressbar during simulation and sampling. True kernel Optional[str] Perturbation kernel. 'gaussian' algorithm_variant str Indicating the choice of algorithm variant, A, B, or C. 'C' Source code in sbi/inference/abc/smcabc.py def __init__ ( self , simulator : Callable , prior : Distribution , distance : Union [ str , Callable ] = \"l2\" , num_workers : int = 1 , simulation_batch_size : int = 1 , show_progress_bars : bool = True , kernel : Optional [ str ] = \"gaussian\" , algorithm_variant : str = \"C\" , ): r \"\"\"Sequential Monte Carlo Approximate Bayesian Computation. We distinguish between three different SMC methods here: - A: Toni et al. 2010 (Phd Thesis) - B: Sisson et al. 2007 (with correction from 2009) - C: Beaumont et al. 2009 In Toni et al. 2010 we find an overview of the differences on page 34: - B: same as A except for resampling of weights if the effective sampling size is too small. - C: same as A except for calculation of the covariance of the perturbation kernel: the kernel covariance is a scaled version of the covariance of the previous population. Args: simulator: A function that takes parameters $\\theta$ and maps them to simulations, or observations, `x`, $\\mathrm{sim}(\\theta)\\to x$. Any regular Python callable (i.e. function or class with `__call__` method) can be used. prior: A probability distribution that expresses prior knowledge about the parameters, e.g. which ranges are meaningful for them. Any object with `.log_prob()`and `.sample()` (for example, a PyTorch distribution) can be used. distance: Distance function to compare observed and simulated data. Can be a custom function or one of `l1`, `l2`, `mse`. num_workers: Number of parallel workers to use for simulations. simulation_batch_size: Number of parameter sets that the simulator maps to data x at once. If None, we simulate all parameter sets at the same time. If >= 1, the simulator has to process data of shape (simulation_batch_size, parameter_dimension). show_progress_bars: Whether to show a progressbar during simulation and sampling. kernel: Perturbation kernel. algorithm_variant: Indicating the choice of algorithm variant, A, B, or C. \"\"\" super () . __init__ ( simulator = simulator , prior = prior , distance = distance , num_workers = num_workers , simulation_batch_size = simulation_batch_size , show_progress_bars = show_progress_bars , ) kernels = ( \"gaussian\" , \"uniform\" ) assert ( kernel in kernels ), f \"Kernel ' { kernel } ' not supported. Choose one from { kernels } .\" self . kernel = kernel algorithm_variants = ( \"A\" , \"B\" , \"C\" ) assert algorithm_variant in algorithm_variants , ( f \"SMCABC variant ' { algorithm_variant } ' not supported, choose one from\" \" {algorithm_variants} .\" ) self . algorithm_variant = algorithm_variant self . distance_to_x0 = None self . simulation_counter = 0 self . num_simulations = 0 # Define simulator that keeps track of budget. def simulate_with_budget ( theta ): self . simulation_counter += theta . shape [ 0 ] return self . _batched_simulator ( theta ) self . _simulate_with_budget = simulate_with_budget choose_distance_function ( distance_type = 'l2' ) inherited \u00b6 Return distance function for given distance type. Source code in sbi/inference/abc/smcabc.py @staticmethod def choose_distance_function ( distance_type : str = \"l2\" ) -> Callable : \"\"\"Return distance function for given distance type.\"\"\" if distance_type == \"mse\" : distance = lambda xo , x : torch . mean (( xo - x ) ** 2 , dim =- 1 ) elif distance_type == \"l2\" : distance = lambda xo , x : torch . norm (( xo - x ), dim =- 1 ) elif distance_type == \"l1\" : distance = lambda xo , x : torch . mean ( abs ( xo - x ), dim =- 1 ) else : raise ValueError ( r \"Distance {distance_type} not supported.\" ) def distance_fun ( observed_data : Tensor , simulated_data : Tensor ) -> Tensor : \"\"\"Return distance over batch dimension. Args: observed_data: Observed data, could be 1D. simulated_data: Batch of simulated data, has batch dimension. Returns: Torch tensor with batch of distances. \"\"\" assert simulated_data . ndim == 2 , \"simulated data needs batch dimension\" return distance ( observed_data , simulated_data ) return distance_fun get_new_kernel ( self , thetas ) \u00b6 Return new kernel distribution for a given set of paramters. Source code in sbi/inference/abc/smcabc.py def get_new_kernel ( self , thetas : Tensor ) -> Distribution : \"\"\"Return new kernel distribution for a given set of paramters.\"\"\" if self . kernel == \"gaussian\" : assert self . kernel_variance . ndim == 2 return MultivariateNormal ( loc = thetas , covariance_matrix = self . kernel_variance ) elif self . kernel == \"uniform\" : low = thetas - self . kernel_variance high = thetas + self . kernel_variance # Move batch shape to event shape to get Uniform that is multivariate in # parameter dimension. return Uniform ( low = low , high = high ) . to_event ( 1 ) else : raise ValueError ( f \"Kernel, ' { self . kernel } ' not supported.\" ) get_particle_ranges ( self , particles , weights , samples_per_dim = 100 ) \u00b6 Return range of particles in each parameter dimension. Source code in sbi/inference/abc/smcabc.py def get_particle_ranges ( self , particles : Tensor , weights : Tensor , samples_per_dim : int = 100 ) -> Tensor : \"\"\"Return range of particles in each parameter dimension.\"\"\" # get weighted samples samples = self . sample_from_population_with_weights ( particles , weights , num_samples = samples_per_dim * particles . shape [ 1 ], ) # Variance spans the range of particles for every dimension. particle_ranges = tensor ([ max ( column ) - min ( column ) for column in samples . T ]) assert particle_ranges . ndim < 2 return particle_ranges get_sass_transform ( theta , x , expansion_degree = 1 , sample_weight = None ) inherited \u00b6 Return semi-automatic summary statitics function. Running weighted linear regressin as in Fearnhead & Prandle 2012: https://arxiv.org/abs/1004.1112 Following implementation in https://abcpy.readthedocs.io/en/latest/_modules/abcpy/statistics.html#Identity and https://pythonhosted.org/abcpy/_modules/abcpy/summaryselections.html#Semiautomatic Source code in sbi/inference/abc/smcabc.py @staticmethod def get_sass_transform ( theta : torch . Tensor , x : torch . Tensor , expansion_degree : int = 1 , sample_weight = None , ) -> Callable : \"\"\"Return semi-automatic summary statitics function. Running weighted linear regressin as in Fearnhead & Prandle 2012: https://arxiv.org/abs/1004.1112 Following implementation in https://abcpy.readthedocs.io/en/latest/_modules/abcpy/statistics.html#Identity and https://pythonhosted.org/abcpy/_modules/abcpy/summaryselections.html#Semiautomatic \"\"\" expansion = PolynomialFeatures ( degree = expansion_degree , include_bias = False ) # Transform x, remove intercept. x_expanded = expansion . fit_transform ( x ) sumstats_map = np . zeros (( x_expanded . shape [ 1 ], theta . shape [ 1 ])) for parameter_idx in range ( theta . shape [ 1 ]): regression_model = LinearRegression ( fit_intercept = True ) regression_model . fit ( X = x_expanded , y = theta [:, parameter_idx ], sample_weight = sample_weight ) sumstats_map [:, parameter_idx ] = regression_model . coef_ sumstats_map = torch . tensor ( sumstats_map , dtype = torch . float32 ) def sumstats_transform ( x ): x_expanded = torch . tensor ( expansion . fit_transform ( x ), dtype = torch . float32 ) return x_expanded . mm ( sumstats_map ) return sumstats_transform resample_if_ess_too_small ( self , particles , log_weights , ess_min , pop_idx ) \u00b6 Return resampled particles and uniform weights if effectice sampling size is too small. Source code in sbi/inference/abc/smcabc.py def resample_if_ess_too_small ( self , particles : Tensor , log_weights : Tensor , ess_min : float , pop_idx : int , ) -> Tuple [ Tensor , Tensor ]: \"\"\"Return resampled particles and uniform weights if effectice sampling size is too small. \"\"\" num_particles = particles . shape [ 0 ] ess = ( 1 / torch . sum ( torch . exp ( 2.0 * log_weights ), dim = 0 )) / num_particles # Resampling of weights for low ESS only for Sisson et al. 2007. if ess < ess_min : self . logger . info ( f \"ESS= { ess : .2f } too low, resampling pop { pop_idx } ...\" ) # First resample, then set to uniform weights as in Sisson et al. 2007. particles = self . sample_from_population_with_weights ( particles , torch . exp ( log_weights ), num_samples = num_particles ) log_weights = torch . log ( 1 / num_particles * ones ( num_particles )) return particles , log_weights run_lra ( theta , x , observation , sample_weight = None ) inherited \u00b6 Return parameters adjusted with linear regression adjustment. Implementation as in Beaumont et al. 2002: https://arxiv.org/abs/1707.01254 Source code in sbi/inference/abc/smcabc.py @staticmethod def run_lra ( theta : torch . Tensor , x : torch . Tensor , observation : torch . Tensor , sample_weight = None , ) -> torch . Tensor : \"\"\"Return parameters adjusted with linear regression adjustment. Implementation as in Beaumont et al. 2002: https://arxiv.org/abs/1707.01254 \"\"\" theta_adjusted = theta for parameter_idx in range ( theta . shape [ 1 ]): regression_model = LinearRegression ( fit_intercept = True ) regression_model . fit ( X = x , y = theta [:, parameter_idx ], sample_weight = sample_weight , ) theta_adjusted [:, parameter_idx ] += regression_model . predict ( observation . reshape ( 1 , - 1 ) ) theta_adjusted [:, parameter_idx ] -= regression_model . predict ( x ) return theta_adjusted run_lra_update_weights ( self , particles , xs , observation , log_weights , lra_with_weights ) \u00b6 Return particles and weights adjusted with LRA. Runs (weighted) linear regression from xs onto particles to adjust the particles. Updates the SMC weights according to the new particles. Source code in sbi/inference/abc/smcabc.py def run_lra_update_weights ( self , particles : Tensor , xs : Tensor , observation : Tensor , log_weights : Tensor , lra_with_weights : bool , ) -> Tuple [ Tensor , Tensor ]: \"\"\"Return particles and weights adjusted with LRA. Runs (weighted) linear regression from xs onto particles to adjust the particles. Updates the SMC weights according to the new particles. \"\"\" adjusted_particels = self . run_lra ( theta = particles , x = xs , observation = observation , sample_weight = log_weights . exp () if lra_with_weights else None , ) # Update SMC weights with LRA adjusted weights adjusted_log_weights = self . _calculate_new_log_weights ( new_particles = adjusted_particels , old_particles = particles , old_log_weights = log_weights , ) return adjusted_particels , adjusted_log_weights run_sass_set_xo ( self , num_particles , num_pilot_simulations , x_o , lra = False , sass_expansion_degree = 1 ) \u00b6 Return transform for semi-automatic summary statistics. Runs an single round of rejection abc with fixed budget and accepts num_particles simulations to run the regression for sass. Sets self.x_o once the x_shape can be derived from simulations. Source code in sbi/inference/abc/smcabc.py def run_sass_set_xo ( self , num_particles : int , num_pilot_simulations : int , x_o , lra : bool = False , sass_expansion_degree : int = 1 , ) -> Callable : \"\"\"Return transform for semi-automatic summary statistics. Runs an single round of rejection abc with fixed budget and accepts num_particles simulations to run the regression for sass. Sets self.x_o once the x_shape can be derived from simulations. \"\"\" ( pilot_particles , _ , _ , pilot_xs ,) = self . _set_xo_and_sample_initial_population ( x_o , num_particles , num_pilot_simulations ) # Adjust with LRA. if lra : pilot_particles = self . run_lra ( pilot_particles , pilot_xs , self . x_o ) sass_transform = self . get_sass_transform ( pilot_particles , pilot_xs , expansion_degree = sass_expansion_degree , sample_weight = None , ) return sass_transform sample_from_population_with_weights ( particles , weights , num_samples = 1 ) staticmethod \u00b6 Return samples from particles sampled with weights. Source code in sbi/inference/abc/smcabc.py @staticmethod def sample_from_population_with_weights ( particles : Tensor , weights : Tensor , num_samples : int = 1 ) -> Tensor : \"\"\"Return samples from particles sampled with weights.\"\"\" # define multinomial with weights as probs multi = Multinomial ( probs = weights ) # sample num samples, with replacement samples = multi . sample ( sample_shape = ( num_samples ,)) # get indices of success trials indices = torch . where ( samples )[ 1 ] # return those indices from trace return particles [ indices ] Posteriors \u00b6 sbi.inference.posteriors.direct_posterior.DirectPosterior ( NeuralPosterior ) \u00b6 Posterior \\(p(\\theta|x)\\) with log_prob() and sample() methods, obtained with SNPE. SNPE trains a neural network to directly approximate the posterior distribution. However, for bounded priors, the neural network can have leakage: it puts non-zero mass in regions where the prior is zero. The DirectPosterior class wraps the trained network to deal with these cases. Specifically, this class offers the following functionality: - correct the calculation of the log probability such that it compensates for the leakage. - reject samples that lie outside of the prior bounds. - alternatively, if leakage is very high (which can happen for multi-round SNPE), sample from the posterior with MCMC. The neural network itself can be accessed via the .net attribute. default_x : Optional [ torch . Tensor ] inherited property writable \u00b6 Return default x used by .sample(), .log_prob as conditioning context. mcmc_method : str inherited property writable \u00b6 Returns MCMC method. mcmc_parameters : dict inherited property writable \u00b6 Returns MCMC parameters. rejection_sampling_parameters : dict inherited property writable \u00b6 Returns rejection sampling parameters. sample_with : str inherited property writable \u00b6 Return True if NeuralPosterior instance should use MCMC in .sample() . __init__ ( self , method_family , neural_net , prior , x_shape , sample_with = 'rejection' , mcmc_method = 'slice_np' , mcmc_parameters = None , rejection_sampling_parameters = None , device = 'cpu' ) special \u00b6 Parameters: Name Type Description Default method_family str One of snpe, snl, snre_a or snre_b. required neural_net Module A classifier for SNRE, a density estimator for SNPE and SNL. required prior Prior distribution with .log_prob() and .sample() . required x_shape Size Shape of a single simulator output. required sample_with str Method to use for sampling from the posterior. Must be one of [ mcmc | rejection ]. With default parameters, rejection samples from the posterior estimated by the neural net and rejects only if the samples are outside of the prior support. 'rejection' mcmc_method str Method used for MCMC sampling, one of slice_np , slice , hmc , nuts . Currently defaults to slice_np for a custom numpy implementation of slice sampling; select hmc , nuts or slice for Pyro-based sampling. 'slice_np' mcmc_parameters Optional[Dict[str, Any]] Dictionary overriding the default parameters for MCMC. The following parameters are supported: thin to set the thinning factor for the chain, warmup_steps to set the initial number of samples to discard, num_chains for the number of chains, init_strategy for the initialisation strategy for chains; prior will draw init locations from prior, whereas sir will use Sequential- Importance-Resampling using init_strategy_num_candidates to find init locations. None rejection_sampling_parameters Optional[Dict[str, Any]] Dictionary overriding the default parameters for rejection sampling. The following parameters are supported: proposal as the proposal distribtution (default is the trained neural net). max_sampling_batch_size as the batchsize of samples being drawn from the proposal at every iteration. num_samples_to_find_max as the number of samples that are used to find the maximum of the potential_fn / proposal ratio. num_iter_to_find_max as the number of gradient ascent iterations to find the maximum of that ratio. m as multiplier to that ratio. None device str Training device, e.g., \u201ccpu\u201d, \u201ccuda\u201d or \u201ccuda:{0, 1, \u2026}\u201d. 'cpu' Source code in sbi/inference/posteriors/direct_posterior.py def __init__ ( self , method_family : str , neural_net : nn . Module , prior , x_shape : torch . Size , sample_with : str = \"rejection\" , mcmc_method : str = \"slice_np\" , mcmc_parameters : Optional [ Dict [ str , Any ]] = None , rejection_sampling_parameters : Optional [ Dict [ str , Any ]] = None , device : str = \"cpu\" , ): \"\"\" Args: method_family: One of snpe, snl, snre_a or snre_b. neural_net: A classifier for SNRE, a density estimator for SNPE and SNL. prior: Prior distribution with `.log_prob()` and `.sample()`. x_shape: Shape of a single simulator output. sample_with: Method to use for sampling from the posterior. Must be one of [`mcmc` | `rejection`]. With default parameters, `rejection` samples from the posterior estimated by the neural net and rejects only if the samples are outside of the prior support. mcmc_method: Method used for MCMC sampling, one of `slice_np`, `slice`, `hmc`, `nuts`. Currently defaults to `slice_np` for a custom numpy implementation of slice sampling; select `hmc`, `nuts` or `slice` for Pyro-based sampling. mcmc_parameters: Dictionary overriding the default parameters for MCMC. The following parameters are supported: `thin` to set the thinning factor for the chain, `warmup_steps` to set the initial number of samples to discard, `num_chains` for the number of chains, `init_strategy` for the initialisation strategy for chains; `prior` will draw init locations from prior, whereas `sir` will use Sequential- Importance-Resampling using `init_strategy_num_candidates` to find init locations. rejection_sampling_parameters: Dictionary overriding the default parameters for rejection sampling. The following parameters are supported: `proposal` as the proposal distribtution (default is the trained neural net). `max_sampling_batch_size` as the batchsize of samples being drawn from the proposal at every iteration. `num_samples_to_find_max` as the number of samples that are used to find the maximum of the `potential_fn / proposal` ratio. `num_iter_to_find_max` as the number of gradient ascent iterations to find the maximum of that ratio. `m` as multiplier to that ratio. device: Training device, e.g., \"cpu\", \"cuda\" or \"cuda:{0, 1, ...}\". \"\"\" kwargs = del_entries ( locals (), entries = ( \"self\" , \"__class__\" )) super () . __init__ ( ** kwargs ) self . _purpose = ( \"It allows to .sample() and .log_prob() the posterior and wraps the \" \"output of the .net to avoid leakage into regions with 0 prior probability.\" ) copy_hyperparameters_from ( self , posterior ) inherited \u00b6 Copies the hyperparameters from a given posterior to self . The hyperparameters that are copied are: Sampling parameters (MCMC for all methods, rejection sampling for SNPE). default_x at which to evaluate the posterior. Parameters: Name Type Description Default posterior NeuralPosterior Posterior that the hyperparameters are copied from. required Returns: Type Description Posterior object with the same hyperparameters as the passed posterior. This makes the call chainable posterior = infer.build_posterior().copy_hyperparameters_from(proposal) Source code in sbi/inference/posteriors/direct_posterior.py def copy_hyperparameters_from ( self , posterior : \"NeuralPosterior\" ) -> \"NeuralPosterior\" : \"\"\" Copies the hyperparameters from a given posterior to `self`. The hyperparameters that are copied are: - Sampling parameters (MCMC for all methods, rejection sampling for SNPE). - `default_x` at which to evaluate the posterior. Args: posterior: Posterior that the hyperparameters are copied from. Returns: Posterior object with the same hyperparameters as the passed posterior. This makes the call chainable: `posterior = infer.build_posterior().copy_hyperparameters_from(proposal)` \"\"\" assert isinstance ( posterior , NeuralPosterior ), \"`copy_state_from` must be a `NeuralPosterior`.\" self . set_mcmc_method ( posterior . _mcmc_method ) self . set_mcmc_parameters ( posterior . _mcmc_parameters ) self . set_default_x ( posterior . default_x ) self . _mcmc_init_params = posterior . _mcmc_init_params if hasattr ( self , \"_sample_with_mcmc\" ): self . set_sample_with_mcmc ( posterior . _sample_with_mcmc ) if hasattr ( self , \"_rejection_sampling_parameters\" ): self . set_rejection_sampling_parameters ( posterior . _rejection_sampling_parameters ) return self leakage_correction ( self , x , num_rejection_samples = 10000 , force_update = False , show_progress_bars = False , rejection_sampling_batch_size = 10000 ) \u00b6 Return leakage correction factor for a leaky posterior density estimate. The factor is estimated from the acceptance probability during rejection sampling from the posterior. This is to avoid re-estimating the acceptance probability from scratch whenever log_prob is called and norm_posterior=True . Here, it is estimated only once for self.default_x and saved for later. We re-evaluate only whenever a new x is passed. Parameters: Name Type Description Default x Tensor Conditioning context for posterior \\(p(\\theta|x)\\) . required num_rejection_samples int Number of samples used to estimate correction factor. 10000 force_update bool Whether to force a reevaluation of the leakage correction even if the context x is the same as self.default_x . This is useful to enforce a new leakage estimate for rounds after the first (2, 3,..). False show_progress_bars bool Whether to show a progress bar during sampling. False rejection_sampling_batch_size int Batch size for rejection sampling. 10000 Returns: Type Description Tensor Saved or newly-estimated correction factor (as a scalar Tensor ). Source code in sbi/inference/posteriors/direct_posterior.py @torch . no_grad () def leakage_correction ( self , x : Tensor , num_rejection_samples : int = 10_000 , force_update : bool = False , show_progress_bars : bool = False , rejection_sampling_batch_size : int = 10_000 , ) -> Tensor : r \"\"\"Return leakage correction factor for a leaky posterior density estimate. The factor is estimated from the acceptance probability during rejection sampling from the posterior. This is to avoid re-estimating the acceptance probability from scratch whenever `log_prob` is called and `norm_posterior=True`. Here, it is estimated only once for `self.default_x` and saved for later. We re-evaluate only whenever a new `x` is passed. Arguments: x: Conditioning context for posterior $p(\\theta|x)$. num_rejection_samples: Number of samples used to estimate correction factor. force_update: Whether to force a reevaluation of the leakage correction even if the context `x` is the same as `self.default_x`. This is useful to enforce a new leakage estimate for rounds after the first (2, 3,..). show_progress_bars: Whether to show a progress bar during sampling. rejection_sampling_batch_size: Batch size for rejection sampling. Returns: Saved or newly-estimated correction factor (as a scalar `Tensor`). \"\"\" def acceptance_at ( x : Tensor ) -> Tensor : return utils . rejection_sample_posterior_within_prior ( posterior_nn = self . net , prior = self . _prior , x = x . to ( self . _device ), num_samples = num_rejection_samples , show_progress_bars = show_progress_bars , sample_for_correction_factor = True , max_sampling_batch_size = rejection_sampling_batch_size , )[ 1 ] # Check if the provided x matches the default x (short-circuit on identity). is_new_x = self . default_x is None or ( x is not self . default_x and ( x != self . default_x ) . any () ) not_saved_at_default_x = self . _leakage_density_correction_factor is None if is_new_x : # Calculate at x; don't save. return acceptance_at ( x ) elif not_saved_at_default_x or force_update : # Calculate at default_x; save. self . _leakage_density_correction_factor = acceptance_at ( self . default_x ) return self . _leakage_density_correction_factor # type:ignore log_prob ( self , theta , x = None , norm_posterior = True , track_gradients = False , leakage_correction_params = None ) \u00b6 Returns the log-probability of the posterior \\(p(\\theta|x).\\) Parameters: Name Type Description Default theta Tensor Parameters \\(\\theta\\) . required x Optional[torch.Tensor] Conditioning context for posterior \\(p(\\theta|x)\\) . If not provided, fall back onto x passed to set_default_x() . None norm_posterior bool Whether to enforce a normalized posterior density. Renormalization of the posterior is useful when some probability falls out or leaks out of the prescribed prior support. The normalizing factor is calculated via rejection sampling, so if you need speedier but unnormalized log posterior estimates set here norm_posterior=False . The returned log posterior is set to -\u221e outside of the prior support regardless of this setting. True track_gradients bool Whether the returned tensor supports tracking gradients. This can be helpful for e.g. sensitivity analysis, but increases memory consumption. False leakage_correction_params Optional[dict] A dict of keyword arguments to override the default values of leakage_correction() . Possible options are: num_rejection_samples , force_update , show_progress_bars , and rejection_sampling_batch_size . These parameters only have an effect if norm_posterior=True . None Returns: Type Description Tensor (len(\u03b8),) -shaped log posterior probability \\(\\log p(\\theta|x)\\) for \u03b8 in the support of the prior, -\u221e (corresponding to 0 probability) outside. Source code in sbi/inference/posteriors/direct_posterior.py def log_prob ( self , theta : Tensor , x : Optional [ Tensor ] = None , norm_posterior : bool = True , track_gradients : bool = False , leakage_correction_params : Optional [ dict ] = None , ) -> Tensor : r \"\"\" Returns the log-probability of the posterior $p(\\theta|x).$ Args: theta: Parameters $\\theta$. x: Conditioning context for posterior $p(\\theta|x)$. If not provided, fall back onto `x` passed to `set_default_x()`. norm_posterior: Whether to enforce a normalized posterior density. Renormalization of the posterior is useful when some probability falls out or leaks out of the prescribed prior support. The normalizing factor is calculated via rejection sampling, so if you need speedier but unnormalized log posterior estimates set here `norm_posterior=False`. The returned log posterior is set to -\u221e outside of the prior support regardless of this setting. track_gradients: Whether the returned tensor supports tracking gradients. This can be helpful for e.g. sensitivity analysis, but increases memory consumption. leakage_correction_params: A `dict` of keyword arguments to override the default values of `leakage_correction()`. Possible options are: `num_rejection_samples`, `force_update`, `show_progress_bars`, and `rejection_sampling_batch_size`. These parameters only have an effect if `norm_posterior=True`. Returns: `(len(\u03b8),)`-shaped log posterior probability $\\log p(\\theta|x)$ for \u03b8 in the support of the prior, -\u221e (corresponding to 0 probability) outside. \"\"\" # TODO Train exited here, entered after sampling? self . net . eval () theta , x = self . _prepare_theta_and_x_for_log_prob_ ( theta , x ) theta_repeated , x_repeated = self . _match_theta_and_x_batch_shapes ( theta , x ) with torch . set_grad_enabled ( track_gradients ): # Evaluate on device, move back to cpu for comparison with prior. unnorm_log_prob = self . net . log_prob ( theta_repeated , x_repeated ) # Force probability to be zero outside prior support. in_prior_support = within_support ( self . _prior , theta ) masked_log_prob = torch . where ( in_prior_support , unnorm_log_prob , torch . tensor ( float ( \"-inf\" ), dtype = torch . float32 , device = self . _device ), ) if leakage_correction_params is None : leakage_correction_params = dict () # use defaults log_factor = ( log ( self . leakage_correction ( x = batched_first_of_batch ( x ), ** leakage_correction_params ) ) if norm_posterior else 0 ) return masked_log_prob - log_factor log_prob_conditional ( self , theta , condition , dims_to_evaluate , x = None ) \u00b6 Evaluates the conditional posterior probability of a MDN at a context x for a value theta given a condition. This function only works for MDN based posteriors, becuase evaluation is done analytically. For all other density estimators a NotImplementedError will be raised! Parameters: Name Type Description Default theta Tensor Parameters $ heta$. required condition Tensor Parameter set that all dimensions not specified in dims_to_sample will be fixed to. Should contain dim_theta elements, i.e. it could e.g. be a sample from the posterior distribution. The entries at all dims_to_sample will be ignored. required dims_to_evaluate List[int] Which dimensions to evaluate the sample for. The dimensions not specified in dims_to_evaluate will be fixed to values given in condition . required x Optional[torch.Tensor] Conditioning context for posterior \\(p( heta|x)\\) . If not provided, fall back onto x passed to set_default_x() . None Returns: Type Description log_prob (len(\u03b8),) -shaped normalized (!) log posterior probability $\\log p( heta|x) for \u03b8 in the support of the prior, -\u221e (corresponding to 0 probability) outside. Source code in sbi/inference/posteriors/direct_posterior.py def log_prob_conditional ( self , theta : Tensor , condition : Tensor , dims_to_evaluate : List [ int ], x : Optional [ Tensor ] = None , ) -> Tensor : \"\"\"Evaluates the conditional posterior probability of a MDN at a context x for a value theta given a condition. This function only works for MDN based posteriors, becuase evaluation is done analytically. For all other density estimators a `NotImplementedError` will be raised! Args: theta: Parameters $\\theta$. condition: Parameter set that all dimensions not specified in `dims_to_sample` will be fixed to. Should contain dim_theta elements, i.e. it could e.g. be a sample from the posterior distribution. The entries at all `dims_to_sample` will be ignored. dims_to_evaluate: Which dimensions to evaluate the sample for. The dimensions not specified in `dims_to_evaluate` will be fixed to values given in `condition`. x: Conditioning context for posterior $p(\\theta|x)$. If not provided, fall back onto `x` passed to `set_default_x()`. Returns: log_prob: `(len(\u03b8),)`-shaped normalized (!) log posterior probability $\\log p(\\theta|x) for \u03b8 in the support of the prior, -\u221e (corresponding to 0 probability) outside. \"\"\" if type ( self . net . _distribution ) == mdn : logits , means , precfs , sumlogdiag = extract_and_transform_mog ( self , x ) logits , means , precfs , sumlogdiag = condition_mog ( self . _prior , condition , dims_to_evaluate , logits , means , precfs ) batch_size , dim = theta . shape prec = precfs . transpose ( 3 , 2 ) @ precfs self . net . eval () # leakage correction requires eval mode if dim != len ( dims_to_evaluate ): X = X [:, dims_to_evaluate ] # Implementing leakage correction is difficult for conditioned MDNs, # because samples from self i.e. the full posterior are used rather # then from the new, conditioned posterior. warn ( \"Probabilities are not adjusted for leakage.\" ) log_prob = mdn . log_prob_mog ( theta , logits . repeat ( batch_size , 1 ), means . repeat ( batch_size , 1 , 1 ), prec . repeat ( batch_size , 1 , 1 , 1 ), sumlogdiag . repeat ( batch_size , 1 ), ) self . net . train ( True ) return log_prob . detach () else : raise NotImplementedError ( \"This functionality is only available for MDN based posteriors.\" ) map ( self , x = None , num_iter = 1000 , learning_rate = 0.01 , init_method = 'posterior' , num_init_samples = 1000 , num_to_optimize = 100 , save_best_every = 10 , show_progress_bars = True ) \u00b6 Returns the maximum-a-posteriori estimate (MAP). The method can be interrupted (Ctrl-C) when the user sees that the log-probability converges. The best estimate will be saved in self.map_ . The MAP is obtained by running gradient ascent from a given number of starting positions (samples from the posterior with the highest log-probability). After the optimization is done, we select the parameter set that has the highest log-probability after the optimization. Warning: The default values used by this function are not well-tested. They might require hand-tuning for the problem at hand. Parameters: Name Type Description Default x Optional[torch.Tensor] Conditioning context for posterior \\(p( heta|x)\\) . If not provided, fall back onto x passed to set_default_x() . None num_iter int Number of optimization steps that the algorithm takes to find the MAP. 1000 learning_rate float Learning rate of the optimizer. 0.01 init_method Union[str, torch.Tensor] How to select the starting parameters for the optimization. If it is a string, it can be either [ posterior , prior ], which samples the respective distribution num_init_samples times. If it is a, the tensor will be used as init locations. 'posterior' num_init_samples int Draw this number of samples from the posterior and evaluate the log-probability of all of them. 1000 num_to_optimize int From the drawn num_init_samples , use the num_to_optimize with highest log-probability as the initial points for the optimization. 100 save_best_every int The best log-probability is computed, saved in the map -attribute, and printed every print_best_every -th iteration. Computing the best log-probability creates a significant overhead (thus, the default is 10 .) 10 show_progress_bars bool Whether or not to show a progressbar for sampling from the posterior. True Returns: Type Description Tensor The MAP estimate. Source code in sbi/inference/posteriors/direct_posterior.py def map ( self , x : Optional [ Tensor ] = None , num_iter : int = 1000 , learning_rate : float = 1e-2 , init_method : Union [ str , Tensor ] = \"posterior\" , num_init_samples : int = 1_000 , num_to_optimize : int = 100 , save_best_every : int = 10 , show_progress_bars : bool = True , ) -> Tensor : \"\"\" Returns the maximum-a-posteriori estimate (MAP). The method can be interrupted (Ctrl-C) when the user sees that the log-probability converges. The best estimate will be saved in `self.map_`. The MAP is obtained by running gradient ascent from a given number of starting positions (samples from the posterior with the highest log-probability). After the optimization is done, we select the parameter set that has the highest log-probability after the optimization. Warning: The default values used by this function are not well-tested. They might require hand-tuning for the problem at hand. Args: x: Conditioning context for posterior $p(\\theta|x)$. If not provided, fall back onto `x` passed to `set_default_x()`. num_iter: Number of optimization steps that the algorithm takes to find the MAP. learning_rate: Learning rate of the optimizer. init_method: How to select the starting parameters for the optimization. If it is a string, it can be either [`posterior`, `prior`], which samples the respective distribution `num_init_samples` times. If it is a, the tensor will be used as init locations. num_init_samples: Draw this number of samples from the posterior and evaluate the log-probability of all of them. num_to_optimize: From the drawn `num_init_samples`, use the `num_to_optimize` with highest log-probability as the initial points for the optimization. save_best_every: The best log-probability is computed, saved in the `map`-attribute, and printed every `print_best_every`-th iteration. Computing the best log-probability creates a significant overhead (thus, the default is `10`.) show_progress_bars: Whether or not to show a progressbar for sampling from the posterior. Returns: The MAP estimate. \"\"\" return super () . map ( x = x , num_iter = num_iter , learning_rate = learning_rate , init_method = init_method , num_init_samples = num_init_samples , num_to_optimize = num_to_optimize , save_best_every = save_best_every , show_progress_bars = show_progress_bars , log_prob_kwargs = { \"norm_posterior\" : False }, ) sample ( self , sample_shape = torch . Size ([]), x = None , show_progress_bars = True , sample_with = None , mcmc_method = None , mcmc_parameters = None , rejection_sampling_parameters = None , sample_with_mcmc = None ) \u00b6 Return samples from posterior distribution \\(p(\\theta|x)\\) . Samples are obtained either with rejection sampling or MCMC. Rejection sampling will be a lot faster if leakage is rather low. If leakage is high (e.g. over 99%, which can happen in multi-round SNPE), MCMC can be faster than rejection sampling. Parameters: Name Type Description Default sample_shape Union[torch.Size, Tuple[int, ...]] Desired shape of samples that are drawn from posterior. If sample_shape is multidimensional we simply draw sample_shape.numel() samples and then reshape into the desired shape. torch.Size([]) x Optional[torch.Tensor] Conditioning context for posterior \\(p(\\theta|x)\\) . If not provided, fall back onto x passed to set_default_x() . None show_progress_bars bool Whether to show sampling progress monitor. True sample_with Optional[str] Method to use for sampling from the posterior. Must be one of [ mcmc | rejection ]. With default parameters, rejection samples from the posterior estimated by the neural net and rejects only if the samples are outside of the prior support. None mcmc_method Optional[str] Optional parameter to override self.mcmc_method . None mcmc_parameters Optional[Dict[str, Any]] Dictionary overriding the default parameters for MCMC. The following parameters are supported: thin to set the thinning factor for the chain. warmup_steps to set the initial number of samples to discard. num_chains for the number of chains. init_strategy for the initialisation strategy for chains; prior will draw init locations from prior, whereas sir will use Sequential- Importance-Resampling using init_strategy_num_candidates to find init locations. enable_transform a bool indicating whether MCMC is performed in z-scored (and unconstrained) space. None rejection_sampling_parameters Optional[Dict[str, Any]] Dictionary overriding the default parameters for rejection sampling. The following parameters are supported: proposal as the proposal distribtution (default is the trained neural net). max_sampling_batch_size as the batchsize of samples being drawn from the proposal at every iteration. num_samples_to_find_max as the number of samples that are used to find the maximum of the potential_fn / proposal ratio. num_iter_to_find_max as the number of gradient ascent iterations to find the maximum of that ratio. m as multiplier to that ratio. None sample_with_mcmc Optional[bool] Deprecated since sbi v0.17.0 . Use sample_with=mcmc instead. None Returns: Type Description Tensor Samples from posterior. Source code in sbi/inference/posteriors/direct_posterior.py def sample ( self , sample_shape : Shape = torch . Size (), x : Optional [ Tensor ] = None , show_progress_bars : bool = True , sample_with : Optional [ str ] = None , mcmc_method : Optional [ str ] = None , mcmc_parameters : Optional [ Dict [ str , Any ]] = None , rejection_sampling_parameters : Optional [ Dict [ str , Any ]] = None , sample_with_mcmc : Optional [ bool ] = None , ) -> Tensor : r \"\"\" Return samples from posterior distribution $p(\\theta|x)$. Samples are obtained either with rejection sampling or MCMC. Rejection sampling will be a lot faster if leakage is rather low. If leakage is high (e.g. over 99%, which can happen in multi-round SNPE), MCMC can be faster than rejection sampling. Args: sample_shape: Desired shape of samples that are drawn from posterior. If sample_shape is multidimensional we simply draw `sample_shape.numel()` samples and then reshape into the desired shape. x: Conditioning context for posterior $p(\\theta|x)$. If not provided, fall back onto `x` passed to `set_default_x()`. show_progress_bars: Whether to show sampling progress monitor. sample_with: Method to use for sampling from the posterior. Must be one of [`mcmc` | `rejection`]. With default parameters, `rejection` samples from the posterior estimated by the neural net and rejects only if the samples are outside of the prior support. mcmc_method: Optional parameter to override `self.mcmc_method`. mcmc_parameters: Dictionary overriding the default parameters for MCMC. The following parameters are supported: `thin` to set the thinning factor for the chain. `warmup_steps` to set the initial number of samples to discard. `num_chains` for the number of chains. `init_strategy` for the initialisation strategy for chains; `prior` will draw init locations from prior, whereas `sir` will use Sequential- Importance-Resampling using `init_strategy_num_candidates` to find init locations. `enable_transform` a bool indicating whether MCMC is performed in z-scored (and unconstrained) space. rejection_sampling_parameters: Dictionary overriding the default parameters for rejection sampling. The following parameters are supported: `proposal` as the proposal distribtution (default is the trained neural net). `max_sampling_batch_size` as the batchsize of samples being drawn from the proposal at every iteration. `num_samples_to_find_max` as the number of samples that are used to find the maximum of the `potential_fn / proposal` ratio. `num_iter_to_find_max` as the number of gradient ascent iterations to find the maximum of that ratio. `m` as multiplier to that ratio. sample_with_mcmc: Deprecated since `sbi v0.17.0`. Use `sample_with=mcmc` instead. Returns: Samples from posterior. \"\"\" if sample_with_mcmc is not None : warn ( f \"You set `sample_with_mcmc= { sample_with_mcmc } `. This is deprecated \" \"since `sbi v0.17.0` and will lead to an error in future versions. \" \"Please use `sample_with='mcmc'` instead.\" ) if sample_with_mcmc : sample_with = \"mcmc\" self . net . eval () sample_with = sample_with if sample_with is not None else self . _sample_with x , num_samples = self . _prepare_for_sample ( x , sample_shape ) potential_fn_provider = PotentialFunctionProvider () if sample_with == \"mcmc\" : mcmc_method , mcmc_parameters = self . _potentially_replace_mcmc_parameters ( mcmc_method , mcmc_parameters ) transform = mcmc_transform ( self . _prior , device = self . _device , ** mcmc_parameters ) transformed_samples = self . _sample_posterior_mcmc ( num_samples = num_samples , potential_fn = potential_fn_provider ( self . _prior , self . net , x , mcmc_method , transform ), init_fn = self . _build_mcmc_init_fn ( self . _prior , potential_fn_provider ( self . _prior , self . net , x , \"slice_np\" , transform ), transform = transform , ** mcmc_parameters , ), mcmc_method = mcmc_method , show_progress_bars = show_progress_bars , ** mcmc_parameters , ) samples = transform . inv ( transformed_samples ) elif sample_with == \"rejection\" : rejection_sampling_parameters = ( self . _potentially_replace_rejection_parameters ( rejection_sampling_parameters ) ) if \"proposal\" not in rejection_sampling_parameters : assert ( not self . net . training ), \"Posterior nn must be in eval mode for sampling.\" # If the user does not explictly pass a `proposal`, we sample from the # neural net estimating the posterior and reject only those samples # that are outside of the prior support. This can be considered as # rejection sampling with a very good proposal. samples = utils . rejection_sample_posterior_within_prior ( posterior_nn = self . net , prior = self . _prior , x = x . to ( self . _device ), num_samples = num_samples , show_progress_bars = show_progress_bars , sample_for_correction_factor = True , ** rejection_sampling_parameters , )[ 0 ] else : samples , _ = rejection_sample ( potential_fn = potential_fn_provider ( self . _prior , self . net , x , \"rejection\" ), num_samples = num_samples , show_progress_bars = show_progress_bars , ** rejection_sampling_parameters , ) else : raise NameError ( \"The only implemented sampling methods are `mcmc` and `rejection`.\" ) self . net . train ( True ) return samples . reshape (( * sample_shape , - 1 )) sample_conditional ( self , sample_shape , condition , dims_to_sample , x = None , sample_with = 'mcmc' , show_progress_bars = True , mcmc_method = None , mcmc_parameters = None , rejection_sampling_parameters = None ) \u00b6 Return samples from conditional posterior \\(p(\\theta_i|\\theta_j, x)\\) . In this function, we do not sample from the full posterior, but instead only from a few parameter dimensions while the other parameter dimensions are kept fixed at values specified in condition . Samples are obtained with MCMC. Parameters: Name Type Description Default sample_shape Union[torch.Size, Tuple[int, ...]] Desired shape of samples that are drawn from posterior. If sample_shape is multidimensional we simply draw sample_shape.numel() samples and then reshape into the desired shape. required condition Tensor Parameter set that all dimensions not specified in dims_to_sample will be fixed to. Should contain dim_theta elements, i.e. it could e.g. be a sample from the posterior distribution. The entries at all dims_to_sample will be ignored. required dims_to_sample List[int] Which dimensions to sample from. The dimensions not specified in dims_to_sample will be fixed to values given in condition . required x Optional[torch.Tensor] Conditioning context for posterior \\(p(\\theta|x)\\) . If not provided, fall back onto x passed to set_default_x() . None sample_with str Method to use for sampling from the posterior. Must be one of [ mcmc | rejection ]. In this method, the value of self.sample_with will be ignored. 'mcmc' show_progress_bars bool Whether to show sampling progress monitor. True mcmc_method Optional[str] Optional parameter to override self.mcmc_method . None mcmc_parameters Optional[Dict[str, Any]] Dictionary overriding the default parameters for MCMC. The following parameters are supported: thin to set the thinning factor for the chain. warmup_steps to set the initial number of samples to discard. num_chains for the number of chains. init_strategy for the initialisation strategy for chains; prior will draw init locations from prior, whereas sir will use Sequential- Importance-Resampling using init_strategy_num_candidates to find init locations. enable_transform a bool indicating whether MCMC is performed in z-scored (and unconstrained) space. None rejection_sampling_parameters Optional[Dict[str, Any]] Dictionary overriding the default parameters for rejection sampling. The following parameters are supported: proposal as the proposal distribtution (default is the prior). max_sampling_batch_size as the batchsize of samples being drawn from the proposal at every iteration. num_samples_to_find_max as the number of samples that are used to find the maximum of the potential_fn / proposal ratio. num_iter_to_find_max as the number of gradient ascent iterations to find the maximum of that ratio. m as multiplier to that ratio. None Returns: Type Description Tensor Samples from conditional posterior. Source code in sbi/inference/posteriors/direct_posterior.py def sample_conditional ( self , sample_shape : Shape , condition : Tensor , dims_to_sample : List [ int ], x : Optional [ Tensor ] = None , sample_with : str = \"mcmc\" , show_progress_bars : bool = True , mcmc_method : Optional [ str ] = None , mcmc_parameters : Optional [ Dict [ str , Any ]] = None , rejection_sampling_parameters : Optional [ Dict [ str , Any ]] = None , ) -> Tensor : r \"\"\" Return samples from conditional posterior $p(\\theta_i|\\theta_j, x)$. In this function, we do not sample from the full posterior, but instead only from a few parameter dimensions while the other parameter dimensions are kept fixed at values specified in `condition`. Samples are obtained with MCMC. Args: sample_shape: Desired shape of samples that are drawn from posterior. If sample_shape is multidimensional we simply draw `sample_shape.numel()` samples and then reshape into the desired shape. condition: Parameter set that all dimensions not specified in `dims_to_sample` will be fixed to. Should contain dim_theta elements, i.e. it could e.g. be a sample from the posterior distribution. The entries at all `dims_to_sample` will be ignored. dims_to_sample: Which dimensions to sample from. The dimensions not specified in `dims_to_sample` will be fixed to values given in `condition`. x: Conditioning context for posterior $p(\\theta|x)$. If not provided, fall back onto `x` passed to `set_default_x()`. sample_with: Method to use for sampling from the posterior. Must be one of [`mcmc` | `rejection`]. In this method, the value of `self.sample_with` will be ignored. show_progress_bars: Whether to show sampling progress monitor. mcmc_method: Optional parameter to override `self.mcmc_method`. mcmc_parameters: Dictionary overriding the default parameters for MCMC. The following parameters are supported: `thin` to set the thinning factor for the chain. `warmup_steps` to set the initial number of samples to discard. `num_chains` for the number of chains. `init_strategy` for the initialisation strategy for chains; `prior` will draw init locations from prior, whereas `sir` will use Sequential- Importance-Resampling using `init_strategy_num_candidates` to find init locations. `enable_transform` a bool indicating whether MCMC is performed in z-scored (and unconstrained) space. rejection_sampling_parameters: Dictionary overriding the default parameters for rejection sampling. The following parameters are supported: `proposal` as the proposal distribtution (default is the prior). `max_sampling_batch_size` as the batchsize of samples being drawn from the proposal at every iteration. `num_samples_to_find_max` as the number of samples that are used to find the maximum of the `potential_fn / proposal` ratio. `num_iter_to_find_max` as the number of gradient ascent iterations to find the maximum of that ratio. `m` as multiplier to that ratio. Returns: Samples from conditional posterior. \"\"\" if not hasattr ( self . net , \"_distribution\" ): raise NotImplementedError ( \"`sample_conditional` is not implemented for SNPE-A.\" ) net = self . net x = atleast_2d_float32_tensor ( self . _x_else_default_x ( x )) if type ( net . _distribution ) is mdn : condition = atleast_2d_float32_tensor ( condition ) num_samples = torch . Size ( sample_shape ) . numel () logits , means , precfs , _ = extract_and_transform_mog ( nn = net , context = x ) logits , means , precfs , _ = condition_mog ( self . _prior , condition , dims_to_sample , logits , means , precfs ) # Currently difficult to integrate `sample_posterior_within_prior`. warn ( \"Sampling MoG analytically. \" \"Some of the samples might not be within the prior support!\" ) samples = mdn . sample_mog ( num_samples , logits , means , precfs ) return samples . detach () . reshape (( * sample_shape , - 1 )) else : return super () . sample_conditional ( PotentialFunctionProvider (), sample_shape , condition , dims_to_sample , x , sample_with , show_progress_bars , mcmc_method , mcmc_parameters , rejection_sampling_parameters , ) set_default_x ( self , x ) inherited \u00b6 Set new default x for .sample(), .log_prob to use as conditioning context. This is a pure convenience to avoid having to repeatedly specify x in calls to .sample() and .log_prob() - only \u03b8 needs to be passed. This convenience is particularly useful when the posterior is focused, i.e. has been trained over multiple rounds to be accurate in the vicinity of a particular x=x_o (you can check if your posterior object is focused by printing it). NOTE: this method is chainable, i.e. will return the NeuralPosterior object so that calls like posterior.set_default_x(my_x).sample(mytheta) are possible. Parameters: Name Type Description Default x Tensor The default observation to set for the posterior \\(p(theta|x)\\) . required Returns: Type Description NeuralPosterior NeuralPosterior that will use a default x when not explicitly passed. Source code in sbi/inference/posteriors/direct_posterior.py def set_default_x ( self , x : Tensor ) -> \"NeuralPosterior\" : \"\"\"Set new default x for `.sample(), .log_prob` to use as conditioning context. This is a pure convenience to avoid having to repeatedly specify `x` in calls to `.sample()` and `.log_prob()` - only \u03b8 needs to be passed. This convenience is particularly useful when the posterior is focused, i.e. has been trained over multiple rounds to be accurate in the vicinity of a particular `x=x_o` (you can check if your posterior object is focused by printing it). NOTE: this method is chainable, i.e. will return the NeuralPosterior object so that calls like `posterior.set_default_x(my_x).sample(mytheta)` are possible. Args: x: The default observation to set for the posterior $p(theta|x)$. Returns: `NeuralPosterior` that will use a default `x` when not explicitly passed. \"\"\" self . _x = process_x ( x , self . _x_shape , allow_iid_x = self . _allow_iid_x ) . to ( self . _device ) self . _num_iid_trials = self . _x . shape [ 0 ] return self set_mcmc_method ( self , method ) inherited \u00b6 Sets sampling method to for MCMC and returns NeuralPosterior . Parameters: Name Type Description Default method str Method to use. required Returns: Type Description NeuralPosterior NeuralPosterior for chainable calls. Source code in sbi/inference/posteriors/direct_posterior.py def set_mcmc_method ( self , method : str ) -> \"NeuralPosterior\" : \"\"\"Sets sampling method to for MCMC and returns `NeuralPosterior`. Args: method: Method to use. Returns: `NeuralPosterior` for chainable calls. \"\"\" self . _mcmc_method = method return self set_mcmc_parameters ( self , parameters ) inherited \u00b6 Sets parameters for MCMC and returns NeuralPosterior . Parameters: Name Type Description Default parameters Dict[str, Any] Dictionary overriding the default parameters for MCMC. The following parameters are supported: thin to set the thinning factor for the chain, warmup_steps to set the initial number of samples to discard, num_chains for the number of chains, init_strategy for the initialisation strategy for chains; prior will draw init locations from prior, whereas sir will use Sequential- Importance-Resampling using init_strategy_num_candidates to find init locations. required Returns: Type Description NeuralPosterior NeuralPosterior for chainable calls. Source code in sbi/inference/posteriors/direct_posterior.py def set_mcmc_parameters ( self , parameters : Dict [ str , Any ]) -> \"NeuralPosterior\" : \"\"\"Sets parameters for MCMC and returns `NeuralPosterior`. Args: parameters: Dictionary overriding the default parameters for MCMC. The following parameters are supported: `thin` to set the thinning factor for the chain, `warmup_steps` to set the initial number of samples to discard, `num_chains` for the number of chains, `init_strategy` for the initialisation strategy for chains; `prior` will draw init locations from prior, whereas `sir` will use Sequential- Importance-Resampling using `init_strategy_num_candidates` to find init locations. Returns: `NeuralPosterior` for chainable calls. \"\"\" self . _mcmc_parameters = parameters return self set_rejection_sampling_parameters ( self , parameters ) inherited \u00b6 Sets parameters for rejection sampling and returns NeuralPosterior . Parameters: Name Type Description Default parameters Dict[str, Any] Dictonary overriding the default parameters for rejection sampling. The following parameters are supported: proposal as the proposal distribtution. num_samples_to_find_max as the number of samples that are used to find the maximum of the potential_fn / proposal ratio. m as multiplier to that ratio. sampling_batch_size as the batchsize of samples being drawn from the proposal at every iteration. required Returns: Type Description NeuralPosterior `NeuralPosterior for chainable calls. Source code in sbi/inference/posteriors/direct_posterior.py def set_rejection_sampling_parameters ( self , parameters : Dict [ str , Any ] ) -> \"NeuralPosterior\" : \"\"\"Sets parameters for rejection sampling and returns `NeuralPosterior`. Args: parameters: Dictonary overriding the default parameters for rejection sampling. The following parameters are supported: `proposal` as the proposal distribtution. `num_samples_to_find_max` as the number of samples that are used to find the maximum of the `potential_fn / proposal` ratio. `m` as multiplier to that ratio. `sampling_batch_size` as the batchsize of samples being drawn from the proposal at every iteration. Returns: `NeuralPosterior for chainable calls. \"\"\" self . _rejection_sampling_parameters = parameters return self set_sample_with ( self , sample_with ) inherited \u00b6 Set the sampling method for the NeuralPosterior . Parameters: Name Type Description Default sample_with str The method to sample with. required Returns: Type Description NeuralPosterior NeuralPosterior for chainable calls. Exceptions: Type Description ValueError on attempt to turn off MCMC sampling for family of methods that do not support rejection sampling. Source code in sbi/inference/posteriors/direct_posterior.py def set_sample_with ( self , sample_with : str ) -> \"NeuralPosterior\" : \"\"\"Set the sampling method for the `NeuralPosterior`. Args: sample_with: The method to sample with. Returns: `NeuralPosterior` for chainable calls. Raises: ValueError: on attempt to turn off MCMC sampling for family of methods that do not support rejection sampling. \"\"\" if sample_with not in ( \"mcmc\" , \"rejection\" ): raise NameError ( \"The only implemented sampling methods are `mcmc` and `rejection`.\" ) self . _sample_with = sample_with return self sbi.inference.posteriors.likelihood_based_posterior.LikelihoodBasedPosterior ( NeuralPosterior ) \u00b6 Posterior \\(p(\\theta|x)\\) with log_prob() and sample() methods, obtained with SNLE. SNLE trains a neural network to approximate the likelihood \\(p(x|\\theta)\\) . The SNLE_Posterior class wraps the trained network such that one can directly evaluate the unnormalized posterior log probability \\(p(\\theta|x) \\propto p(x|\\theta) \\cdot p(\\theta)\\) and draw samples from the posterior with MCMC. The neural network itself can be accessed via the .net attribute. default_x : Optional [ torch . Tensor ] inherited property writable \u00b6 Return default x used by .sample(), .log_prob as conditioning context. mcmc_method : str inherited property writable \u00b6 Returns MCMC method. mcmc_parameters : dict inherited property writable \u00b6 Returns MCMC parameters. rejection_sampling_parameters : dict inherited property writable \u00b6 Returns rejection sampling parameters. sample_with : str inherited property writable \u00b6 Return True if NeuralPosterior instance should use MCMC in .sample() . __init__ ( self , method_family , neural_net , prior , x_shape , sample_with = 'mcmc' , mcmc_method = 'slice_np' , mcmc_parameters = None , rejection_sampling_parameters = None , device = 'cpu' ) special \u00b6 Parameters: Name Type Description Default method_family str One of snpe, snl, snre_a or snre_b. required neural_net Module A classifier for SNRE, a density estimator for SNPE and SNL. required prior Prior distribution with .log_prob() and .sample() . required x_shape Size Shape of the simulated data. It can differ from the observed data the posterior is conditioned on later in the batch dimension. If it differs, the additional entries are interpreted as independent and identically distributed data / trials. I.e., the data is assumed to be generated based on the same (unknown) model parameters or experimental condations. required sample_with str Method to use for sampling from the posterior. Must be one of [ mcmc | rejection ]. 'mcmc' mcmc_method str Method used for MCMC sampling, one of slice_np , slice , hmc , nuts . Currently defaults to slice_np for a custom numpy implementation of slice sampling; select hmc , nuts or slice for Pyro-based sampling. 'slice_np' mcmc_parameters Optional[Dict[str, Any]] Dictionary overriding the default parameters for MCMC. The following parameters are supported: thin to set the thinning factor for the chain, warmup_steps to set the initial number of samples to discard, num_chains for the number of chains, init_strategy for the initialisation strategy for chains; prior will draw init locations from prior, whereas sir will use Sequential- Importance-Resampling using init_strategy_num_candidates to find init locations. None rejection_sampling_parameters Optional[Dict[str, Any]] Dictionary overriding the default parameters for rejection sampling. The following parameters are supported: proposal as the proposal distribtution (default is the prior). max_sampling_batch_size as the batchsize of samples being drawn from the proposal at every iteration. num_samples_to_find_max as the number of samples that are used to find the maximum of the potential_fn / proposal ratio. num_iter_to_find_max as the number of gradient ascent iterations to find the maximum of that ratio. m as multiplier to that ratio. None device str Training device, e.g., \u201ccpu\u201d, \u201ccuda\u201d or \u201ccuda:{0, 1, \u2026}\u201d. 'cpu' Source code in sbi/inference/posteriors/likelihood_based_posterior.py def __init__ ( self , method_family : str , neural_net : nn . Module , prior , x_shape : torch . Size , sample_with : str = \"mcmc\" , mcmc_method : str = \"slice_np\" , mcmc_parameters : Optional [ Dict [ str , Any ]] = None , rejection_sampling_parameters : Optional [ Dict [ str , Any ]] = None , device : str = \"cpu\" , ): \"\"\" Args: method_family: One of snpe, snl, snre_a or snre_b. neural_net: A classifier for SNRE, a density estimator for SNPE and SNL. prior: Prior distribution with `.log_prob()` and `.sample()`. x_shape: Shape of the simulated data. It can differ from the observed data the posterior is conditioned on later in the batch dimension. If it differs, the additional entries are interpreted as independent and identically distributed data / trials. I.e., the data is assumed to be generated based on the same (unknown) model parameters or experimental condations. sample_with: Method to use for sampling from the posterior. Must be one of [`mcmc` | `rejection`]. mcmc_method: Method used for MCMC sampling, one of `slice_np`, `slice`, `hmc`, `nuts`. Currently defaults to `slice_np` for a custom numpy implementation of slice sampling; select `hmc`, `nuts` or `slice` for Pyro-based sampling. mcmc_parameters: Dictionary overriding the default parameters for MCMC. The following parameters are supported: `thin` to set the thinning factor for the chain, `warmup_steps` to set the initial number of samples to discard, `num_chains` for the number of chains, `init_strategy` for the initialisation strategy for chains; `prior` will draw init locations from prior, whereas `sir` will use Sequential- Importance-Resampling using `init_strategy_num_candidates` to find init locations. rejection_sampling_parameters: Dictionary overriding the default parameters for rejection sampling. The following parameters are supported: `proposal` as the proposal distribtution (default is the prior). `max_sampling_batch_size` as the batchsize of samples being drawn from the proposal at every iteration. `num_samples_to_find_max` as the number of samples that are used to find the maximum of the `potential_fn / proposal` ratio. `num_iter_to_find_max` as the number of gradient ascent iterations to find the maximum of that ratio. `m` as multiplier to that ratio. device: Training device, e.g., \"cpu\", \"cuda\" or \"cuda:{0, 1, ...}\". \"\"\" kwargs = del_entries ( locals (), entries = ( \"self\" , \"__class__\" )) super () . __init__ ( ** kwargs ) self . _purpose = ( \"It provides MCMC to .sample() from the posterior and \" \"can evaluate the _unnormalized_ posterior density with .log_prob().\" ) copy_hyperparameters_from ( self , posterior ) inherited \u00b6 Copies the hyperparameters from a given posterior to self . The hyperparameters that are copied are: Sampling parameters (MCMC for all methods, rejection sampling for SNPE). default_x at which to evaluate the posterior. Parameters: Name Type Description Default posterior NeuralPosterior Posterior that the hyperparameters are copied from. required Returns: Type Description Posterior object with the same hyperparameters as the passed posterior. This makes the call chainable posterior = infer.build_posterior().copy_hyperparameters_from(proposal) Source code in sbi/inference/posteriors/likelihood_based_posterior.py def copy_hyperparameters_from ( self , posterior : \"NeuralPosterior\" ) -> \"NeuralPosterior\" : \"\"\" Copies the hyperparameters from a given posterior to `self`. The hyperparameters that are copied are: - Sampling parameters (MCMC for all methods, rejection sampling for SNPE). - `default_x` at which to evaluate the posterior. Args: posterior: Posterior that the hyperparameters are copied from. Returns: Posterior object with the same hyperparameters as the passed posterior. This makes the call chainable: `posterior = infer.build_posterior().copy_hyperparameters_from(proposal)` \"\"\" assert isinstance ( posterior , NeuralPosterior ), \"`copy_state_from` must be a `NeuralPosterior`.\" self . set_mcmc_method ( posterior . _mcmc_method ) self . set_mcmc_parameters ( posterior . _mcmc_parameters ) self . set_default_x ( posterior . default_x ) self . _mcmc_init_params = posterior . _mcmc_init_params if hasattr ( self , \"_sample_with_mcmc\" ): self . set_sample_with_mcmc ( posterior . _sample_with_mcmc ) if hasattr ( self , \"_rejection_sampling_parameters\" ): self . set_rejection_sampling_parameters ( posterior . _rejection_sampling_parameters ) return self log_prob ( self , theta , x = None , track_gradients = False ) \u00b6 Returns the log-probability of \\(p(x|\\theta) \\cdot p(\\theta).\\) This corresponds to an unnormalized posterior log-probability. Parameters: Name Type Description Default theta Tensor Parameters \\(\\theta\\) . required x Optional[torch.Tensor] Conditioning context for posterior \\(p(\\theta|x)\\) . If not provided, fall back onto x passed to set_default_x() . None track_gradients bool Whether the returned tensor supports tracking gradients. This can be helpful for e.g. sensitivity analysis, but increases memory consumption. False Returns: Type Description Tensor (len(\u03b8),) -shaped log-probability \\(\\log(p(x|\\theta) \\cdot p(\\theta))\\) . Source code in sbi/inference/posteriors/likelihood_based_posterior.py def log_prob ( self , theta : Tensor , x : Optional [ Tensor ] = None , track_gradients : bool = False ) -> Tensor : r \"\"\" Returns the log-probability of $p(x|\\theta) \\cdot p(\\theta).$ This corresponds to an **unnormalized** posterior log-probability. Args: theta: Parameters $\\theta$. x: Conditioning context for posterior $p(\\theta|x)$. If not provided, fall back onto `x` passed to `set_default_x()`. track_gradients: Whether the returned tensor supports tracking gradients. This can be helpful for e.g. sensitivity analysis, but increases memory consumption. Returns: `(len(\u03b8),)`-shaped log-probability $\\log(p(x|\\theta) \\cdot p(\\theta))$. \"\"\" # TODO Train exited here, entered after sampling? self . net . eval () theta , x = self . _prepare_theta_and_x_for_log_prob_ ( theta , x ) # Calculate likelihood over trials and in one batch. warn ( \"The log probability from SNL is only correct up to a normalizing constant.\" ) log_likelihood_trial_sum = self . _log_likelihoods_over_trials ( x = x . to ( self . _device ), theta = theta . to ( self . _device ), net = self . net , track_gradients = track_gradients , ) # Move to cpu for comparison with prior. return log_likelihood_trial_sum + self . _prior . log_prob ( theta ) map ( self , x = None , num_iter = 1000 , learning_rate = 0.01 , init_method = 'posterior' , num_init_samples = 500 , num_to_optimize = 100 , save_best_every = 10 , show_progress_bars = True ) \u00b6 Returns the maximum-a-posteriori estimate (MAP). The method can be interrupted (Ctrl-C) when the user sees that the log-probability converges. The best estimate will be saved in self.map_ . The MAP is obtained by running gradient ascent from a given number of starting positions (samples from the posterior with the highest log-probability). After the optimization is done, we select the parameter set that has the highest log-probability after the optimization. Warning: The default values used by this function are not well-tested. They might require hand-tuning for the problem at hand. Parameters: Name Type Description Default x Optional[torch.Tensor] Conditioning context for posterior \\(p( heta|x)\\) . If not provided, fall back onto x passed to set_default_x() . None num_iter int Number of optimization steps that the algorithm takes to find the MAP. 1000 learning_rate float Learning rate of the optimizer. 0.01 init_method Union[str, torch.Tensor] How to select the starting parameters for the optimization. If it is a string, it can be either [ posterior , prior ], which samples the respective distribution num_init_samples times. If it is a, the tensor will be used as init locations. 'posterior' num_init_samples int Draw this number of samples from the posterior and evaluate the log-probability of all of them. 500 num_to_optimize int From the drawn num_init_samples , use the num_to_optimize with highest log-probability as the initial points for the optimization. 100 save_best_every int The best log-probability is computed, saved in the map -attribute, and printed every print_best_every -th iteration. Computing the best log-probability creates a significant overhead (thus, the default is 10 .) 10 show_progress_bars bool Whether or not to show a progressbar for sampling from the posterior. True Returns: Type Description Tensor The MAP estimate. Source code in sbi/inference/posteriors/likelihood_based_posterior.py def map ( self , x : Optional [ Tensor ] = None , num_iter : int = 1000 , learning_rate : float = 1e-2 , init_method : Union [ str , Tensor ] = \"posterior\" , num_init_samples : int = 500 , num_to_optimize : int = 100 , save_best_every : int = 10 , show_progress_bars : bool = True , ) -> Tensor : \"\"\" Returns the maximum-a-posteriori estimate (MAP). The method can be interrupted (Ctrl-C) when the user sees that the log-probability converges. The best estimate will be saved in `self.map_`. The MAP is obtained by running gradient ascent from a given number of starting positions (samples from the posterior with the highest log-probability). After the optimization is done, we select the parameter set that has the highest log-probability after the optimization. Warning: The default values used by this function are not well-tested. They might require hand-tuning for the problem at hand. Args: x: Conditioning context for posterior $p(\\theta|x)$. If not provided, fall back onto `x` passed to `set_default_x()`. num_iter: Number of optimization steps that the algorithm takes to find the MAP. learning_rate: Learning rate of the optimizer. init_method: How to select the starting parameters for the optimization. If it is a string, it can be either [`posterior`, `prior`], which samples the respective distribution `num_init_samples` times. If it is a, the tensor will be used as init locations. num_init_samples: Draw this number of samples from the posterior and evaluate the log-probability of all of them. num_to_optimize: From the drawn `num_init_samples`, use the `num_to_optimize` with highest log-probability as the initial points for the optimization. save_best_every: The best log-probability is computed, saved in the `map`-attribute, and printed every `print_best_every`-th iteration. Computing the best log-probability creates a significant overhead (thus, the default is `10`.) show_progress_bars: Whether or not to show a progressbar for sampling from the posterior. Returns: The MAP estimate. \"\"\" return super () . map ( x = x , num_iter = num_iter , learning_rate = learning_rate , init_method = init_method , num_init_samples = num_init_samples , num_to_optimize = num_to_optimize , save_best_every = save_best_every , show_progress_bars = show_progress_bars , ) sample ( self , sample_shape = torch . Size ([]), x = None , show_progress_bars = True , sample_with = None , mcmc_method = None , mcmc_parameters = None , rejection_sampling_parameters = None ) \u00b6 Return samples from posterior distribution \\(p(\\theta|x)\\) with MCMC. Parameters: Name Type Description Default sample_shape Union[torch.Size, Tuple[int, ...]] Desired shape of samples that are drawn from posterior. If sample_shape is multidimensional we simply draw sample_shape.numel() samples and then reshape into the desired shape. torch.Size([]) x Optional[torch.Tensor] Conditioning context for posterior \\(p(\\theta|x)\\) . If not provided, fall back onto x passed to set_default_x() . None show_progress_bars bool Whether to show sampling progress monitor. True sample_with Optional[str] Method to use for sampling from the posterior. Must be one of [ mcmc | rejection ]. None mcmc_method Optional[str] Optional parameter to override self.mcmc_method . None mcmc_parameters Optional[Dict[str, Any]] Dictionary overriding the default parameters for MCMC. The following parameters are supported: thin to set the thinning factor for the chain. warmup_steps to set the initial number of samples to discard. num_chains for the number of chains. init_strategy for the initialisation strategy for chains; prior will draw init locations from prior, whereas sir will use Sequential- Importance-Resampling using init_strategy_num_candidates to find init locations. enable_transform a bool indicating whether MCMC is performed in z-scored (and unconstrained) space. None rejection_sampling_parameters Optional[Dict[str, Any]] Dictionary overriding the default parameters for rejection sampling. The following parameters are supported: proposal as the proposal distribtution (default is the prior). max_sampling_batch_size as the batchsize of samples being drawn from the proposal at every iteration. num_samples_to_find_max as the number of samples that are used to find the maximum of the potential_fn / proposal ratio. num_iter_to_find_max as the number of gradient ascent iterations to find the maximum of that ratio. m as multiplier to that ratio. None Returns: Type Description Tensor Samples from posterior. Source code in sbi/inference/posteriors/likelihood_based_posterior.py def sample ( self , sample_shape : Shape = torch . Size (), x : Optional [ Tensor ] = None , show_progress_bars : bool = True , sample_with : Optional [ str ] = None , mcmc_method : Optional [ str ] = None , mcmc_parameters : Optional [ Dict [ str , Any ]] = None , rejection_sampling_parameters : Optional [ Dict [ str , Any ]] = None , ) -> Tensor : r \"\"\" Return samples from posterior distribution $p(\\theta|x)$ with MCMC. Args: sample_shape: Desired shape of samples that are drawn from posterior. If sample_shape is multidimensional we simply draw `sample_shape.numel()` samples and then reshape into the desired shape. x: Conditioning context for posterior $p(\\theta|x)$. If not provided, fall back onto `x` passed to `set_default_x()`. show_progress_bars: Whether to show sampling progress monitor. sample_with: Method to use for sampling from the posterior. Must be one of [`mcmc` | `rejection`]. mcmc_method: Optional parameter to override `self.mcmc_method`. mcmc_parameters: Dictionary overriding the default parameters for MCMC. The following parameters are supported: `thin` to set the thinning factor for the chain. `warmup_steps` to set the initial number of samples to discard. `num_chains` for the number of chains. `init_strategy` for the initialisation strategy for chains; `prior` will draw init locations from prior, whereas `sir` will use Sequential- Importance-Resampling using `init_strategy_num_candidates` to find init locations. `enable_transform` a bool indicating whether MCMC is performed in z-scored (and unconstrained) space. rejection_sampling_parameters: Dictionary overriding the default parameters for rejection sampling. The following parameters are supported: `proposal` as the proposal distribtution (default is the prior). `max_sampling_batch_size` as the batchsize of samples being drawn from the proposal at every iteration. `num_samples_to_find_max` as the number of samples that are used to find the maximum of the `potential_fn / proposal` ratio. `num_iter_to_find_max` as the number of gradient ascent iterations to find the maximum of that ratio. `m` as multiplier to that ratio. Returns: Samples from posterior. \"\"\" self . net . eval () sample_with = sample_with if sample_with is not None else self . _sample_with x , num_samples = self . _prepare_for_sample ( x , sample_shape ) potential_fn_provider = PotentialFunctionProvider () if sample_with == \"mcmc\" : mcmc_method , mcmc_parameters = self . _potentially_replace_mcmc_parameters ( mcmc_method , mcmc_parameters ) transform = mcmc_transform ( self . _prior , device = self . _device , ** mcmc_parameters ) transformed_samples = self . _sample_posterior_mcmc ( num_samples = num_samples , potential_fn = potential_fn_provider ( self . _prior , self . net , x , mcmc_method , transform ), init_fn = self . _build_mcmc_init_fn ( self . _prior , potential_fn_provider ( self . _prior , self . net , x , \"slice_np\" , transform ), transform = transform , ** mcmc_parameters , ), mcmc_method = mcmc_method , show_progress_bars = show_progress_bars , ** mcmc_parameters , ) samples = transform . inv ( transformed_samples ) elif sample_with == \"rejection\" : rejection_sampling_parameters = ( self . _potentially_replace_rejection_parameters ( rejection_sampling_parameters ) ) if \"proposal\" not in rejection_sampling_parameters : rejection_sampling_parameters [ \"proposal\" ] = self . _prior samples , _ = rejection_sample ( potential_fn = potential_fn_provider ( self . _prior , self . net , x , \"rejection\" , ), num_samples = num_samples , ** rejection_sampling_parameters , ) else : raise NameError ( \"The only implemented sampling methods are `mcmc` and `rejection`.\" ) self . net . train ( True ) return samples . reshape (( * sample_shape , - 1 )) sample_conditional ( self , sample_shape , condition , dims_to_sample , x = None , sample_with = 'mcmc' , show_progress_bars = True , mcmc_method = None , mcmc_parameters = None , rejection_sampling_parameters = None ) \u00b6 Return samples from conditional posterior \\(p(\\theta_i|\\theta_j, x)\\) . In this function, we do not sample from the full posterior, but instead only from a few parameter dimensions while the other parameter dimensions are kept fixed at values specified in condition . Samples are obtained with MCMC. Parameters: Name Type Description Default sample_shape Union[torch.Size, Tuple[int, ...]] Desired shape of samples that are drawn from posterior. If sample_shape is multidimensional we simply draw sample_shape.numel() samples and then reshape into the desired shape. required condition Tensor Parameter set that all dimensions not specified in dims_to_sample will be fixed to. Should contain dim_theta elements, i.e. it could e.g. be a sample from the posterior distribution. The entries at all dims_to_sample will be ignored. required dims_to_sample List[int] Which dimensions to sample from. The dimensions not specified in dims_to_sample will be fixed to values given in condition . required x Optional[torch.Tensor] Conditioning context for posterior \\(p(\\theta|x)\\) . If not provided, fall back onto x passed to set_default_x() . None sample_with str Method to use for sampling from the posterior. Must be one of [ mcmc | rejection ]. In this method, the value of self._sample_with will be ignored. 'mcmc' show_progress_bars bool Whether to show sampling progress monitor. True mcmc_method Optional[str] Optional parameter to override self.mcmc_method . None mcmc_parameters Optional[Dict[str, Any]] Dictionary overriding the default parameters for MCMC. The following parameters are supported: thin to set the thinning factor for the chain. warmup_steps to set the initial number of samples to discard. num_chains for the number of chains. init_strategy for the initialisation strategy for chains; prior will draw init locations from prior, whereas sir will use Sequential- Importance-Resampling using init_strategy_num_candidates to find init locations. enable_transform a bool indicating whether MCMC is performed in z-scored (and unconstrained) space. None rejection_sampling_parameters Optional[Dict[str, Any]] Dictionary overriding the default parameters for rejection sampling. The following parameters are supported: proposal as the proposal distribtution (default is the prior). max_sampling_batch_size as the batchsize of samples being drawn from the proposal at every iteration. num_samples_to_find_max as the number of samples that are used to find the maximum of the potential_fn / proposal ratio. num_iter_to_find_max as the number of gradient ascent iterations to find the maximum of that ratio. m as multiplier to that ratio. None Returns: Type Description Tensor Samples from conditional posterior. Source code in sbi/inference/posteriors/likelihood_based_posterior.py def sample_conditional ( self , sample_shape : Shape , condition : Tensor , dims_to_sample : List [ int ], x : Optional [ Tensor ] = None , sample_with : str = \"mcmc\" , show_progress_bars : bool = True , mcmc_method : Optional [ str ] = None , mcmc_parameters : Optional [ Dict [ str , Any ]] = None , rejection_sampling_parameters : Optional [ Dict [ str , Any ]] = None , ) -> Tensor : r \"\"\" Return samples from conditional posterior $p(\\theta_i|\\theta_j, x)$. In this function, we do not sample from the full posterior, but instead only from a few parameter dimensions while the other parameter dimensions are kept fixed at values specified in `condition`. Samples are obtained with MCMC. Args: sample_shape: Desired shape of samples that are drawn from posterior. If sample_shape is multidimensional we simply draw `sample_shape.numel()` samples and then reshape into the desired shape. condition: Parameter set that all dimensions not specified in `dims_to_sample` will be fixed to. Should contain dim_theta elements, i.e. it could e.g. be a sample from the posterior distribution. The entries at all `dims_to_sample` will be ignored. dims_to_sample: Which dimensions to sample from. The dimensions not specified in `dims_to_sample` will be fixed to values given in `condition`. x: Conditioning context for posterior $p(\\theta|x)$. If not provided, fall back onto `x` passed to `set_default_x()`. sample_with: Method to use for sampling from the posterior. Must be one of [`mcmc` | `rejection`]. In this method, the value of `self._sample_with` will be ignored. show_progress_bars: Whether to show sampling progress monitor. mcmc_method: Optional parameter to override `self.mcmc_method`. mcmc_parameters: Dictionary overriding the default parameters for MCMC. The following parameters are supported: `thin` to set the thinning factor for the chain. `warmup_steps` to set the initial number of samples to discard. `num_chains` for the number of chains. `init_strategy` for the initialisation strategy for chains; `prior` will draw init locations from prior, whereas `sir` will use Sequential- Importance-Resampling using `init_strategy_num_candidates` to find init locations. `enable_transform` a bool indicating whether MCMC is performed in z-scored (and unconstrained) space. rejection_sampling_parameters: Dictionary overriding the default parameters for rejection sampling. The following parameters are supported: `proposal` as the proposal distribtution (default is the prior). `max_sampling_batch_size` as the batchsize of samples being drawn from the proposal at every iteration. `num_samples_to_find_max` as the number of samples that are used to find the maximum of the `potential_fn / proposal` ratio. `num_iter_to_find_max` as the number of gradient ascent iterations to find the maximum of that ratio. `m` as multiplier to that ratio. Returns: Samples from conditional posterior. \"\"\" return super () . sample_conditional ( PotentialFunctionProvider (), sample_shape , condition , dims_to_sample , x , sample_with , show_progress_bars , mcmc_method , mcmc_parameters , rejection_sampling_parameters , ) set_default_x ( self , x ) inherited \u00b6 Set new default x for .sample(), .log_prob to use as conditioning context. This is a pure convenience to avoid having to repeatedly specify x in calls to .sample() and .log_prob() - only \u03b8 needs to be passed. This convenience is particularly useful when the posterior is focused, i.e. has been trained over multiple rounds to be accurate in the vicinity of a particular x=x_o (you can check if your posterior object is focused by printing it). NOTE: this method is chainable, i.e. will return the NeuralPosterior object so that calls like posterior.set_default_x(my_x).sample(mytheta) are possible. Parameters: Name Type Description Default x Tensor The default observation to set for the posterior \\(p(theta|x)\\) . required Returns: Type Description NeuralPosterior NeuralPosterior that will use a default x when not explicitly passed. Source code in sbi/inference/posteriors/likelihood_based_posterior.py def set_default_x ( self , x : Tensor ) -> \"NeuralPosterior\" : \"\"\"Set new default x for `.sample(), .log_prob` to use as conditioning context. This is a pure convenience to avoid having to repeatedly specify `x` in calls to `.sample()` and `.log_prob()` - only \u03b8 needs to be passed. This convenience is particularly useful when the posterior is focused, i.e. has been trained over multiple rounds to be accurate in the vicinity of a particular `x=x_o` (you can check if your posterior object is focused by printing it). NOTE: this method is chainable, i.e. will return the NeuralPosterior object so that calls like `posterior.set_default_x(my_x).sample(mytheta)` are possible. Args: x: The default observation to set for the posterior $p(theta|x)$. Returns: `NeuralPosterior` that will use a default `x` when not explicitly passed. \"\"\" self . _x = process_x ( x , self . _x_shape , allow_iid_x = self . _allow_iid_x ) . to ( self . _device ) self . _num_iid_trials = self . _x . shape [ 0 ] return self set_mcmc_method ( self , method ) inherited \u00b6 Sets sampling method to for MCMC and returns NeuralPosterior . Parameters: Name Type Description Default method str Method to use. required Returns: Type Description NeuralPosterior NeuralPosterior for chainable calls. Source code in sbi/inference/posteriors/likelihood_based_posterior.py def set_mcmc_method ( self , method : str ) -> \"NeuralPosterior\" : \"\"\"Sets sampling method to for MCMC and returns `NeuralPosterior`. Args: method: Method to use. Returns: `NeuralPosterior` for chainable calls. \"\"\" self . _mcmc_method = method return self set_mcmc_parameters ( self , parameters ) inherited \u00b6 Sets parameters for MCMC and returns NeuralPosterior . Parameters: Name Type Description Default parameters Dict[str, Any] Dictionary overriding the default parameters for MCMC. The following parameters are supported: thin to set the thinning factor for the chain, warmup_steps to set the initial number of samples to discard, num_chains for the number of chains, init_strategy for the initialisation strategy for chains; prior will draw init locations from prior, whereas sir will use Sequential- Importance-Resampling using init_strategy_num_candidates to find init locations. required Returns: Type Description NeuralPosterior NeuralPosterior for chainable calls. Source code in sbi/inference/posteriors/likelihood_based_posterior.py def set_mcmc_parameters ( self , parameters : Dict [ str , Any ]) -> \"NeuralPosterior\" : \"\"\"Sets parameters for MCMC and returns `NeuralPosterior`. Args: parameters: Dictionary overriding the default parameters for MCMC. The following parameters are supported: `thin` to set the thinning factor for the chain, `warmup_steps` to set the initial number of samples to discard, `num_chains` for the number of chains, `init_strategy` for the initialisation strategy for chains; `prior` will draw init locations from prior, whereas `sir` will use Sequential- Importance-Resampling using `init_strategy_num_candidates` to find init locations. Returns: `NeuralPosterior` for chainable calls. \"\"\" self . _mcmc_parameters = parameters return self set_rejection_sampling_parameters ( self , parameters ) inherited \u00b6 Sets parameters for rejection sampling and returns NeuralPosterior . Parameters: Name Type Description Default parameters Dict[str, Any] Dictonary overriding the default parameters for rejection sampling. The following parameters are supported: proposal as the proposal distribtution. num_samples_to_find_max as the number of samples that are used to find the maximum of the potential_fn / proposal ratio. m as multiplier to that ratio. sampling_batch_size as the batchsize of samples being drawn from the proposal at every iteration. required Returns: Type Description NeuralPosterior `NeuralPosterior for chainable calls. Source code in sbi/inference/posteriors/likelihood_based_posterior.py def set_rejection_sampling_parameters ( self , parameters : Dict [ str , Any ] ) -> \"NeuralPosterior\" : \"\"\"Sets parameters for rejection sampling and returns `NeuralPosterior`. Args: parameters: Dictonary overriding the default parameters for rejection sampling. The following parameters are supported: `proposal` as the proposal distribtution. `num_samples_to_find_max` as the number of samples that are used to find the maximum of the `potential_fn / proposal` ratio. `m` as multiplier to that ratio. `sampling_batch_size` as the batchsize of samples being drawn from the proposal at every iteration. Returns: `NeuralPosterior for chainable calls. \"\"\" self . _rejection_sampling_parameters = parameters return self set_sample_with ( self , sample_with ) inherited \u00b6 Set the sampling method for the NeuralPosterior . Parameters: Name Type Description Default sample_with str The method to sample with. required Returns: Type Description NeuralPosterior NeuralPosterior for chainable calls. Exceptions: Type Description ValueError on attempt to turn off MCMC sampling for family of methods that do not support rejection sampling. Source code in sbi/inference/posteriors/likelihood_based_posterior.py def set_sample_with ( self , sample_with : str ) -> \"NeuralPosterior\" : \"\"\"Set the sampling method for the `NeuralPosterior`. Args: sample_with: The method to sample with. Returns: `NeuralPosterior` for chainable calls. Raises: ValueError: on attempt to turn off MCMC sampling for family of methods that do not support rejection sampling. \"\"\" if sample_with not in ( \"mcmc\" , \"rejection\" ): raise NameError ( \"The only implemented sampling methods are `mcmc` and `rejection`.\" ) self . _sample_with = sample_with return self sbi.inference.posteriors.ratio_based_posterior.RatioBasedPosterior ( NeuralPosterior ) \u00b6 Posterior \\(p(\\theta|x)\\) with log_prob() and sample() methods, obtained with SNRE. SNRE trains a neural network to approximate likelihood ratios, which in turn can be used obtain an unnormalized posterior \\(p(\\theta|x) \\propto p(x|\\theta) \\cdot p(\\theta)\\) . The SNRE_Posterior class wraps the trained network such that one can directly evaluate the unnormalized posterior log-probability \\(p(\\theta|x) \\propto p(x|\\theta) \\cdot p(\\theta)\\) and draw samples from the posterior with MCMC. Note that, in the case of single-round SNRE_A / AALR, it is possible to evaluate the log-probability of the normalized posterior, but sampling still requires MCMC. The neural network itself can be accessed via the .net attribute. default_x : Optional [ torch . Tensor ] inherited property writable \u00b6 Return default x used by .sample(), .log_prob as conditioning context. mcmc_method : str inherited property writable \u00b6 Returns MCMC method. mcmc_parameters : dict inherited property writable \u00b6 Returns MCMC parameters. rejection_sampling_parameters : dict inherited property writable \u00b6 Returns rejection sampling parameters. sample_with : str inherited property writable \u00b6 Return True if NeuralPosterior instance should use MCMC in .sample() . __init__ ( self , method_family , neural_net , prior , x_shape , sample_with = 'mcmc' , mcmc_method = 'slice_np' , mcmc_parameters = None , rejection_sampling_parameters = None , device = 'cpu' ) special \u00b6 Parameters: Name Type Description Default method_family str One of snpe, snl, snre_a or snre_b. required neural_net Module A classifier for SNRE, a density estimator for SNPE and SNL. required prior Prior distribution with .log_prob() and .sample() . required x_shape Size Shape of the simulated data. It can differ from the observed data the posterior is conditioned on later in the batch dimension. If it differs, the additional entries are interpreted as independent and identically distributed data / trials. I.e., the data is assumed to be generated based on the same (unknown) model parameters or experimental condations. required sample_with str Method to use for sampling from the posterior. Must be one of [ mcmc | rejection ]. 'mcmc' mcmc_method str Method used for MCMC sampling, one of slice_np , slice , hmc , nuts . Currently defaults to slice_np for a custom numpy implementation of slice sampling; select hmc , nuts or slice for Pyro-based sampling. 'slice_np' mcmc_parameters Optional[Dict[str, Any]] Dictionary overriding the default parameters for MCMC. The following parameters are supported: thin to set the thinning factor for the chain, warmup_steps to set the initial number of samples to discard, num_chains for the number of chains, init_strategy for the initialisation strategy for chains; prior will draw init locations from prior, whereas sir will use Sequential- Importance-Resampling using init_strategy_num_candidates to find init locations. None rejection_sampling_parameters Optional[Dict[str, Any]] Dictionary overriding the default parameters for rejection sampling. The following parameters are supported: proposal as the proposal distribtution (default is the prior). max_sampling_batch_size as the batchsize of samples being drawn from the proposal at every iteration. num_samples_to_find_max as the number of samples that are used to find the maximum of the potential_fn / proposal ratio. num_iter_to_find_max as the number of gradient ascent iterations to find the maximum of that ratio. m as multiplier to that ratio. None device str Training device, e.g., \u201ccpu\u201d, \u201ccuda\u201d or \u201ccuda:{0, 1, \u2026}\u201d. 'cpu' Source code in sbi/inference/posteriors/ratio_based_posterior.py def __init__ ( self , method_family : str , neural_net : nn . Module , prior , x_shape : torch . Size , sample_with : str = \"mcmc\" , mcmc_method : str = \"slice_np\" , mcmc_parameters : Optional [ Dict [ str , Any ]] = None , rejection_sampling_parameters : Optional [ Dict [ str , Any ]] = None , device : str = \"cpu\" , ): \"\"\" Args: method_family: One of snpe, snl, snre_a or snre_b. neural_net: A classifier for SNRE, a density estimator for SNPE and SNL. prior: Prior distribution with `.log_prob()` and `.sample()`. x_shape: Shape of the simulated data. It can differ from the observed data the posterior is conditioned on later in the batch dimension. If it differs, the additional entries are interpreted as independent and identically distributed data / trials. I.e., the data is assumed to be generated based on the same (unknown) model parameters or experimental condations. sample_with: Method to use for sampling from the posterior. Must be one of [`mcmc` | `rejection`]. mcmc_method: Method used for MCMC sampling, one of `slice_np`, `slice`, `hmc`, `nuts`. Currently defaults to `slice_np` for a custom numpy implementation of slice sampling; select `hmc`, `nuts` or `slice` for Pyro-based sampling. mcmc_parameters: Dictionary overriding the default parameters for MCMC. The following parameters are supported: `thin` to set the thinning factor for the chain, `warmup_steps` to set the initial number of samples to discard, `num_chains` for the number of chains, `init_strategy` for the initialisation strategy for chains; `prior` will draw init locations from prior, whereas `sir` will use Sequential- Importance-Resampling using `init_strategy_num_candidates` to find init locations. rejection_sampling_parameters: Dictionary overriding the default parameters for rejection sampling. The following parameters are supported: `proposal` as the proposal distribtution (default is the prior). `max_sampling_batch_size` as the batchsize of samples being drawn from the proposal at every iteration. `num_samples_to_find_max` as the number of samples that are used to find the maximum of the `potential_fn / proposal` ratio. `num_iter_to_find_max` as the number of gradient ascent iterations to find the maximum of that ratio. `m` as multiplier to that ratio. device: Training device, e.g., \"cpu\", \"cuda\" or \"cuda:{0, 1, ...}\". \"\"\" kwargs = del_entries ( locals (), entries = ( \"self\" , \"__class__\" )) super () . __init__ ( ** kwargs ) copy_hyperparameters_from ( self , posterior ) inherited \u00b6 Copies the hyperparameters from a given posterior to self . The hyperparameters that are copied are: Sampling parameters (MCMC for all methods, rejection sampling for SNPE). default_x at which to evaluate the posterior. Parameters: Name Type Description Default posterior NeuralPosterior Posterior that the hyperparameters are copied from. required Returns: Type Description Posterior object with the same hyperparameters as the passed posterior. This makes the call chainable posterior = infer.build_posterior().copy_hyperparameters_from(proposal) Source code in sbi/inference/posteriors/ratio_based_posterior.py def copy_hyperparameters_from ( self , posterior : \"NeuralPosterior\" ) -> \"NeuralPosterior\" : \"\"\" Copies the hyperparameters from a given posterior to `self`. The hyperparameters that are copied are: - Sampling parameters (MCMC for all methods, rejection sampling for SNPE). - `default_x` at which to evaluate the posterior. Args: posterior: Posterior that the hyperparameters are copied from. Returns: Posterior object with the same hyperparameters as the passed posterior. This makes the call chainable: `posterior = infer.build_posterior().copy_hyperparameters_from(proposal)` \"\"\" assert isinstance ( posterior , NeuralPosterior ), \"`copy_state_from` must be a `NeuralPosterior`.\" self . set_mcmc_method ( posterior . _mcmc_method ) self . set_mcmc_parameters ( posterior . _mcmc_parameters ) self . set_default_x ( posterior . default_x ) self . _mcmc_init_params = posterior . _mcmc_init_params if hasattr ( self , \"_sample_with_mcmc\" ): self . set_sample_with_mcmc ( posterior . _sample_with_mcmc ) if hasattr ( self , \"_rejection_sampling_parameters\" ): self . set_rejection_sampling_parameters ( posterior . _rejection_sampling_parameters ) return self log_prob ( self , theta , x = None , track_gradients = False ) \u00b6 Returns the log-probability of \\(p(x|\\theta) \\cdot p(\\theta).\\) This corresponds to an unnormalized posterior log-probability. Only for single-round SNRE_A / AALR, the returned log-probability will correspond to the normalized log-probability. Parameters: Name Type Description Default theta Tensor Parameters \\(\\theta\\) . required x Optional[torch.Tensor] Conditioning context for posterior \\(p(\\theta|x)\\) . If not provided, fall back onto x passed to set_default_x() . None track_gradients bool Whether the returned tensor supports tracking gradients. This can be helpful for e.g. sensitivity analysis, but increases memory consumption. False Returns: Type Description Tensor (len(\u03b8),) -shaped log-probability \\(\\log(p(x|\\theta) \\cdot p(\\theta))\\) . Source code in sbi/inference/posteriors/ratio_based_posterior.py def log_prob ( self , theta : Tensor , x : Optional [ Tensor ] = None , track_gradients : bool = False ) -> Tensor : r \"\"\"Returns the log-probability of $p(x|\\theta) \\cdot p(\\theta).$ This corresponds to an **unnormalized** posterior log-probability. Only for single-round SNRE_A / AALR, the returned log-probability will correspond to the **normalized** log-probability. Args: theta: Parameters $\\theta$. x: Conditioning context for posterior $p(\\theta|x)$. If not provided, fall back onto `x` passed to `set_default_x()`. track_gradients: Whether the returned tensor supports tracking gradients. This can be helpful for e.g. sensitivity analysis, but increases memory consumption. Returns: `(len(\u03b8),)`-shaped log-probability $\\log(p(x|\\theta) \\cdot p(\\theta))$. \"\"\" # TODO Train exited here, entered after sampling? self . net . eval () theta , x = self . _prepare_theta_and_x_for_log_prob_ ( theta , x ) self . _warn_log_prob_snre () # Sum log ratios over x batch of iid trials. log_ratio = self . _log_ratios_over_trials ( x . to ( self . _device ), theta . to ( self . _device ), self . net , track_gradients = track_gradients , ) return log_ratio + self . _prior . log_prob ( theta ) map ( self , x = None , num_iter = 1000 , learning_rate = 0.01 , init_method = 'posterior' , num_init_samples = 500 , num_to_optimize = 100 , save_best_every = 10 , show_progress_bars = True ) \u00b6 Returns the maximum-a-posteriori estimate (MAP). The method can be interrupted (Ctrl-C) when the user sees that the log-probability converges. The best estimate will be saved in self.map_ . The MAP is obtained by running gradient ascent from a given number of starting positions (samples from the posterior with the highest log-probability). After the optimization is done, we select the parameter set that has the highest log-probability after the optimization. Warning: The default values used by this function are not well-tested. They might require hand-tuning for the problem at hand. Parameters: Name Type Description Default x Optional[torch.Tensor] Conditioning context for posterior \\(p( heta|x)\\) . If not provided, fall back onto x passed to set_default_x() . None num_iter int Maximum Number of optimization steps that the algorithm takes to find the MAP. 1000 learning_rate float Learning rate of the optimizer. 0.01 init_method Union[str, torch.Tensor] How to select the starting parameters for the optimization. If it is a string, it can be either [ posterior , prior ], which samples the respective distribution num_init_samples times. If it is a, the tensor will be used as init locations. 'posterior' num_init_samples int Draw this number of samples from the posterior and evaluate the log-probability of all of them. 500 num_to_optimize int From the drawn num_init_samples , use the num_to_optimize with highest log-probability as the initial points for the optimization. 100 save_best_every int The best log-probability is computed, saved in the map -attribute, and printed every print_best_every -th iteration. Computing the best log-probability creates a significant overhead (thus, the default is 10 .) 10 show_progress_bars bool Whether or not to show a progressbar for sampling from the posterior. True Returns: Type Description Tensor The MAP estimate. Source code in sbi/inference/posteriors/ratio_based_posterior.py def map ( self , x : Optional [ Tensor ] = None , num_iter : int = 1000 , learning_rate : float = 1e-2 , init_method : Union [ str , Tensor ] = \"posterior\" , num_init_samples : int = 500 , num_to_optimize : int = 100 , save_best_every : int = 10 , show_progress_bars : bool = True , ) -> Tensor : \"\"\" Returns the maximum-a-posteriori estimate (MAP). The method can be interrupted (Ctrl-C) when the user sees that the log-probability converges. The best estimate will be saved in `self.map_`. The MAP is obtained by running gradient ascent from a given number of starting positions (samples from the posterior with the highest log-probability). After the optimization is done, we select the parameter set that has the highest log-probability after the optimization. Warning: The default values used by this function are not well-tested. They might require hand-tuning for the problem at hand. Args: x: Conditioning context for posterior $p(\\theta|x)$. If not provided, fall back onto `x` passed to `set_default_x()`. num_iter: Maximum Number of optimization steps that the algorithm takes to find the MAP. learning_rate: Learning rate of the optimizer. init_method: How to select the starting parameters for the optimization. If it is a string, it can be either [`posterior`, `prior`], which samples the respective distribution `num_init_samples` times. If it is a, the tensor will be used as init locations. num_init_samples: Draw this number of samples from the posterior and evaluate the log-probability of all of them. num_to_optimize: From the drawn `num_init_samples`, use the `num_to_optimize` with highest log-probability as the initial points for the optimization. save_best_every: The best log-probability is computed, saved in the `map`-attribute, and printed every `print_best_every`-th iteration. Computing the best log-probability creates a significant overhead (thus, the default is `10`.) show_progress_bars: Whether or not to show a progressbar for sampling from the posterior. Returns: The MAP estimate. \"\"\" return super () . map ( x = x , num_iter = num_iter , learning_rate = learning_rate , init_method = init_method , num_init_samples = num_init_samples , num_to_optimize = num_to_optimize , save_best_every = save_best_every , show_progress_bars = show_progress_bars , ) sample ( self , sample_shape = torch . Size ([]), x = None , show_progress_bars = True , sample_with = None , mcmc_method = None , mcmc_parameters = None , rejection_sampling_parameters = None ) \u00b6 Return samples from posterior distribution \\(p(\\theta|x)\\) with MCMC. Parameters: Name Type Description Default sample_shape Union[torch.Size, Tuple[int, ...]] Desired shape of samples that are drawn from posterior. If sample_shape is multidimensional we simply draw sample_shape.numel() samples and then reshape into the desired shape. torch.Size([]) x Optional[torch.Tensor] Conditioning context for posterior \\(p(\\theta|x)\\) . If not provided, fall back onto x passed to set_default_x() . None show_progress_bars bool Whether to show sampling progress monitor. True sample_with Optional[str] Method to use for sampling from the posterior. Must be one of [ mcmc | rejection ]. None mcmc_method Optional[str] Optional parameter to override self.mcmc_method . None mcmc_parameters Optional[Dict[str, Any]] Dictionary overriding the default parameters for MCMC. The following parameters are supported: thin to set the thinning factor for the chain. warmup_steps to set the initial number of samples to discard. num_chains for the number of chains. init_strategy for the initialisation strategy for chains; prior will draw init locations from prior, whereas sir will use Sequential- Importance-Resampling using init_strategy_num_candidates to find init locations. enable_transform a bool indicating whether MCMC is performed in z-scored (and unconstrained) space. None rejection_sampling_parameters Optional[Dict[str, Any]] Dictionary overriding the default parameters for rejection sampling. The following parameters are supported: proposal as the proposal distribtution (default is the prior). max_sampling_batch_size as the batchsize of samples being drawn from the proposal at every iteration. num_samples_to_find_max as the number of samples that are used to find the maximum of the potential_fn / proposal ratio. num_iter_to_find_max as the number of gradient ascent iterations to find the maximum of that ratio. m as multiplier to that ratio. None Returns: Type Description Tensor Samples from posterior. Source code in sbi/inference/posteriors/ratio_based_posterior.py def sample ( self , sample_shape : Shape = torch . Size (), x : Optional [ Tensor ] = None , show_progress_bars : bool = True , sample_with : Optional [ str ] = None , mcmc_method : Optional [ str ] = None , mcmc_parameters : Optional [ Dict [ str , Any ]] = None , rejection_sampling_parameters : Optional [ Dict [ str , Any ]] = None , ) -> Tensor : r \"\"\" Return samples from posterior distribution $p(\\theta|x)$ with MCMC. Args: sample_shape: Desired shape of samples that are drawn from posterior. If sample_shape is multidimensional we simply draw `sample_shape.numel()` samples and then reshape into the desired shape. x: Conditioning context for posterior $p(\\theta|x)$. If not provided, fall back onto `x` passed to `set_default_x()`. show_progress_bars: Whether to show sampling progress monitor. sample_with: Method to use for sampling from the posterior. Must be one of [`mcmc` | `rejection`]. mcmc_method: Optional parameter to override `self.mcmc_method`. mcmc_parameters: Dictionary overriding the default parameters for MCMC. The following parameters are supported: `thin` to set the thinning factor for the chain. `warmup_steps` to set the initial number of samples to discard. `num_chains` for the number of chains. `init_strategy` for the initialisation strategy for chains; `prior` will draw init locations from prior, whereas `sir` will use Sequential- Importance-Resampling using `init_strategy_num_candidates` to find init locations. `enable_transform` a bool indicating whether MCMC is performed in z-scored (and unconstrained) space. rejection_sampling_parameters: Dictionary overriding the default parameters for rejection sampling. The following parameters are supported: `proposal` as the proposal distribtution (default is the prior). `max_sampling_batch_size` as the batchsize of samples being drawn from the proposal at every iteration. `num_samples_to_find_max` as the number of samples that are used to find the maximum of the `potential_fn / proposal` ratio. `num_iter_to_find_max` as the number of gradient ascent iterations to find the maximum of that ratio. `m` as multiplier to that ratio. Returns: Samples from posterior. \"\"\" self . net . eval () sample_with = sample_with if sample_with is not None else self . _sample_with x , num_samples = self . _prepare_for_sample ( x , sample_shape ) potential_fn_provider = PotentialFunctionProvider () if sample_with == \"mcmc\" : mcmc_method , mcmc_parameters = self . _potentially_replace_mcmc_parameters ( mcmc_method , mcmc_parameters ) transform = mcmc_transform ( self . _prior , device = self . _device , ** mcmc_parameters ) transformed_samples = self . _sample_posterior_mcmc ( num_samples = num_samples , potential_fn = potential_fn_provider ( self . _prior , self . net , x , mcmc_method , transform ), init_fn = self . _build_mcmc_init_fn ( self . _prior , potential_fn_provider ( self . _prior , self . net , x , \"slice_np\" , transform ), transform = transform , ** mcmc_parameters , ), mcmc_method = mcmc_method , show_progress_bars = show_progress_bars , ** mcmc_parameters , ) samples = transform . inv ( transformed_samples ) elif sample_with == \"rejection\" : rejection_sampling_parameters = ( self . _potentially_replace_rejection_parameters ( rejection_sampling_parameters ) ) if \"proposal\" not in rejection_sampling_parameters : rejection_sampling_parameters [ \"proposal\" ] = self . _prior samples , _ = rejection_sample ( potential_fn = potential_fn_provider ( self . _prior , self . net , x , \"rejection\" ), num_samples = num_samples , ** rejection_sampling_parameters , ) else : raise NameError ( \"The only implemented sampling methods are `mcmc` and `rejection`.\" ) self . net . train ( True ) return samples . reshape (( * sample_shape , - 1 )) sample_conditional ( self , sample_shape , condition , dims_to_sample , x = None , sample_with = 'mcmc' , show_progress_bars = True , mcmc_method = None , mcmc_parameters = None , rejection_sampling_parameters = None ) \u00b6 Return samples from conditional posterior \\(p(\\theta_i|\\theta_j, x)\\) . In this function, we do not sample from the full posterior, but instead only from a few parameter dimensions while the other parameter dimensions are kept fixed at values specified in condition . Samples are obtained with MCMC. Parameters: Name Type Description Default sample_shape Union[torch.Size, Tuple[int, ...]] Desired shape of samples that are drawn from posterior. If sample_shape is multidimensional we simply draw sample_shape.numel() samples and then reshape into the desired shape. required condition Tensor Parameter set that all dimensions not specified in dims_to_sample will be fixed to. Should contain dim_theta elements, i.e. it could e.g. be a sample from the posterior distribution. The entries at all dims_to_sample will be ignored. required dims_to_sample List[int] Which dimensions to sample from. The dimensions not specified in dims_to_sample will be fixed to values given in condition . required x Optional[torch.Tensor] Conditioning context for posterior \\(p(\\theta|x)\\) . If not provided, fall back onto x passed to set_default_x() . None sample_with str Method to use for sampling from the posterior. Must be one of [ mcmc | rejection ]. In this method, the value of self._sample_with will be ignored. 'mcmc' show_progress_bars bool Whether to show sampling progress monitor. True mcmc_method Optional[str] Optional parameter to override self.mcmc_method . None mcmc_parameters Optional[Dict[str, Any]] Dictionary overriding the default parameters for MCMC. The following parameters are supported: thin to set the thinning factor for the chain. warmup_steps to set the initial number of samples to discard. num_chains for the number of chains. init_strategy for the initialisation strategy for chains; prior will draw init locations from prior, whereas sir will use Sequential- Importance-Resampling using init_strategy_num_candidates to find init locations. enable_transform a bool indicating whether MCMC is performed in z-scored (and unconstrained) space. None rejection_sampling_parameters Optional[Dict[str, Any]] Dictionary overriding the default parameters for rejection sampling. The following parameters are supported: proposal as the proposal distribtution (default is the prior). max_sampling_batch_size as the batchsize of samples being drawn from the proposal at every iteration. num_samples_to_find_max as the number of samples that are used to find the maximum of the potential_fn / proposal ratio. num_iter_to_find_max as the number of gradient ascent iterations to find the maximum of that ratio. m as multiplier to that ratio. None Returns: Type Description Tensor Samples from conditional posterior. Source code in sbi/inference/posteriors/ratio_based_posterior.py def sample_conditional ( self , sample_shape : Shape , condition : Tensor , dims_to_sample : List [ int ], x : Optional [ Tensor ] = None , sample_with : str = \"mcmc\" , show_progress_bars : bool = True , mcmc_method : Optional [ str ] = None , mcmc_parameters : Optional [ Dict [ str , Any ]] = None , rejection_sampling_parameters : Optional [ Dict [ str , Any ]] = None , ) -> Tensor : r \"\"\" Return samples from conditional posterior $p(\\theta_i|\\theta_j, x)$. In this function, we do not sample from the full posterior, but instead only from a few parameter dimensions while the other parameter dimensions are kept fixed at values specified in `condition`. Samples are obtained with MCMC. Args: sample_shape: Desired shape of samples that are drawn from posterior. If sample_shape is multidimensional we simply draw `sample_shape.numel()` samples and then reshape into the desired shape. condition: Parameter set that all dimensions not specified in `dims_to_sample` will be fixed to. Should contain dim_theta elements, i.e. it could e.g. be a sample from the posterior distribution. The entries at all `dims_to_sample` will be ignored. dims_to_sample: Which dimensions to sample from. The dimensions not specified in `dims_to_sample` will be fixed to values given in `condition`. x: Conditioning context for posterior $p(\\theta|x)$. If not provided, fall back onto `x` passed to `set_default_x()`. sample_with: Method to use for sampling from the posterior. Must be one of [`mcmc` | `rejection`]. In this method, the value of `self._sample_with` will be ignored. show_progress_bars: Whether to show sampling progress monitor. mcmc_method: Optional parameter to override `self.mcmc_method`. mcmc_parameters: Dictionary overriding the default parameters for MCMC. The following parameters are supported: `thin` to set the thinning factor for the chain. `warmup_steps` to set the initial number of samples to discard. `num_chains` for the number of chains. `init_strategy` for the initialisation strategy for chains; `prior` will draw init locations from prior, whereas `sir` will use Sequential- Importance-Resampling using `init_strategy_num_candidates` to find init locations. `enable_transform` a bool indicating whether MCMC is performed in z-scored (and unconstrained) space. rejection_sampling_parameters: Dictionary overriding the default parameters for rejection sampling. The following parameters are supported: `proposal` as the proposal distribtution (default is the prior). `max_sampling_batch_size` as the batchsize of samples being drawn from the proposal at every iteration. `num_samples_to_find_max` as the number of samples that are used to find the maximum of the `potential_fn / proposal` ratio. `num_iter_to_find_max` as the number of gradient ascent iterations to find the maximum of that ratio. `m` as multiplier to that ratio. Returns: Samples from conditional posterior. \"\"\" return super () . sample_conditional ( PotentialFunctionProvider (), sample_shape , condition , dims_to_sample , x , sample_with , show_progress_bars , mcmc_method , mcmc_parameters , rejection_sampling_parameters , ) set_default_x ( self , x ) inherited \u00b6 Set new default x for .sample(), .log_prob to use as conditioning context. This is a pure convenience to avoid having to repeatedly specify x in calls to .sample() and .log_prob() - only \u03b8 needs to be passed. This convenience is particularly useful when the posterior is focused, i.e. has been trained over multiple rounds to be accurate in the vicinity of a particular x=x_o (you can check if your posterior object is focused by printing it). NOTE: this method is chainable, i.e. will return the NeuralPosterior object so that calls like posterior.set_default_x(my_x).sample(mytheta) are possible. Parameters: Name Type Description Default x Tensor The default observation to set for the posterior \\(p(theta|x)\\) . required Returns: Type Description NeuralPosterior NeuralPosterior that will use a default x when not explicitly passed. Source code in sbi/inference/posteriors/ratio_based_posterior.py def set_default_x ( self , x : Tensor ) -> \"NeuralPosterior\" : \"\"\"Set new default x for `.sample(), .log_prob` to use as conditioning context. This is a pure convenience to avoid having to repeatedly specify `x` in calls to `.sample()` and `.log_prob()` - only \u03b8 needs to be passed. This convenience is particularly useful when the posterior is focused, i.e. has been trained over multiple rounds to be accurate in the vicinity of a particular `x=x_o` (you can check if your posterior object is focused by printing it). NOTE: this method is chainable, i.e. will return the NeuralPosterior object so that calls like `posterior.set_default_x(my_x).sample(mytheta)` are possible. Args: x: The default observation to set for the posterior $p(theta|x)$. Returns: `NeuralPosterior` that will use a default `x` when not explicitly passed. \"\"\" self . _x = process_x ( x , self . _x_shape , allow_iid_x = self . _allow_iid_x ) . to ( self . _device ) self . _num_iid_trials = self . _x . shape [ 0 ] return self set_mcmc_method ( self , method ) inherited \u00b6 Sets sampling method to for MCMC and returns NeuralPosterior . Parameters: Name Type Description Default method str Method to use. required Returns: Type Description NeuralPosterior NeuralPosterior for chainable calls. Source code in sbi/inference/posteriors/ratio_based_posterior.py def set_mcmc_method ( self , method : str ) -> \"NeuralPosterior\" : \"\"\"Sets sampling method to for MCMC and returns `NeuralPosterior`. Args: method: Method to use. Returns: `NeuralPosterior` for chainable calls. \"\"\" self . _mcmc_method = method return self set_mcmc_parameters ( self , parameters ) inherited \u00b6 Sets parameters for MCMC and returns NeuralPosterior . Parameters: Name Type Description Default parameters Dict[str, Any] Dictionary overriding the default parameters for MCMC. The following parameters are supported: thin to set the thinning factor for the chain, warmup_steps to set the initial number of samples to discard, num_chains for the number of chains, init_strategy for the initialisation strategy for chains; prior will draw init locations from prior, whereas sir will use Sequential- Importance-Resampling using init_strategy_num_candidates to find init locations. required Returns: Type Description NeuralPosterior NeuralPosterior for chainable calls. Source code in sbi/inference/posteriors/ratio_based_posterior.py def set_mcmc_parameters ( self , parameters : Dict [ str , Any ]) -> \"NeuralPosterior\" : \"\"\"Sets parameters for MCMC and returns `NeuralPosterior`. Args: parameters: Dictionary overriding the default parameters for MCMC. The following parameters are supported: `thin` to set the thinning factor for the chain, `warmup_steps` to set the initial number of samples to discard, `num_chains` for the number of chains, `init_strategy` for the initialisation strategy for chains; `prior` will draw init locations from prior, whereas `sir` will use Sequential- Importance-Resampling using `init_strategy_num_candidates` to find init locations. Returns: `NeuralPosterior` for chainable calls. \"\"\" self . _mcmc_parameters = parameters return self set_rejection_sampling_parameters ( self , parameters ) inherited \u00b6 Sets parameters for rejection sampling and returns NeuralPosterior . Parameters: Name Type Description Default parameters Dict[str, Any] Dictonary overriding the default parameters for rejection sampling. The following parameters are supported: proposal as the proposal distribtution. num_samples_to_find_max as the number of samples that are used to find the maximum of the potential_fn / proposal ratio. m as multiplier to that ratio. sampling_batch_size as the batchsize of samples being drawn from the proposal at every iteration. required Returns: Type Description NeuralPosterior `NeuralPosterior for chainable calls. Source code in sbi/inference/posteriors/ratio_based_posterior.py def set_rejection_sampling_parameters ( self , parameters : Dict [ str , Any ] ) -> \"NeuralPosterior\" : \"\"\"Sets parameters for rejection sampling and returns `NeuralPosterior`. Args: parameters: Dictonary overriding the default parameters for rejection sampling. The following parameters are supported: `proposal` as the proposal distribtution. `num_samples_to_find_max` as the number of samples that are used to find the maximum of the `potential_fn / proposal` ratio. `m` as multiplier to that ratio. `sampling_batch_size` as the batchsize of samples being drawn from the proposal at every iteration. Returns: `NeuralPosterior for chainable calls. \"\"\" self . _rejection_sampling_parameters = parameters return self set_sample_with ( self , sample_with ) inherited \u00b6 Set the sampling method for the NeuralPosterior . Parameters: Name Type Description Default sample_with str The method to sample with. required Returns: Type Description NeuralPosterior NeuralPosterior for chainable calls. Exceptions: Type Description ValueError on attempt to turn off MCMC sampling for family of methods that do not support rejection sampling. Source code in sbi/inference/posteriors/ratio_based_posterior.py def set_sample_with ( self , sample_with : str ) -> \"NeuralPosterior\" : \"\"\"Set the sampling method for the `NeuralPosterior`. Args: sample_with: The method to sample with. Returns: `NeuralPosterior` for chainable calls. Raises: ValueError: on attempt to turn off MCMC sampling for family of methods that do not support rejection sampling. \"\"\" if sample_with not in ( \"mcmc\" , \"rejection\" ): raise NameError ( \"The only implemented sampling methods are `mcmc` and `rejection`.\" ) self . _sample_with = sample_with return self Models \u00b6 sbi . utils . get_nn_models . posterior_nn ( model , z_score_theta = True , z_score_x = True , hidden_features = 50 , num_transforms = 5 , num_bins = 10 , embedding_net = Identity (), num_components = 10 ) \u00b6 Returns a function that builds a density estimator for learning the posterior. This function will usually be used for SNPE. The returned function is to be passed to the inference class when using the flexible interface. Parameters: Name Type Description Default model str The type of density estimator that will be created. One of [ mdn , made , maf , nsf ]. required z_score_theta bool Whether to z-score parameters \\(\\theta\\) before passing them into the network. True z_score_x bool Whether to z-score simulation outputs \\(x\\) before passing them into the network. True hidden_features int Number of hidden features. 50 num_transforms int Number of transforms when a flow is used. Only relevant if density estimator is a normalizing flow (i.e. currently either a maf or a nsf ). Ignored if density estimator is a mdn or made . 5 num_bins int Number of bins used for the splines in nsf . Ignored if density estimator not nsf . 10 embedding_net Module Optional embedding network for simulation outputs \\(x\\) . This embedding net allows to learn features from potentially high-dimensional simulation outputs. Identity() num_components int Number of mixture components for a mixture of Gaussians. Ignored if density estimator is not an mdn. 10 Source code in sbi/utils/get_nn_models.py def posterior_nn ( model : str , z_score_theta : bool = True , z_score_x : bool = True , hidden_features : int = 50 , num_transforms : int = 5 , num_bins : int = 10 , embedding_net : nn . Module = nn . Identity (), num_components : int = 10 , ) -> Callable : r \"\"\" Returns a function that builds a density estimator for learning the posterior. This function will usually be used for SNPE. The returned function is to be passed to the inference class when using the flexible interface. Args: model: The type of density estimator that will be created. One of [`mdn`, `made`, `maf`, `nsf`]. z_score_theta: Whether to z-score parameters $\\theta$ before passing them into the network. z_score_x: Whether to z-score simulation outputs $x$ before passing them into the network. hidden_features: Number of hidden features. num_transforms: Number of transforms when a flow is used. Only relevant if density estimator is a normalizing flow (i.e. currently either a `maf` or a `nsf`). Ignored if density estimator is a `mdn` or `made`. num_bins: Number of bins used for the splines in `nsf`. Ignored if density estimator not `nsf`. embedding_net: Optional embedding network for simulation outputs $x$. This embedding net allows to learn features from potentially high-dimensional simulation outputs. num_components: Number of mixture components for a mixture of Gaussians. Ignored if density estimator is not an mdn. \"\"\" kwargs = dict ( zip ( ( \"z_score_x\" , \"z_score_y\" , \"hidden_features\" , \"num_transforms\" , \"num_bins\" , \"embedding_net\" , \"num_components\" , ), ( z_score_theta , z_score_x , hidden_features , num_transforms , num_bins , embedding_net , num_components , ), ) ) if model == \"mdn_snpe_a\" : if num_components != 10 : raise ValueError ( \"You set `num_components`. For SNPE-A, this has to be done at \" \"instantiation of the inference object, i.e. \" \"`inference = SNPE_A(..., num_components=20)`\" ) kwargs . pop ( \"num_components\" ) def build_fn ( batch_theta , batch_x , num_components ): # Extract the number of components from the kwargs, such that # they are exposed as a kwargs, offering the possibility to later # override this kwarg with functools.partial. This is necessary # in order to make sure that the MDN in SNPE-A only has one # component when running the Algorithm 1 part. return build_mdn ( batch_x = batch_theta , batch_y = batch_x , num_components = num_components , ** kwargs ) else : def build_fn ( batch_theta , batch_x ): if model == \"mdn\" : return build_mdn ( batch_x = batch_theta , batch_y = batch_x , ** kwargs ) elif model == \"made\" : return build_made ( batch_x = batch_theta , batch_y = batch_x , ** kwargs ) elif model == \"maf\" : return build_maf ( batch_x = batch_theta , batch_y = batch_x , ** kwargs ) elif model == \"nsf\" : return build_nsf ( batch_x = batch_theta , batch_y = batch_x , ** kwargs ) else : raise NotImplementedError return build_fn sbi . utils . get_nn_models . likelihood_nn ( model , z_score_theta = True , z_score_x = True , hidden_features = 50 , num_transforms = 5 , num_bins = 10 , embedding_net = Identity (), num_components = 10 ) \u00b6 Returns a function that builds a density estimator for learning the likelihood. This function will usually be used for SNLE. The returned function is to be passed to the inference class when using the flexible interface. Parameters: Name Type Description Default model str The type of density estimator that will be created. One of [ mdn , made , maf , nsf ]. required z_score_theta bool Whether to z-score parameters \\(\\theta\\) before passing them into the network. True z_score_x bool Whether to z-score simulation outputs \\(x\\) before passing them into the network. True hidden_features int Number of hidden features. 50 num_transforms int Number of transforms when a flow is used. Only relevant if density estimator is a normalizing flow (i.e. currently either a maf or a nsf ). Ignored if density estimator is a mdn or made . 5 num_bins int Number of bins used for the splines in nsf . Ignored if density estimator not nsf . 10 embedding_net Module Optional embedding network for parameters \\(\\theta\\) . Identity() num_components int Number of mixture components for a mixture of Gaussians. Ignored if density estimator is not an mdn. 10 Source code in sbi/utils/get_nn_models.py def likelihood_nn ( model : str , z_score_theta : bool = True , z_score_x : bool = True , hidden_features : int = 50 , num_transforms : int = 5 , num_bins : int = 10 , embedding_net : nn . Module = nn . Identity (), num_components : int = 10 , ) -> Callable : r \"\"\" Returns a function that builds a density estimator for learning the likelihood. This function will usually be used for SNLE. The returned function is to be passed to the inference class when using the flexible interface. Args: model: The type of density estimator that will be created. One of [`mdn`, `made`, `maf`, `nsf`]. z_score_theta: Whether to z-score parameters $\\theta$ before passing them into the network. z_score_x: Whether to z-score simulation outputs $x$ before passing them into the network. hidden_features: Number of hidden features. num_transforms: Number of transforms when a flow is used. Only relevant if density estimator is a normalizing flow (i.e. currently either a `maf` or a `nsf`). Ignored if density estimator is a `mdn` or `made`. num_bins: Number of bins used for the splines in `nsf`. Ignored if density estimator not `nsf`. embedding_net: Optional embedding network for parameters $\\theta$. num_components: Number of mixture components for a mixture of Gaussians. Ignored if density estimator is not an mdn. \"\"\" kwargs = dict ( zip ( ( \"z_score_x\" , \"z_score_y\" , \"hidden_features\" , \"num_transforms\" , \"num_bins\" , \"embedding_net\" , \"num_components\" , ), ( z_score_x , z_score_theta , hidden_features , num_transforms , num_bins , embedding_net , num_components , ), ) ) def build_fn ( batch_theta , batch_x ): if model == \"mdn\" : return build_mdn ( batch_x = batch_x , batch_y = batch_theta , ** kwargs ) if model == \"made\" : return build_made ( batch_x = batch_x , batch_y = batch_theta , ** kwargs ) if model == \"maf\" : return build_maf ( batch_x = batch_x , batch_y = batch_theta , ** kwargs ) elif model == \"nsf\" : return build_nsf ( batch_x = batch_x , batch_y = batch_theta , ** kwargs ) else : raise NotImplementedError return build_fn sbi . utils . get_nn_models . classifier_nn ( model , z_score_theta = True , z_score_x = True , hidden_features = 50 , embedding_net_theta = Identity (), embedding_net_x = Identity ()) \u00b6 Returns a function that builds a classifier for learning density ratios. This function will usually be used for SNRE. The returned function is to be passed to the inference class when using the flexible interface. Note that in the view of the SNRE classifier we build below, x=theta and y=x. Parameters: Name Type Description Default model str The type of classifier that will be created. One of [ linear , mlp , resnet ]. required z_score_theta bool Whether to z-score parameters \\(\\theta\\) before passing them into the network. True z_score_x bool Whether to z-score simulation outputs \\(x\\) before passing them into the network. True hidden_features int Number of hidden features. 50 embedding_net_theta Module Optional embedding network for parameters \\(\\theta\\) . Identity() embedding_net_x Module Optional embedding network for simulation outputs \\(x\\) . This embedding net allows to learn features from potentially high-dimensional simulation outputs. Identity() Source code in sbi/utils/get_nn_models.py def classifier_nn ( model : str , z_score_theta : bool = True , z_score_x : bool = True , hidden_features : int = 50 , embedding_net_theta : nn . Module = nn . Identity (), embedding_net_x : nn . Module = nn . Identity (), ) -> Callable : r \"\"\" Returns a function that builds a classifier for learning density ratios. This function will usually be used for SNRE. The returned function is to be passed to the inference class when using the flexible interface. Note that in the view of the SNRE classifier we build below, x=theta and y=x. Args: model: The type of classifier that will be created. One of [`linear`, `mlp`, `resnet`]. z_score_theta: Whether to z-score parameters $\\theta$ before passing them into the network. z_score_x: Whether to z-score simulation outputs $x$ before passing them into the network. hidden_features: Number of hidden features. embedding_net_theta: Optional embedding network for parameters $\\theta$. embedding_net_x: Optional embedding network for simulation outputs $x$. This embedding net allows to learn features from potentially high-dimensional simulation outputs. \"\"\" kwargs = dict ( zip ( ( \"z_score_x\" , \"z_score_y\" , \"hidden_features\" , \"embedding_net_x\" , \"embedding_net_y\" , ), ( z_score_theta , z_score_x , hidden_features , embedding_net_theta , embedding_net_x , ), ) ) def build_fn ( batch_theta , batch_x ): if model == \"linear\" : return build_linear_classifier ( batch_x = batch_theta , batch_y = batch_x , ** kwargs ) if model == \"mlp\" : return build_mlp_classifier ( batch_x = batch_theta , batch_y = batch_x , ** kwargs ) if model == \"resnet\" : return build_resnet_classifier ( batch_x = batch_theta , batch_y = batch_x , ** kwargs ) else : raise NotImplementedError return build_fn Analysis \u00b6 sbi . analysis . plot . pairplot ( samples = None , points = None , limits = None , subset = None , upper = 'hist' , diag = 'hist' , figsize = ( 10 , 10 ), labels = None , ticks = [], points_colors = [ '#1f77b4' , '#ff7f0e' , '#2ca02c' , '#d62728' , '#9467bd' , '#8c564b' , '#e377c2' , '#7f7f7f' , '#bcbd22' , '#17becf' ], ** kwargs ) \u00b6 Plot samples in a 2D grid showing marginals and pairwise marginals. Each of the diagonal plots can be interpreted as a 1D-marginal of the distribution that the samples were drawn from. Each upper-diagonal plot can be interpreted as a 2D-marginal of the distribution. Parameters: Name Type Description Default samples Union[List[numpy.ndarray], List[torch.Tensor], numpy.ndarray, torch.Tensor] Samples used to build the histogram. None points Union[List[numpy.ndarray], List[torch.Tensor], numpy.ndarray, torch.Tensor] List of additional points to scatter. None limits Union[List, torch.Tensor] Array containing the plot xlim for each parameter dimension. If None, just use the min and max of the passed samples None subset List[int] List containing the dimensions to plot. E.g. subset=[1,3] will plot plot only the 1 st and 3 rd dimension but will discard the 0 th and 2 nd (and, if they exist, the 4 th , 5 th and so on). None upper Optional[str] Plotting style for upper diagonal, {hist, scatter, contour, cond, None}. 'hist' diag Optional[str] Plotting style for diagonal, {hist, cond, None}. 'hist' figsize Tuple Size of the entire figure. (10, 10) labels Optional[List[str]] List of strings specifying the names of the parameters. None ticks Union[List, torch.Tensor] Position of the ticks. [] points_colors List[str] Colors of the points . ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf'] **kwargs Additional arguments to adjust the plot, see the source code in _get_default_opts() in sbi.utils.plot for more details. {} Returns: figure and axis of posterior distribution plot Source code in sbi/analysis/plot.py def pairplot ( samples : Union [ List [ np . ndarray ], List [ torch . Tensor ], np . ndarray , torch . Tensor ] = None , points : Optional [ Union [ List [ np . ndarray ], List [ torch . Tensor ], np . ndarray , torch . Tensor ] ] = None , limits : Optional [ Union [ List , torch . Tensor ]] = None , subset : List [ int ] = None , upper : Optional [ str ] = \"hist\" , diag : Optional [ str ] = \"hist\" , figsize : Tuple = ( 10 , 10 ), labels : Optional [ List [ str ]] = None , ticks : Union [ List , torch . Tensor ] = [], points_colors : List [ str ] = plt . rcParams [ \"axes.prop_cycle\" ] . by_key ()[ \"color\" ], ** kwargs ): \"\"\" Plot samples in a 2D grid showing marginals and pairwise marginals. Each of the diagonal plots can be interpreted as a 1D-marginal of the distribution that the samples were drawn from. Each upper-diagonal plot can be interpreted as a 2D-marginal of the distribution. Args: samples: Samples used to build the histogram. points: List of additional points to scatter. limits: Array containing the plot xlim for each parameter dimension. If None, just use the min and max of the passed samples subset: List containing the dimensions to plot. E.g. subset=[1,3] will plot plot only the 1st and 3rd dimension but will discard the 0th and 2nd (and, if they exist, the 4th, 5th and so on). upper: Plotting style for upper diagonal, {hist, scatter, contour, cond, None}. diag: Plotting style for diagonal, {hist, cond, None}. figsize: Size of the entire figure. labels: List of strings specifying the names of the parameters. ticks: Position of the ticks. points_colors: Colors of the `points`. **kwargs: Additional arguments to adjust the plot, see the source code in `_get_default_opts()` in `sbi.utils.plot` for more details. Returns: figure and axis of posterior distribution plot \"\"\" return utils_pairplot ( samples = samples , points = points , limits = limits , subset = subset , upper = upper , diag = diag , figsize = figsize , labels = labels , ticks = ticks , points_colors = points_colors , warn_about_deprecation = False , ** kwargs , ) sbi . analysis . plot . conditional_pairplot ( density , condition , limits , points = None , subset = None , resolution = 50 , figsize = ( 10 , 10 ), labels = None , ticks = [], points_colors = [ '#1f77b4' , '#ff7f0e' , '#2ca02c' , '#d62728' , '#9467bd' , '#8c564b' , '#e377c2' , '#7f7f7f' , '#bcbd22' , '#17becf' ], ** kwargs ) \u00b6 Plot conditional distribution given all other parameters. The conditionals can be interpreted as slices through the density at a location given by condition . For example: Say we have a 3D density with parameters \\(\\theta_0\\) , \\(\\theta_1\\) , \\(\\theta_2\\) and a condition \\(c\\) passed by the user in the condition argument. For the plot of \\(\\theta_0\\) on the diagonal, this will plot the conditional \\(p(\\theta_0 | \\theta_1=c[1], \\theta_2=c[2])\\) . For the upper diagonal of \\(\\theta_1\\) and \\(\\theta_2\\) , it will plot \\(p(\\theta_1, \\theta_2 | \\theta_0=c[0])\\) . All other diagonals and upper-diagonals are built in the corresponding way. Parameters: Name Type Description Default density Any Probability density with a log_prob() method. required condition Tensor Condition that all but the one/two regarded parameters are fixed to. The condition should be of shape (1, dim_theta), i.e. it could e.g. be a sample from the posterior distribution. required limits Union[List, torch.Tensor] Limits in between which each parameter will be evaluated. required points Union[List[numpy.ndarray], List[torch.Tensor], numpy.ndarray, torch.Tensor] Additional points to scatter. None subset List[int] List containing the dimensions to plot. E.g. subset=[1,3] will plot plot only the 1 st and 3 rd dimension but will discard the 0 th and 2 nd (and, if they exist, the 4 th , 5 th and so on) None resolution int Resolution of the grid at which we evaluate the pdf . 50 figsize Tuple Size of the entire figure. (10, 10) labels Optional[List[str]] List of strings specifying the names of the parameters. None ticks Union[List, torch.Tensor] Position of the ticks. [] points_colors List[str] Colors of the points . ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf'] **kwargs Additional arguments to adjust the plot, see the source code in _get_default_opts() in sbi.utils.plot for more details. {} Returns: figure and axis of posterior distribution plot Source code in sbi/analysis/plot.py def conditional_pairplot ( density : Any , condition : torch . Tensor , limits : Union [ List , torch . Tensor ], points : Optional [ Union [ List [ np . ndarray ], List [ torch . Tensor ], np . ndarray , torch . Tensor ] ] = None , subset : List [ int ] = None , resolution : int = 50 , figsize : Tuple = ( 10 , 10 ), labels : Optional [ List [ str ]] = None , ticks : Union [ List , torch . Tensor ] = [], points_colors : List [ str ] = plt . rcParams [ \"axes.prop_cycle\" ] . by_key ()[ \"color\" ], ** kwargs ): r \"\"\" Plot conditional distribution given all other parameters. The conditionals can be interpreted as slices through the `density` at a location given by `condition`. For example: Say we have a 3D density with parameters $\\theta_0$, $\\theta_1$, $\\theta_2$ and a condition $c$ passed by the user in the `condition` argument. For the plot of $\\theta_0$ on the diagonal, this will plot the conditional $p(\\theta_0 | \\theta_1=c[1], \\theta_2=c[2])$. For the upper diagonal of $\\theta_1$ and $\\theta_2$, it will plot $p(\\theta_1, \\theta_2 | \\theta_0=c[0])$. All other diagonals and upper-diagonals are built in the corresponding way. Args: density: Probability density with a `log_prob()` method. condition: Condition that all but the one/two regarded parameters are fixed to. The condition should be of shape (1, dim_theta), i.e. it could e.g. be a sample from the posterior distribution. limits: Limits in between which each parameter will be evaluated. points: Additional points to scatter. subset: List containing the dimensions to plot. E.g. subset=[1,3] will plot plot only the 1st and 3rd dimension but will discard the 0th and 2nd (and, if they exist, the 4th, 5th and so on) resolution: Resolution of the grid at which we evaluate the `pdf`. figsize: Size of the entire figure. labels: List of strings specifying the names of the parameters. ticks: Position of the ticks. points_colors: Colors of the `points`. **kwargs: Additional arguments to adjust the plot, see the source code in `_get_default_opts()` in `sbi.utils.plot` for more details. Returns: figure and axis of posterior distribution plot \"\"\" return utils_conditional_pairplot ( density = density , condition = condition , limits = limits , points = points , subset = subset , resolution = resolution , figsize = figsize , labels = labels , ticks = ticks , points_colors = points_colors , warn_about_deprecation = False , ** kwargs , ) sbi . analysis . conditional_density . conditional_corrcoeff ( density , limits , condition , subset = None , resolution = 50 ) \u00b6 Returns the conditional correlation matrix of a distribution. To compute the conditional distribution, we condition all but two parameters to values from condition , and then compute the Pearson correlation coefficient \\(\\rho\\) between the remaining two parameters under the distribution density . We do so for any pair of parameters specified in subset , thus creating a matrix containing conditional correlations between any pair of parameters. If condition is a batch of conditions, this function computes the conditional correlation matrix for each one of them and returns the mean. Parameters: Name Type Description Default density Any Probability density function with .log_prob() function. required limits Tensor Limits within which to evaluate the density . required condition Tensor Values to condition the density on. If a batch of conditions is passed, we compute the conditional correlation matrix for each of them and return the average conditional correlation matrix. required subset Optional[List[int]] Evaluate the conditional distribution only on a subset of dimensions. If None this function uses all dimensions. None resolution int Number of grid points on which the conditional distribution is evaluated. A higher value increases the accuracy of the estimated correlation but also increases the computational cost. 50 Returns: Average conditional correlation matrix of shape either (num_dim, num_dim) or (len(subset), len(subset)) if subset was specified. Source code in sbi/analysis/conditional_density.py def conditional_corrcoeff ( density : Any , limits : Tensor , condition : Tensor , subset : Optional [ List [ int ]] = None , resolution : int = 50 , ) -> Tensor : r \"\"\" Returns the conditional correlation matrix of a distribution. To compute the conditional distribution, we condition all but two parameters to values from `condition`, and then compute the Pearson correlation coefficient $\\rho$ between the remaining two parameters under the distribution `density`. We do so for any pair of parameters specified in `subset`, thus creating a matrix containing conditional correlations between any pair of parameters. If `condition` is a batch of conditions, this function computes the conditional correlation matrix for each one of them and returns the mean. Args: density: Probability density function with `.log_prob()` function. limits: Limits within which to evaluate the `density`. condition: Values to condition the `density` on. If a batch of conditions is passed, we compute the conditional correlation matrix for each of them and return the average conditional correlation matrix. subset: Evaluate the conditional distribution only on a subset of dimensions. If `None` this function uses all dimensions. resolution: Number of grid points on which the conditional distribution is evaluated. A higher value increases the accuracy of the estimated correlation but also increases the computational cost. Returns: Average conditional correlation matrix of shape either `(num_dim, num_dim)` or `(len(subset), len(subset))` if `subset` was specified. \"\"\" return utils_conditional_corrcoeff ( density = density , limits = limits , condition = condition , subset = subset , resolution = resolution , warn_about_deprecation = False , )","title":"API Reference"},{"location":"reference/#api-reference","text":"","title":"API Reference"},{"location":"reference/#inference","text":"","title":"Inference"},{"location":"reference/#sbi.inference.base.infer","text":"Return posterior distribution by running simulation-based inference. This function provides a simple interface to run sbi. Inference is run for a single round and hence the returned posterior \\(p(\\theta|x)\\) can be sampled and evaluated for any \\(x\\) (i.e. it is amortized). The scope of this function is limited to the most essential features of sbi. For more flexibility (e.g. multi-round inference, different density estimators) please use the flexible interface described here: https://www.mackelab.org/sbi/tutorial/02_flexible_interface/ Parameters: Name Type Description Default simulator Callable A function that takes parameters \\(\\theta\\) and maps them to simulations, or observations, x , \\(\\mathrm{sim}(\\theta)\\to x\\) . Any regular Python callable (i.e. function or class with __call__ method) can be used. required prior A probability distribution that expresses prior knowledge about the parameters, e.g. which ranges are meaningful for them. Any object with .log_prob() and .sample() (for example, a PyTorch distribution) can be used. required method str What inference method to use. Either of SNPE, SNLE or SNRE. required num_simulations int Number of simulation calls. More simulations means a longer runtime, but a better posterior estimate. required num_workers int Number of parallel workers to use for simulations. 1 Returns: Posterior over parameters conditional on observations (amortized). Source code in sbi/inference/base.py def infer ( simulator : Callable , prior , method : str , num_simulations : int , num_workers : int = 1 ) -> NeuralPosterior : r \"\"\" Return posterior distribution by running simulation-based inference. This function provides a simple interface to run sbi. Inference is run for a single round and hence the returned posterior $p(\\theta|x)$ can be sampled and evaluated for any $x$ (i.e. it is amortized). The scope of this function is limited to the most essential features of sbi. For more flexibility (e.g. multi-round inference, different density estimators) please use the flexible interface described here: https://www.mackelab.org/sbi/tutorial/02_flexible_interface/ Args: simulator: A function that takes parameters $\\theta$ and maps them to simulations, or observations, `x`, $\\mathrm{sim}(\\theta)\\to x$. Any regular Python callable (i.e. function or class with `__call__` method) can be used. prior: A probability distribution that expresses prior knowledge about the parameters, e.g. which ranges are meaningful for them. Any object with `.log_prob()`and `.sample()` (for example, a PyTorch distribution) can be used. method: What inference method to use. Either of SNPE, SNLE or SNRE. num_simulations: Number of simulation calls. More simulations means a longer runtime, but a better posterior estimate. num_workers: Number of parallel workers to use for simulations. Returns: Posterior over parameters conditional on observations (amortized). \"\"\" try : method_fun : Callable = getattr ( sbi . inference , method . upper ()) except AttributeError : raise NameError ( \"Method not available. `method` must be one of 'SNPE', 'SNLE', 'SNRE'.\" ) simulator , prior = prepare_for_sbi ( simulator , prior ) inference = method_fun ( prior ) theta , x = simulate_for_sbi ( simulator = simulator , proposal = prior , num_simulations = num_simulations , num_workers = num_workers , ) _ = inference . append_simulations ( theta , x ) . train () posterior = inference . build_posterior () return posterior","title":"infer()"},{"location":"reference/#sbi.utils.user_input_checks.prepare_for_sbi","text":"Prepare simulator, prior and for usage in sbi. One of the goals is to allow you to use sbi with inputs computed in numpy. Attempts to meet the following requirements by reshaping and type-casting: the simulator function receives as input and returns a Tensor. the simulator can simulate batches of parameters and return batches of data. the prior does not produce batches and samples and evaluates to Tensor. the output shape is a torch.Size((1,N)) (i.e, has a leading batch dimension 1). If this is not possible, a suitable exception will be raised. Parameters: Name Type Description Default simulator Callable Simulator as provided by the user. required prior Prior as provided by the user. required Returns: Type Description Tuple[Callable, torch.distributions.distribution.Distribution] Tuple (simulator, prior, x_shape) checked and matching the requirements of sbi. Source code in sbi/utils/user_input_checks.py def prepare_for_sbi ( simulator : Callable , prior ) -> Tuple [ Callable , Distribution ]: \"\"\"Prepare simulator, prior and for usage in sbi. One of the goals is to allow you to use sbi with inputs computed in numpy. Attempts to meet the following requirements by reshaping and type-casting: - the simulator function receives as input and returns a Tensor.<br/> - the simulator can simulate batches of parameters and return batches of data.<br/> - the prior does not produce batches and samples and evaluates to Tensor.<br/> - the output shape is a `torch.Size((1,N))` (i.e, has a leading batch dimension 1). If this is not possible, a suitable exception will be raised. Args: simulator: Simulator as provided by the user. prior: Prior as provided by the user. Returns: Tuple (simulator, prior, x_shape) checked and matching the requirements of sbi. \"\"\" # Check prior, return PyTorch prior. prior , _ , prior_returns_numpy = process_prior ( prior ) # Check simulator, returns PyTorch simulator able to simulate batches. simulator = process_simulator ( simulator , prior , prior_returns_numpy ) # Consistency check after making ready for sbi. check_sbi_inputs ( simulator , prior ) return simulator , prior","title":"prepare_for_sbi()"},{"location":"reference/#sbi.inference.base.simulate_for_sbi","text":"Returns ( \\(\\theta, x\\) ) pairs obtained from sampling the proposal and simulating. This function performs two steps: Sample parameters \\(\\theta\\) from the proposal . Simulate these parameters to obtain \\(x\\) . Parameters: Name Type Description Default simulator Callable A function that takes parameters \\(\\theta\\) and maps them to simulations, or observations, x , \\(\\text{sim}(\\theta)\\to x\\) . Any regular Python callable (i.e. function or class with __call__ method) can be used. required proposal Any Probability distribution that the parameters \\(\\theta\\) are sampled from. required num_simulations int Number of simulations that are run. required num_workers int Number of parallel workers to use for simulations. 1 simulation_batch_size int Number of parameter sets that the simulator maps to data x at once. If None, we simulate all parameter sets at the same time. If >= 1, the simulator has to process data of shape (simulation_batch_size, parameter_dimension). 1 show_progress_bar bool Whether to show a progress bar for simulating. This will not affect whether there will be a progressbar while drawing samples from the proposal. True Returns: Sampled parameters \\(\\theta\\) and simulation-outputs \\(x\\) . Source code in sbi/inference/base.py def simulate_for_sbi ( simulator : Callable , proposal : Any , num_simulations : int , num_workers : int = 1 , simulation_batch_size : int = 1 , show_progress_bar : bool = True , ) -> Tuple [ Tensor , Tensor ]: r \"\"\" Returns ($\\theta, x$) pairs obtained from sampling the proposal and simulating. This function performs two steps: - Sample parameters $\\theta$ from the `proposal`. - Simulate these parameters to obtain $x$. Args: simulator: A function that takes parameters $\\theta$ and maps them to simulations, or observations, `x`, $\\text{sim}(\\theta)\\to x$. Any regular Python callable (i.e. function or class with `__call__` method) can be used. proposal: Probability distribution that the parameters $\\theta$ are sampled from. num_simulations: Number of simulations that are run. num_workers: Number of parallel workers to use for simulations. simulation_batch_size: Number of parameter sets that the simulator maps to data x at once. If None, we simulate all parameter sets at the same time. If >= 1, the simulator has to process data of shape (simulation_batch_size, parameter_dimension). show_progress_bar: Whether to show a progress bar for simulating. This will not affect whether there will be a progressbar while drawing samples from the proposal. Returns: Sampled parameters $\\theta$ and simulation-outputs $x$. \"\"\" check_if_proposal_has_default_x ( proposal ) theta = proposal . sample (( num_simulations ,)) x = simulate_in_batches ( simulator , theta , simulation_batch_size , num_workers , show_progress_bar ) return theta , x","title":"simulate_for_sbi()"},{"location":"reference/#sbi.inference.snpe.snpe_c.SNPE_C","text":"","title":"SNPE_C"},{"location":"reference/#sbi.inference.snle.snle_a.SNLE_A","text":"","title":"SNLE_A"},{"location":"reference/#sbi.inference.snre.snre_a.SNRE_A","text":"","title":"SNRE_A"},{"location":"reference/#sbi.inference.snre.snre_b.SNRE_B","text":"","title":"SNRE_B"},{"location":"reference/#sbi.inference.abc.mcabc.MCABC","text":"","title":"MCABC"},{"location":"reference/#sbi.inference.abc.smcabc.SMCABC","text":"","title":"SMCABC"},{"location":"reference/#posteriors","text":"","title":"Posteriors"},{"location":"reference/#sbi.inference.posteriors.direct_posterior.DirectPosterior","text":"Posterior \\(p(\\theta|x)\\) with log_prob() and sample() methods, obtained with SNPE. SNPE trains a neural network to directly approximate the posterior distribution. However, for bounded priors, the neural network can have leakage: it puts non-zero mass in regions where the prior is zero. The DirectPosterior class wraps the trained network to deal with these cases. Specifically, this class offers the following functionality: - correct the calculation of the log probability such that it compensates for the leakage. - reject samples that lie outside of the prior bounds. - alternatively, if leakage is very high (which can happen for multi-round SNPE), sample from the posterior with MCMC. The neural network itself can be accessed via the .net attribute.","title":"DirectPosterior"},{"location":"reference/#sbi.inference.posteriors.likelihood_based_posterior.LikelihoodBasedPosterior","text":"Posterior \\(p(\\theta|x)\\) with log_prob() and sample() methods, obtained with SNLE. SNLE trains a neural network to approximate the likelihood \\(p(x|\\theta)\\) . The SNLE_Posterior class wraps the trained network such that one can directly evaluate the unnormalized posterior log probability \\(p(\\theta|x) \\propto p(x|\\theta) \\cdot p(\\theta)\\) and draw samples from the posterior with MCMC. The neural network itself can be accessed via the .net attribute.","title":"LikelihoodBasedPosterior"},{"location":"reference/#sbi.inference.posteriors.ratio_based_posterior.RatioBasedPosterior","text":"Posterior \\(p(\\theta|x)\\) with log_prob() and sample() methods, obtained with SNRE. SNRE trains a neural network to approximate likelihood ratios, which in turn can be used obtain an unnormalized posterior \\(p(\\theta|x) \\propto p(x|\\theta) \\cdot p(\\theta)\\) . The SNRE_Posterior class wraps the trained network such that one can directly evaluate the unnormalized posterior log-probability \\(p(\\theta|x) \\propto p(x|\\theta) \\cdot p(\\theta)\\) and draw samples from the posterior with MCMC. Note that, in the case of single-round SNRE_A / AALR, it is possible to evaluate the log-probability of the normalized posterior, but sampling still requires MCMC. The neural network itself can be accessed via the .net attribute.","title":"RatioBasedPosterior"},{"location":"reference/#models","text":"","title":"Models"},{"location":"reference/#sbi.utils.get_nn_models.posterior_nn","text":"Returns a function that builds a density estimator for learning the posterior. This function will usually be used for SNPE. The returned function is to be passed to the inference class when using the flexible interface. Parameters: Name Type Description Default model str The type of density estimator that will be created. One of [ mdn , made , maf , nsf ]. required z_score_theta bool Whether to z-score parameters \\(\\theta\\) before passing them into the network. True z_score_x bool Whether to z-score simulation outputs \\(x\\) before passing them into the network. True hidden_features int Number of hidden features. 50 num_transforms int Number of transforms when a flow is used. Only relevant if density estimator is a normalizing flow (i.e. currently either a maf or a nsf ). Ignored if density estimator is a mdn or made . 5 num_bins int Number of bins used for the splines in nsf . Ignored if density estimator not nsf . 10 embedding_net Module Optional embedding network for simulation outputs \\(x\\) . This embedding net allows to learn features from potentially high-dimensional simulation outputs. Identity() num_components int Number of mixture components for a mixture of Gaussians. Ignored if density estimator is not an mdn. 10 Source code in sbi/utils/get_nn_models.py def posterior_nn ( model : str , z_score_theta : bool = True , z_score_x : bool = True , hidden_features : int = 50 , num_transforms : int = 5 , num_bins : int = 10 , embedding_net : nn . Module = nn . Identity (), num_components : int = 10 , ) -> Callable : r \"\"\" Returns a function that builds a density estimator for learning the posterior. This function will usually be used for SNPE. The returned function is to be passed to the inference class when using the flexible interface. Args: model: The type of density estimator that will be created. One of [`mdn`, `made`, `maf`, `nsf`]. z_score_theta: Whether to z-score parameters $\\theta$ before passing them into the network. z_score_x: Whether to z-score simulation outputs $x$ before passing them into the network. hidden_features: Number of hidden features. num_transforms: Number of transforms when a flow is used. Only relevant if density estimator is a normalizing flow (i.e. currently either a `maf` or a `nsf`). Ignored if density estimator is a `mdn` or `made`. num_bins: Number of bins used for the splines in `nsf`. Ignored if density estimator not `nsf`. embedding_net: Optional embedding network for simulation outputs $x$. This embedding net allows to learn features from potentially high-dimensional simulation outputs. num_components: Number of mixture components for a mixture of Gaussians. Ignored if density estimator is not an mdn. \"\"\" kwargs = dict ( zip ( ( \"z_score_x\" , \"z_score_y\" , \"hidden_features\" , \"num_transforms\" , \"num_bins\" , \"embedding_net\" , \"num_components\" , ), ( z_score_theta , z_score_x , hidden_features , num_transforms , num_bins , embedding_net , num_components , ), ) ) if model == \"mdn_snpe_a\" : if num_components != 10 : raise ValueError ( \"You set `num_components`. For SNPE-A, this has to be done at \" \"instantiation of the inference object, i.e. \" \"`inference = SNPE_A(..., num_components=20)`\" ) kwargs . pop ( \"num_components\" ) def build_fn ( batch_theta , batch_x , num_components ): # Extract the number of components from the kwargs, such that # they are exposed as a kwargs, offering the possibility to later # override this kwarg with functools.partial. This is necessary # in order to make sure that the MDN in SNPE-A only has one # component when running the Algorithm 1 part. return build_mdn ( batch_x = batch_theta , batch_y = batch_x , num_components = num_components , ** kwargs ) else : def build_fn ( batch_theta , batch_x ): if model == \"mdn\" : return build_mdn ( batch_x = batch_theta , batch_y = batch_x , ** kwargs ) elif model == \"made\" : return build_made ( batch_x = batch_theta , batch_y = batch_x , ** kwargs ) elif model == \"maf\" : return build_maf ( batch_x = batch_theta , batch_y = batch_x , ** kwargs ) elif model == \"nsf\" : return build_nsf ( batch_x = batch_theta , batch_y = batch_x , ** kwargs ) else : raise NotImplementedError return build_fn","title":"posterior_nn()"},{"location":"reference/#sbi.utils.get_nn_models.likelihood_nn","text":"Returns a function that builds a density estimator for learning the likelihood. This function will usually be used for SNLE. The returned function is to be passed to the inference class when using the flexible interface. Parameters: Name Type Description Default model str The type of density estimator that will be created. One of [ mdn , made , maf , nsf ]. required z_score_theta bool Whether to z-score parameters \\(\\theta\\) before passing them into the network. True z_score_x bool Whether to z-score simulation outputs \\(x\\) before passing them into the network. True hidden_features int Number of hidden features. 50 num_transforms int Number of transforms when a flow is used. Only relevant if density estimator is a normalizing flow (i.e. currently either a maf or a nsf ). Ignored if density estimator is a mdn or made . 5 num_bins int Number of bins used for the splines in nsf . Ignored if density estimator not nsf . 10 embedding_net Module Optional embedding network for parameters \\(\\theta\\) . Identity() num_components int Number of mixture components for a mixture of Gaussians. Ignored if density estimator is not an mdn. 10 Source code in sbi/utils/get_nn_models.py def likelihood_nn ( model : str , z_score_theta : bool = True , z_score_x : bool = True , hidden_features : int = 50 , num_transforms : int = 5 , num_bins : int = 10 , embedding_net : nn . Module = nn . Identity (), num_components : int = 10 , ) -> Callable : r \"\"\" Returns a function that builds a density estimator for learning the likelihood. This function will usually be used for SNLE. The returned function is to be passed to the inference class when using the flexible interface. Args: model: The type of density estimator that will be created. One of [`mdn`, `made`, `maf`, `nsf`]. z_score_theta: Whether to z-score parameters $\\theta$ before passing them into the network. z_score_x: Whether to z-score simulation outputs $x$ before passing them into the network. hidden_features: Number of hidden features. num_transforms: Number of transforms when a flow is used. Only relevant if density estimator is a normalizing flow (i.e. currently either a `maf` or a `nsf`). Ignored if density estimator is a `mdn` or `made`. num_bins: Number of bins used for the splines in `nsf`. Ignored if density estimator not `nsf`. embedding_net: Optional embedding network for parameters $\\theta$. num_components: Number of mixture components for a mixture of Gaussians. Ignored if density estimator is not an mdn. \"\"\" kwargs = dict ( zip ( ( \"z_score_x\" , \"z_score_y\" , \"hidden_features\" , \"num_transforms\" , \"num_bins\" , \"embedding_net\" , \"num_components\" , ), ( z_score_x , z_score_theta , hidden_features , num_transforms , num_bins , embedding_net , num_components , ), ) ) def build_fn ( batch_theta , batch_x ): if model == \"mdn\" : return build_mdn ( batch_x = batch_x , batch_y = batch_theta , ** kwargs ) if model == \"made\" : return build_made ( batch_x = batch_x , batch_y = batch_theta , ** kwargs ) if model == \"maf\" : return build_maf ( batch_x = batch_x , batch_y = batch_theta , ** kwargs ) elif model == \"nsf\" : return build_nsf ( batch_x = batch_x , batch_y = batch_theta , ** kwargs ) else : raise NotImplementedError return build_fn","title":"likelihood_nn()"},{"location":"reference/#sbi.utils.get_nn_models.classifier_nn","text":"Returns a function that builds a classifier for learning density ratios. This function will usually be used for SNRE. The returned function is to be passed to the inference class when using the flexible interface. Note that in the view of the SNRE classifier we build below, x=theta and y=x. Parameters: Name Type Description Default model str The type of classifier that will be created. One of [ linear , mlp , resnet ]. required z_score_theta bool Whether to z-score parameters \\(\\theta\\) before passing them into the network. True z_score_x bool Whether to z-score simulation outputs \\(x\\) before passing them into the network. True hidden_features int Number of hidden features. 50 embedding_net_theta Module Optional embedding network for parameters \\(\\theta\\) . Identity() embedding_net_x Module Optional embedding network for simulation outputs \\(x\\) . This embedding net allows to learn features from potentially high-dimensional simulation outputs. Identity() Source code in sbi/utils/get_nn_models.py def classifier_nn ( model : str , z_score_theta : bool = True , z_score_x : bool = True , hidden_features : int = 50 , embedding_net_theta : nn . Module = nn . Identity (), embedding_net_x : nn . Module = nn . Identity (), ) -> Callable : r \"\"\" Returns a function that builds a classifier for learning density ratios. This function will usually be used for SNRE. The returned function is to be passed to the inference class when using the flexible interface. Note that in the view of the SNRE classifier we build below, x=theta and y=x. Args: model: The type of classifier that will be created. One of [`linear`, `mlp`, `resnet`]. z_score_theta: Whether to z-score parameters $\\theta$ before passing them into the network. z_score_x: Whether to z-score simulation outputs $x$ before passing them into the network. hidden_features: Number of hidden features. embedding_net_theta: Optional embedding network for parameters $\\theta$. embedding_net_x: Optional embedding network for simulation outputs $x$. This embedding net allows to learn features from potentially high-dimensional simulation outputs. \"\"\" kwargs = dict ( zip ( ( \"z_score_x\" , \"z_score_y\" , \"hidden_features\" , \"embedding_net_x\" , \"embedding_net_y\" , ), ( z_score_theta , z_score_x , hidden_features , embedding_net_theta , embedding_net_x , ), ) ) def build_fn ( batch_theta , batch_x ): if model == \"linear\" : return build_linear_classifier ( batch_x = batch_theta , batch_y = batch_x , ** kwargs ) if model == \"mlp\" : return build_mlp_classifier ( batch_x = batch_theta , batch_y = batch_x , ** kwargs ) if model == \"resnet\" : return build_resnet_classifier ( batch_x = batch_theta , batch_y = batch_x , ** kwargs ) else : raise NotImplementedError return build_fn","title":"classifier_nn()"},{"location":"reference/#analysis","text":"","title":"Analysis"},{"location":"reference/#sbi.analysis.plot.pairplot","text":"Plot samples in a 2D grid showing marginals and pairwise marginals. Each of the diagonal plots can be interpreted as a 1D-marginal of the distribution that the samples were drawn from. Each upper-diagonal plot can be interpreted as a 2D-marginal of the distribution. Parameters: Name Type Description Default samples Union[List[numpy.ndarray], List[torch.Tensor], numpy.ndarray, torch.Tensor] Samples used to build the histogram. None points Union[List[numpy.ndarray], List[torch.Tensor], numpy.ndarray, torch.Tensor] List of additional points to scatter. None limits Union[List, torch.Tensor] Array containing the plot xlim for each parameter dimension. If None, just use the min and max of the passed samples None subset List[int] List containing the dimensions to plot. E.g. subset=[1,3] will plot plot only the 1 st and 3 rd dimension but will discard the 0 th and 2 nd (and, if they exist, the 4 th , 5 th and so on). None upper Optional[str] Plotting style for upper diagonal, {hist, scatter, contour, cond, None}. 'hist' diag Optional[str] Plotting style for diagonal, {hist, cond, None}. 'hist' figsize Tuple Size of the entire figure. (10, 10) labels Optional[List[str]] List of strings specifying the names of the parameters. None ticks Union[List, torch.Tensor] Position of the ticks. [] points_colors List[str] Colors of the points . ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf'] **kwargs Additional arguments to adjust the plot, see the source code in _get_default_opts() in sbi.utils.plot for more details. {} Returns: figure and axis of posterior distribution plot Source code in sbi/analysis/plot.py def pairplot ( samples : Union [ List [ np . ndarray ], List [ torch . Tensor ], np . ndarray , torch . Tensor ] = None , points : Optional [ Union [ List [ np . ndarray ], List [ torch . Tensor ], np . ndarray , torch . Tensor ] ] = None , limits : Optional [ Union [ List , torch . Tensor ]] = None , subset : List [ int ] = None , upper : Optional [ str ] = \"hist\" , diag : Optional [ str ] = \"hist\" , figsize : Tuple = ( 10 , 10 ), labels : Optional [ List [ str ]] = None , ticks : Union [ List , torch . Tensor ] = [], points_colors : List [ str ] = plt . rcParams [ \"axes.prop_cycle\" ] . by_key ()[ \"color\" ], ** kwargs ): \"\"\" Plot samples in a 2D grid showing marginals and pairwise marginals. Each of the diagonal plots can be interpreted as a 1D-marginal of the distribution that the samples were drawn from. Each upper-diagonal plot can be interpreted as a 2D-marginal of the distribution. Args: samples: Samples used to build the histogram. points: List of additional points to scatter. limits: Array containing the plot xlim for each parameter dimension. If None, just use the min and max of the passed samples subset: List containing the dimensions to plot. E.g. subset=[1,3] will plot plot only the 1st and 3rd dimension but will discard the 0th and 2nd (and, if they exist, the 4th, 5th and so on). upper: Plotting style for upper diagonal, {hist, scatter, contour, cond, None}. diag: Plotting style for diagonal, {hist, cond, None}. figsize: Size of the entire figure. labels: List of strings specifying the names of the parameters. ticks: Position of the ticks. points_colors: Colors of the `points`. **kwargs: Additional arguments to adjust the plot, see the source code in `_get_default_opts()` in `sbi.utils.plot` for more details. Returns: figure and axis of posterior distribution plot \"\"\" return utils_pairplot ( samples = samples , points = points , limits = limits , subset = subset , upper = upper , diag = diag , figsize = figsize , labels = labels , ticks = ticks , points_colors = points_colors , warn_about_deprecation = False , ** kwargs , )","title":"pairplot()"},{"location":"reference/#sbi.analysis.plot.conditional_pairplot","text":"Plot conditional distribution given all other parameters. The conditionals can be interpreted as slices through the density at a location given by condition . For example: Say we have a 3D density with parameters \\(\\theta_0\\) , \\(\\theta_1\\) , \\(\\theta_2\\) and a condition \\(c\\) passed by the user in the condition argument. For the plot of \\(\\theta_0\\) on the diagonal, this will plot the conditional \\(p(\\theta_0 | \\theta_1=c[1], \\theta_2=c[2])\\) . For the upper diagonal of \\(\\theta_1\\) and \\(\\theta_2\\) , it will plot \\(p(\\theta_1, \\theta_2 | \\theta_0=c[0])\\) . All other diagonals and upper-diagonals are built in the corresponding way. Parameters: Name Type Description Default density Any Probability density with a log_prob() method. required condition Tensor Condition that all but the one/two regarded parameters are fixed to. The condition should be of shape (1, dim_theta), i.e. it could e.g. be a sample from the posterior distribution. required limits Union[List, torch.Tensor] Limits in between which each parameter will be evaluated. required points Union[List[numpy.ndarray], List[torch.Tensor], numpy.ndarray, torch.Tensor] Additional points to scatter. None subset List[int] List containing the dimensions to plot. E.g. subset=[1,3] will plot plot only the 1 st and 3 rd dimension but will discard the 0 th and 2 nd (and, if they exist, the 4 th , 5 th and so on) None resolution int Resolution of the grid at which we evaluate the pdf . 50 figsize Tuple Size of the entire figure. (10, 10) labels Optional[List[str]] List of strings specifying the names of the parameters. None ticks Union[List, torch.Tensor] Position of the ticks. [] points_colors List[str] Colors of the points . ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf'] **kwargs Additional arguments to adjust the plot, see the source code in _get_default_opts() in sbi.utils.plot for more details. {} Returns: figure and axis of posterior distribution plot Source code in sbi/analysis/plot.py def conditional_pairplot ( density : Any , condition : torch . Tensor , limits : Union [ List , torch . Tensor ], points : Optional [ Union [ List [ np . ndarray ], List [ torch . Tensor ], np . ndarray , torch . Tensor ] ] = None , subset : List [ int ] = None , resolution : int = 50 , figsize : Tuple = ( 10 , 10 ), labels : Optional [ List [ str ]] = None , ticks : Union [ List , torch . Tensor ] = [], points_colors : List [ str ] = plt . rcParams [ \"axes.prop_cycle\" ] . by_key ()[ \"color\" ], ** kwargs ): r \"\"\" Plot conditional distribution given all other parameters. The conditionals can be interpreted as slices through the `density` at a location given by `condition`. For example: Say we have a 3D density with parameters $\\theta_0$, $\\theta_1$, $\\theta_2$ and a condition $c$ passed by the user in the `condition` argument. For the plot of $\\theta_0$ on the diagonal, this will plot the conditional $p(\\theta_0 | \\theta_1=c[1], \\theta_2=c[2])$. For the upper diagonal of $\\theta_1$ and $\\theta_2$, it will plot $p(\\theta_1, \\theta_2 | \\theta_0=c[0])$. All other diagonals and upper-diagonals are built in the corresponding way. Args: density: Probability density with a `log_prob()` method. condition: Condition that all but the one/two regarded parameters are fixed to. The condition should be of shape (1, dim_theta), i.e. it could e.g. be a sample from the posterior distribution. limits: Limits in between which each parameter will be evaluated. points: Additional points to scatter. subset: List containing the dimensions to plot. E.g. subset=[1,3] will plot plot only the 1st and 3rd dimension but will discard the 0th and 2nd (and, if they exist, the 4th, 5th and so on) resolution: Resolution of the grid at which we evaluate the `pdf`. figsize: Size of the entire figure. labels: List of strings specifying the names of the parameters. ticks: Position of the ticks. points_colors: Colors of the `points`. **kwargs: Additional arguments to adjust the plot, see the source code in `_get_default_opts()` in `sbi.utils.plot` for more details. Returns: figure and axis of posterior distribution plot \"\"\" return utils_conditional_pairplot ( density = density , condition = condition , limits = limits , points = points , subset = subset , resolution = resolution , figsize = figsize , labels = labels , ticks = ticks , points_colors = points_colors , warn_about_deprecation = False , ** kwargs , )","title":"conditional_pairplot()"},{"location":"reference/#sbi.analysis.conditional_density.conditional_corrcoeff","text":"Returns the conditional correlation matrix of a distribution. To compute the conditional distribution, we condition all but two parameters to values from condition , and then compute the Pearson correlation coefficient \\(\\rho\\) between the remaining two parameters under the distribution density . We do so for any pair of parameters specified in subset , thus creating a matrix containing conditional correlations between any pair of parameters. If condition is a batch of conditions, this function computes the conditional correlation matrix for each one of them and returns the mean. Parameters: Name Type Description Default density Any Probability density function with .log_prob() function. required limits Tensor Limits within which to evaluate the density . required condition Tensor Values to condition the density on. If a batch of conditions is passed, we compute the conditional correlation matrix for each of them and return the average conditional correlation matrix. required subset Optional[List[int]] Evaluate the conditional distribution only on a subset of dimensions. If None this function uses all dimensions. None resolution int Number of grid points on which the conditional distribution is evaluated. A higher value increases the accuracy of the estimated correlation but also increases the computational cost. 50 Returns: Average conditional correlation matrix of shape either (num_dim, num_dim) or (len(subset), len(subset)) if subset was specified. Source code in sbi/analysis/conditional_density.py def conditional_corrcoeff ( density : Any , limits : Tensor , condition : Tensor , subset : Optional [ List [ int ]] = None , resolution : int = 50 , ) -> Tensor : r \"\"\" Returns the conditional correlation matrix of a distribution. To compute the conditional distribution, we condition all but two parameters to values from `condition`, and then compute the Pearson correlation coefficient $\\rho$ between the remaining two parameters under the distribution `density`. We do so for any pair of parameters specified in `subset`, thus creating a matrix containing conditional correlations between any pair of parameters. If `condition` is a batch of conditions, this function computes the conditional correlation matrix for each one of them and returns the mean. Args: density: Probability density function with `.log_prob()` function. limits: Limits within which to evaluate the `density`. condition: Values to condition the `density` on. If a batch of conditions is passed, we compute the conditional correlation matrix for each of them and return the average conditional correlation matrix. subset: Evaluate the conditional distribution only on a subset of dimensions. If `None` this function uses all dimensions. resolution: Number of grid points on which the conditional distribution is evaluated. A higher value increases the accuracy of the estimated correlation but also increases the computational cost. Returns: Average conditional correlation matrix of shape either `(num_dim, num_dim)` or `(len(subset), len(subset))` if `subset` was specified. \"\"\" return utils_conditional_corrcoeff ( density = density , limits = limits , condition = condition , subset = subset , resolution = resolution , warn_about_deprecation = False , )","title":"conditional_corrcoeff()"},{"location":"examples/00_HH_simulator/","text":"Inference on Hodgkin-Huxley model: tutorial \u00b6 In this tutorial, we use sbi to do inference on a Hodgkin-Huxley model from neuroscience (Hodgkin and Huxley, 1952). We will learn two parameters ( \\(\\bar g_{Na}\\) , \\(\\bar g_K\\) ) based on a current-clamp recording, that we generate synthetically (in practice, this would be an experimental observation). Note, you find the original version of this notebook at https://github.com/mackelab/sbi/blob/main/examples/00_HH_simulator.ipynb in the sbi repository. First we are going to import basic packages. import numpy as np import torch # visualization import matplotlib as mpl import matplotlib.pyplot as plt # sbi from sbi import utils as utils from sbi import analysis as analysis from sbi.inference.base import infer # remove top and right axis from plots mpl . rcParams [ 'axes.spines.right' ] = False mpl . rcParams [ 'axes.spines.top' ] = False Different required components \u00b6 Before running inference, let us define the different required components: observed data prior over model parameters simulator 1. Observed data \u00b6 Let us assume we current-clamped a neuron and recorded the following voltage trace: In fact, this voltage trace was not measured experimentally but synthetically generated by simulating a Hodgkin-Huxley model with particular parameters ( \\(\\bar g_{Na}\\) , \\(\\bar g_K\\) ). We will come back to this point later in the tutorial. 2. Simulator \u00b6 We would like to infer the posterior over the two parameters ( \\(\\bar g_{Na}\\) , \\(\\bar g_K\\) ) of a Hodgkin-Huxley model, given the observed electrophysiological recording above. The model has channel kinetics as in Pospischil et al. 2008 , and is defined by the following set of differential equations (parameters of interest highlighted in orange): \\[ \\scriptsize \\begin{align} C_m\\frac{dV}{dt}&=g_1\\left(E_1-V\\right)+ \\color{orange}{\\bar{g}_{Na}}m^3h\\left(E_{Na}-V\\right)+ \\color{orange}{\\bar{g}_{K}}n^4\\left(E_K-V\\right)+ \\bar{g}_Mp\\left(E_K-V\\right)+ I_{inj}+ \\sigma\\eta\\left(t\\right)\\\\ \\frac{dq}{dt}&=\\frac{q_\\infty\\left(V\\right)-q}{\\tau_q\\left(V\\right)},\\;q\\in\\{m,h,n,p\\} \\end{align} \\] Above, \\(V\\) represents the membrane potential, \\(C_m\\) is the membrane capacitance, \\(g_{\\text{l}}\\) is the leak conductance, \\(E_{\\text{l}}\\) is the membrane reversal potential, \\(\\bar{g}_c\\) is the density of channels of type \\(c\\) ( \\(\\text{Na}^+\\) , \\(\\text{K}^+\\) , M), \\(E_c\\) is the reversal potential of \\(c\\) , ( \\(m\\) , \\(h\\) , \\(n\\) , \\(p\\) ) are the respective channel gating kinetic variables, and \\(\\sigma \\eta(t)\\) is the intrinsic neural noise. The right hand side of the voltage dynamics is composed of a leak current, a voltage-dependent \\(\\text{Na}^+\\) current, a delayed-rectifier \\(\\text{K}^+\\) current, a slow voltage-dependent \\(\\text{K}^+\\) current responsible for spike-frequency adaptation, and an injected current \\(I_{\\text{inj}}\\) . Channel gating variables \\(q\\) have dynamics fully characterized by the neuron membrane potential \\(V\\) , given the respective steady-state \\(q_{\\infty}(V)\\) and time constant \\(\\tau_{q}(V)\\) (details in Pospischil et al. 2008). The input current \\(I_{\\text{inj}}\\) is defined as from HH_helper_functions import syn_current I , t_on , t_off , dt , t , A_soma = syn_current () The Hodgkin-Huxley simulator is given by: from HH_helper_functions import HHsimulator Putting the input current and the simulator together: def run_HH_model ( params ): params = np . asarray ( params ) # input current, time step I , t_on , t_off , dt , t , A_soma = syn_current () t = np . arange ( 0 , len ( I ), 1 ) * dt # initial voltage V0 = - 70 states = HHsimulator ( V0 , params . reshape ( 1 , - 1 ), dt , t , I ) return dict ( data = states . reshape ( - 1 ), time = t , dt = dt , I = I . reshape ( - 1 )) To get an idea of the output of the Hodgkin-Huxley model, let us generate some voltage traces for different parameters ( \\(\\bar g_{Na}\\) , \\(\\bar g_K\\) ), given the input current \\(I_{\\text{inj}}\\) : # three sets of (g_Na, g_K) params = np . array ([[ 50. , 1. ],[ 4. , 1.5 ],[ 20. , 15. ]]) num_samples = len ( params [:, 0 ]) sim_samples = np . zeros (( num_samples , len ( I ))) for i in range ( num_samples ): sim_samples [ i ,:] = run_HH_model ( params = params [ i ,:])[ 'data' ] # colors for traces col_min = 2 num_colors = num_samples + col_min cm1 = mpl . cm . Blues col1 = [ cm1 ( 1. * i / num_colors ) for i in range ( col_min , num_colors )] fig = plt . figure ( figsize = ( 7 , 5 )) gs = mpl . gridspec . GridSpec ( 2 , 1 , height_ratios = [ 4 , 1 ]) ax = plt . subplot ( gs [ 0 ]) for i in range ( num_samples ): plt . plot ( t , sim_samples [ i ,:], color = col1 [ i ], lw = 2 ) plt . ylabel ( 'voltage (mV)' ) ax . set_xticks ([]) ax . set_yticks ([ - 80 , - 20 , 40 ]) ax = plt . subplot ( gs [ 1 ]) plt . plot ( t , I * A_soma * 1e3 , 'k' , lw = 2 ) plt . xlabel ( 'time (ms)' ) plt . ylabel ( 'input (nA)' ) ax . set_xticks ([ 0 , max ( t ) / 2 , max ( t )]) ax . set_yticks ([ 0 , 1.1 * np . max ( I * A_soma * 1e3 )]) ax . yaxis . set_major_formatter ( mpl . ticker . FormatStrFormatter ( ' %.2f ' )) plt . show () As can be seen, the voltage traces can be quite diverse for different parameter values. Often, we are not interested in matching the exact trace, but only in matching certain features thereof. In this example of the Hodgkin-Huxley model, the summary features are the number of spikes, the mean resting potential, the standard deviation of the resting potential, and the first four voltage moments: mean, standard deviation, skewness and kurtosis. Using the function calculate_summary_statistics() imported below, we obtain these statistics from the output of the Hodgkin Huxley simulator. from HH_helper_functions import calculate_summary_statistics Lastly, we define a function that performs all of the above steps at once. The function simulation_wrapper takes in conductance values, runs the Hodgkin Huxley model and then returns the summary statistics. def simulation_wrapper ( params ): \"\"\" Returns summary statistics from conductance values in `params`. Summarizes the output of the HH simulator and converts it to `torch.Tensor`. \"\"\" obs = run_HH_model ( params ) summstats = torch . as_tensor ( calculate_summary_statistics ( obs )) return summstats sbi takes any function as simulator. Thus, sbi also has the flexibility to use simulators that utilize external packages, e.g., Brian ( http://briansimulator.org/ ), nest ( https://www.nest-simulator.org/ ), or NEURON ( https://neuron.yale.edu/neuron/ ). External simulators do not even need to be Python-based as long as they store simulation outputs in a format that can be read from Python. All that is necessary is to wrap your external simulator of choice into a Python callable that takes a parameter set and outputs a set of summary statistics we want to fit the parameters to. 3. Prior over model parameters \u00b6 Now that we have the simulator, we need to define a function with the prior over the model parameters ( \\(\\bar g_{Na}\\) , \\(\\bar g_K\\) ), which in this case is chosen to be a Uniform distribution: prior_min = [ .5 , 1e-4 ] prior_max = [ 80. , 15. ] prior = utils . torchutils . BoxUniform ( low = torch . as_tensor ( prior_min ), high = torch . as_tensor ( prior_max )) Inference \u00b6 Now that we have all the required components, we can run inference with SNPE to identify parameters whose activity matches this trace. posterior = infer ( simulation_wrapper , prior , method = 'SNPE' , num_simulations = 300 , num_workers = 4 ) HBox(children=(FloatProgress(value=0.0, description='Running 300 simulations in 300 batches.', max=300.0, styl\u2026 Neural network successfully converged after 233 epochs. Note sbi can parallelize your simulator. If you experience problems with parallelization, try setting num_workers=1 and please give us an error report as a GitHub issue . Coming back to the observed data \u00b6 As mentioned at the beginning of the tutorial, the observed data are generated by the Hodgkin-Huxley model with a set of known parameters ( \\(\\bar g_{Na}\\) , \\(\\bar g_K\\) ). To illustrate how to compute the summary statistics of the observed data, let us regenerate the observed data: # true parameters and respective labels true_params = np . array ([ 50. , 5. ]) labels_params = [ r '$g_ {Na} $' , r '$g_ {K} $' ] observation_trace = run_HH_model ( true_params ) observation_summary_statistics = calculate_summary_statistics ( observation_trace ) As we already shown above, the observed voltage traces look as follows: fig = plt . figure ( figsize = ( 7 , 5 )) gs = mpl . gridspec . GridSpec ( 2 , 1 , height_ratios = [ 4 , 1 ]) ax = plt . subplot ( gs [ 0 ]) plt . plot ( observation_trace [ 'time' ], observation_trace [ 'data' ]) plt . ylabel ( 'voltage (mV)' ) plt . title ( 'observed data' ) plt . setp ( ax , xticks = [], yticks = [ - 80 , - 20 , 40 ]) ax = plt . subplot ( gs [ 1 ]) plt . plot ( observation_trace [ 'time' ], I * A_soma * 1e3 , 'k' , lw = 2 ) plt . xlabel ( 'time (ms)' ) plt . ylabel ( 'input (nA)' ) ax . set_xticks ([ 0 , max ( observation_trace [ 'time' ]) / 2 , max ( observation_trace [ 'time' ])]) ax . set_yticks ([ 0 , 1.1 * np . max ( I * A_soma * 1e3 )]) ax . yaxis . set_major_formatter ( mpl . ticker . FormatStrFormatter ( ' %.2f ' )) Analysis of the posterior given the observed data \u00b6 After running the inference algorithm, let us inspect the inferred posterior distribution over the parameters ( \\(\\bar g_{Na}\\) , \\(\\bar g_K\\) ), given the observed trace. To do so, we first draw samples (i.e. consistent parameter sets) from the posterior: samples = posterior . sample (( 10000 ,), x = observation_summary_statistics ) HBox(children=(FloatProgress(value=0.0, description='Drawing 10000 posterior samples', max=10000.0, style=Prog\u2026 fig , axes = analysis . pairplot ( samples , limits = [[ .5 , 80 ], [ 1e-4 , 15. ]], ticks = [[ .5 , 80 ], [ 1e-4 , 15. ]], figsize = ( 5 , 5 ), points = true_params , points_offdiag = { 'markersize' : 6 }, points_colors = 'r' ); As can be seen, the inferred posterior contains the ground-truth parameters (red) in a high-probability region. Now, let us sample parameters from the posterior distribution, simulate the Hodgkin-Huxley model for this parameter set and compare the simulations with the observed data: # Draw a sample from the posterior and convert to numpy for plotting. posterior_sample = posterior . sample (( 1 ,), x = observation_summary_statistics ) . numpy () HBox(children=(FloatProgress(value=0.0, description='Drawing 1 posterior samples', max=1.0, style=ProgressStyl\u2026 fig = plt . figure ( figsize = ( 7 , 5 )) # plot observation t = observation_trace [ 'time' ] y_obs = observation_trace [ 'data' ] plt . plot ( t , y_obs , lw = 2 , label = 'observation' ) # simulate and plot samples x = run_HH_model ( posterior_sample ) plt . plot ( t , x [ 'data' ], '--' , lw = 2 , label = 'posterior sample' ) plt . xlabel ( 'time (ms)' ) plt . ylabel ( 'voltage (mV)' ) ax = plt . gca () handles , labels = ax . get_legend_handles_labels () ax . legend ( handles [:: - 1 ], labels [:: - 1 ], bbox_to_anchor = ( 1.3 , 1 ), loc = 'upper right' ) ax . set_xticks ([ 0 , 60 , 120 ]) ax . set_yticks ([ - 80 , - 20 , 40 ]); As can be seen, the sample from the inferred posterior leads to simulations that closely resemble the observed data, confirming that SNPE did a good job at capturing the observed data in this simple case. References \u00b6 A. L. Hodgkin and A. F. Huxley. A quantitative description of membrane current and its application to conduction and excitation in nerve. The Journal of Physiology, 117(4):500\u2013544, 1952. M. Pospischil, M. Toledo-Rodriguez, C. Monier, Z. Piwkowska, T. Bal, Y. Fr\u00e9gnac, H. Markram, and A. Destexhe. Minimal Hodgkin-Huxley type models for different classes of cortical and thalamic neurons. Biological Cybernetics, 99(4-5), 2008.","title":"Hodgkin-Huxley example"},{"location":"examples/00_HH_simulator/#inference-on-hodgkin-huxley-model-tutorial","text":"In this tutorial, we use sbi to do inference on a Hodgkin-Huxley model from neuroscience (Hodgkin and Huxley, 1952). We will learn two parameters ( \\(\\bar g_{Na}\\) , \\(\\bar g_K\\) ) based on a current-clamp recording, that we generate synthetically (in practice, this would be an experimental observation). Note, you find the original version of this notebook at https://github.com/mackelab/sbi/blob/main/examples/00_HH_simulator.ipynb in the sbi repository. First we are going to import basic packages. import numpy as np import torch # visualization import matplotlib as mpl import matplotlib.pyplot as plt # sbi from sbi import utils as utils from sbi import analysis as analysis from sbi.inference.base import infer # remove top and right axis from plots mpl . rcParams [ 'axes.spines.right' ] = False mpl . rcParams [ 'axes.spines.top' ] = False","title":"Inference on Hodgkin-Huxley model: tutorial"},{"location":"examples/00_HH_simulator/#different-required-components","text":"Before running inference, let us define the different required components: observed data prior over model parameters simulator","title":"Different required components"},{"location":"examples/00_HH_simulator/#1-observed-data","text":"Let us assume we current-clamped a neuron and recorded the following voltage trace: In fact, this voltage trace was not measured experimentally but synthetically generated by simulating a Hodgkin-Huxley model with particular parameters ( \\(\\bar g_{Na}\\) , \\(\\bar g_K\\) ). We will come back to this point later in the tutorial.","title":"1. Observed data"},{"location":"examples/00_HH_simulator/#2-simulator","text":"We would like to infer the posterior over the two parameters ( \\(\\bar g_{Na}\\) , \\(\\bar g_K\\) ) of a Hodgkin-Huxley model, given the observed electrophysiological recording above. The model has channel kinetics as in Pospischil et al. 2008 , and is defined by the following set of differential equations (parameters of interest highlighted in orange): \\[ \\scriptsize \\begin{align} C_m\\frac{dV}{dt}&=g_1\\left(E_1-V\\right)+ \\color{orange}{\\bar{g}_{Na}}m^3h\\left(E_{Na}-V\\right)+ \\color{orange}{\\bar{g}_{K}}n^4\\left(E_K-V\\right)+ \\bar{g}_Mp\\left(E_K-V\\right)+ I_{inj}+ \\sigma\\eta\\left(t\\right)\\\\ \\frac{dq}{dt}&=\\frac{q_\\infty\\left(V\\right)-q}{\\tau_q\\left(V\\right)},\\;q\\in\\{m,h,n,p\\} \\end{align} \\] Above, \\(V\\) represents the membrane potential, \\(C_m\\) is the membrane capacitance, \\(g_{\\text{l}}\\) is the leak conductance, \\(E_{\\text{l}}\\) is the membrane reversal potential, \\(\\bar{g}_c\\) is the density of channels of type \\(c\\) ( \\(\\text{Na}^+\\) , \\(\\text{K}^+\\) , M), \\(E_c\\) is the reversal potential of \\(c\\) , ( \\(m\\) , \\(h\\) , \\(n\\) , \\(p\\) ) are the respective channel gating kinetic variables, and \\(\\sigma \\eta(t)\\) is the intrinsic neural noise. The right hand side of the voltage dynamics is composed of a leak current, a voltage-dependent \\(\\text{Na}^+\\) current, a delayed-rectifier \\(\\text{K}^+\\) current, a slow voltage-dependent \\(\\text{K}^+\\) current responsible for spike-frequency adaptation, and an injected current \\(I_{\\text{inj}}\\) . Channel gating variables \\(q\\) have dynamics fully characterized by the neuron membrane potential \\(V\\) , given the respective steady-state \\(q_{\\infty}(V)\\) and time constant \\(\\tau_{q}(V)\\) (details in Pospischil et al. 2008). The input current \\(I_{\\text{inj}}\\) is defined as from HH_helper_functions import syn_current I , t_on , t_off , dt , t , A_soma = syn_current () The Hodgkin-Huxley simulator is given by: from HH_helper_functions import HHsimulator Putting the input current and the simulator together: def run_HH_model ( params ): params = np . asarray ( params ) # input current, time step I , t_on , t_off , dt , t , A_soma = syn_current () t = np . arange ( 0 , len ( I ), 1 ) * dt # initial voltage V0 = - 70 states = HHsimulator ( V0 , params . reshape ( 1 , - 1 ), dt , t , I ) return dict ( data = states . reshape ( - 1 ), time = t , dt = dt , I = I . reshape ( - 1 )) To get an idea of the output of the Hodgkin-Huxley model, let us generate some voltage traces for different parameters ( \\(\\bar g_{Na}\\) , \\(\\bar g_K\\) ), given the input current \\(I_{\\text{inj}}\\) : # three sets of (g_Na, g_K) params = np . array ([[ 50. , 1. ],[ 4. , 1.5 ],[ 20. , 15. ]]) num_samples = len ( params [:, 0 ]) sim_samples = np . zeros (( num_samples , len ( I ))) for i in range ( num_samples ): sim_samples [ i ,:] = run_HH_model ( params = params [ i ,:])[ 'data' ] # colors for traces col_min = 2 num_colors = num_samples + col_min cm1 = mpl . cm . Blues col1 = [ cm1 ( 1. * i / num_colors ) for i in range ( col_min , num_colors )] fig = plt . figure ( figsize = ( 7 , 5 )) gs = mpl . gridspec . GridSpec ( 2 , 1 , height_ratios = [ 4 , 1 ]) ax = plt . subplot ( gs [ 0 ]) for i in range ( num_samples ): plt . plot ( t , sim_samples [ i ,:], color = col1 [ i ], lw = 2 ) plt . ylabel ( 'voltage (mV)' ) ax . set_xticks ([]) ax . set_yticks ([ - 80 , - 20 , 40 ]) ax = plt . subplot ( gs [ 1 ]) plt . plot ( t , I * A_soma * 1e3 , 'k' , lw = 2 ) plt . xlabel ( 'time (ms)' ) plt . ylabel ( 'input (nA)' ) ax . set_xticks ([ 0 , max ( t ) / 2 , max ( t )]) ax . set_yticks ([ 0 , 1.1 * np . max ( I * A_soma * 1e3 )]) ax . yaxis . set_major_formatter ( mpl . ticker . FormatStrFormatter ( ' %.2f ' )) plt . show () As can be seen, the voltage traces can be quite diverse for different parameter values. Often, we are not interested in matching the exact trace, but only in matching certain features thereof. In this example of the Hodgkin-Huxley model, the summary features are the number of spikes, the mean resting potential, the standard deviation of the resting potential, and the first four voltage moments: mean, standard deviation, skewness and kurtosis. Using the function calculate_summary_statistics() imported below, we obtain these statistics from the output of the Hodgkin Huxley simulator. from HH_helper_functions import calculate_summary_statistics Lastly, we define a function that performs all of the above steps at once. The function simulation_wrapper takes in conductance values, runs the Hodgkin Huxley model and then returns the summary statistics. def simulation_wrapper ( params ): \"\"\" Returns summary statistics from conductance values in `params`. Summarizes the output of the HH simulator and converts it to `torch.Tensor`. \"\"\" obs = run_HH_model ( params ) summstats = torch . as_tensor ( calculate_summary_statistics ( obs )) return summstats sbi takes any function as simulator. Thus, sbi also has the flexibility to use simulators that utilize external packages, e.g., Brian ( http://briansimulator.org/ ), nest ( https://www.nest-simulator.org/ ), or NEURON ( https://neuron.yale.edu/neuron/ ). External simulators do not even need to be Python-based as long as they store simulation outputs in a format that can be read from Python. All that is necessary is to wrap your external simulator of choice into a Python callable that takes a parameter set and outputs a set of summary statistics we want to fit the parameters to.","title":"2. Simulator"},{"location":"examples/00_HH_simulator/#3-prior-over-model-parameters","text":"Now that we have the simulator, we need to define a function with the prior over the model parameters ( \\(\\bar g_{Na}\\) , \\(\\bar g_K\\) ), which in this case is chosen to be a Uniform distribution: prior_min = [ .5 , 1e-4 ] prior_max = [ 80. , 15. ] prior = utils . torchutils . BoxUniform ( low = torch . as_tensor ( prior_min ), high = torch . as_tensor ( prior_max ))","title":"3. Prior over model parameters"},{"location":"examples/00_HH_simulator/#inference","text":"Now that we have all the required components, we can run inference with SNPE to identify parameters whose activity matches this trace. posterior = infer ( simulation_wrapper , prior , method = 'SNPE' , num_simulations = 300 , num_workers = 4 ) HBox(children=(FloatProgress(value=0.0, description='Running 300 simulations in 300 batches.', max=300.0, styl\u2026 Neural network successfully converged after 233 epochs. Note sbi can parallelize your simulator. If you experience problems with parallelization, try setting num_workers=1 and please give us an error report as a GitHub issue .","title":"Inference"},{"location":"examples/00_HH_simulator/#coming-back-to-the-observed-data","text":"As mentioned at the beginning of the tutorial, the observed data are generated by the Hodgkin-Huxley model with a set of known parameters ( \\(\\bar g_{Na}\\) , \\(\\bar g_K\\) ). To illustrate how to compute the summary statistics of the observed data, let us regenerate the observed data: # true parameters and respective labels true_params = np . array ([ 50. , 5. ]) labels_params = [ r '$g_ {Na} $' , r '$g_ {K} $' ] observation_trace = run_HH_model ( true_params ) observation_summary_statistics = calculate_summary_statistics ( observation_trace ) As we already shown above, the observed voltage traces look as follows: fig = plt . figure ( figsize = ( 7 , 5 )) gs = mpl . gridspec . GridSpec ( 2 , 1 , height_ratios = [ 4 , 1 ]) ax = plt . subplot ( gs [ 0 ]) plt . plot ( observation_trace [ 'time' ], observation_trace [ 'data' ]) plt . ylabel ( 'voltage (mV)' ) plt . title ( 'observed data' ) plt . setp ( ax , xticks = [], yticks = [ - 80 , - 20 , 40 ]) ax = plt . subplot ( gs [ 1 ]) plt . plot ( observation_trace [ 'time' ], I * A_soma * 1e3 , 'k' , lw = 2 ) plt . xlabel ( 'time (ms)' ) plt . ylabel ( 'input (nA)' ) ax . set_xticks ([ 0 , max ( observation_trace [ 'time' ]) / 2 , max ( observation_trace [ 'time' ])]) ax . set_yticks ([ 0 , 1.1 * np . max ( I * A_soma * 1e3 )]) ax . yaxis . set_major_formatter ( mpl . ticker . FormatStrFormatter ( ' %.2f ' ))","title":"Coming back to the observed data"},{"location":"examples/00_HH_simulator/#analysis-of-the-posterior-given-the-observed-data","text":"After running the inference algorithm, let us inspect the inferred posterior distribution over the parameters ( \\(\\bar g_{Na}\\) , \\(\\bar g_K\\) ), given the observed trace. To do so, we first draw samples (i.e. consistent parameter sets) from the posterior: samples = posterior . sample (( 10000 ,), x = observation_summary_statistics ) HBox(children=(FloatProgress(value=0.0, description='Drawing 10000 posterior samples', max=10000.0, style=Prog\u2026 fig , axes = analysis . pairplot ( samples , limits = [[ .5 , 80 ], [ 1e-4 , 15. ]], ticks = [[ .5 , 80 ], [ 1e-4 , 15. ]], figsize = ( 5 , 5 ), points = true_params , points_offdiag = { 'markersize' : 6 }, points_colors = 'r' ); As can be seen, the inferred posterior contains the ground-truth parameters (red) in a high-probability region. Now, let us sample parameters from the posterior distribution, simulate the Hodgkin-Huxley model for this parameter set and compare the simulations with the observed data: # Draw a sample from the posterior and convert to numpy for plotting. posterior_sample = posterior . sample (( 1 ,), x = observation_summary_statistics ) . numpy () HBox(children=(FloatProgress(value=0.0, description='Drawing 1 posterior samples', max=1.0, style=ProgressStyl\u2026 fig = plt . figure ( figsize = ( 7 , 5 )) # plot observation t = observation_trace [ 'time' ] y_obs = observation_trace [ 'data' ] plt . plot ( t , y_obs , lw = 2 , label = 'observation' ) # simulate and plot samples x = run_HH_model ( posterior_sample ) plt . plot ( t , x [ 'data' ], '--' , lw = 2 , label = 'posterior sample' ) plt . xlabel ( 'time (ms)' ) plt . ylabel ( 'voltage (mV)' ) ax = plt . gca () handles , labels = ax . get_legend_handles_labels () ax . legend ( handles [:: - 1 ], labels [:: - 1 ], bbox_to_anchor = ( 1.3 , 1 ), loc = 'upper right' ) ax . set_xticks ([ 0 , 60 , 120 ]) ax . set_yticks ([ - 80 , - 20 , 40 ]); As can be seen, the sample from the inferred posterior leads to simulations that closely resemble the observed data, confirming that SNPE did a good job at capturing the observed data in this simple case.","title":"Analysis of the posterior given the observed data"},{"location":"examples/00_HH_simulator/#references","text":"A. L. Hodgkin and A. F. Huxley. A quantitative description of membrane current and its application to conduction and excitation in nerve. The Journal of Physiology, 117(4):500\u2013544, 1952. M. Pospischil, M. Toledo-Rodriguez, C. Monier, Z. Piwkowska, T. Bal, Y. Fr\u00e9gnac, H. Markram, and A. Destexhe. Minimal Hodgkin-Huxley type models for different classes of cortical and thalamic neurons. Biological Cybernetics, 99(4-5), 2008.","title":"References"},{"location":"faq/question_01/","text":"What should I do when my \u2018posterior samples are outside of the prior support\u2019 in SNPE? \u00b6 When working with multi-round SNPE, you might have experienced the following warning: Only x% posterior samples are within the prior support. It may take a long time to collect the remaining 10000 samples. Consider interrupting (Ctrl-C) and switching to 'sample_with_mcmc=True'. This reason for this issue is described in more detail here and here . The following fixes are possible: sample with MCMC: samples = posterior((num_samples,), x=x_o, sample_with_mcmc=True) . This will make sampling slower, but samples will not \u2018leak\u2019. resort to single-round SNPE and (if necessary) increase your simulation budget. if your prior is either Gaussian (torch.distributions.multivariateNormal) or Uniform (sbi.utils.BoxUniform), you can avoid leakage by using a mixture density network as density estimator. I.e., using the flexible interface , set density_estimator='mdn' . When running inference, there should be a print statement \u201cUsing SNPE-C with non-atomic loss\u201d use a different algorithm, e.g. SNRE and SNLE. Note, however, that these algorithms can have different issues and potential pitfalls.","title":"What should I do when my 'posterior samples are outside of the prior support' in SNPE?"},{"location":"faq/question_01/#what-should-i-do-when-my-posterior-samples-are-outside-of-the-prior-support-in-snpe","text":"When working with multi-round SNPE, you might have experienced the following warning: Only x% posterior samples are within the prior support. It may take a long time to collect the remaining 10000 samples. Consider interrupting (Ctrl-C) and switching to 'sample_with_mcmc=True'. This reason for this issue is described in more detail here and here . The following fixes are possible: sample with MCMC: samples = posterior((num_samples,), x=x_o, sample_with_mcmc=True) . This will make sampling slower, but samples will not \u2018leak\u2019. resort to single-round SNPE and (if necessary) increase your simulation budget. if your prior is either Gaussian (torch.distributions.multivariateNormal) or Uniform (sbi.utils.BoxUniform), you can avoid leakage by using a mixture density network as density estimator. I.e., using the flexible interface , set density_estimator='mdn' . When running inference, there should be a print statement \u201cUsing SNPE-C with non-atomic loss\u201d use a different algorithm, e.g. SNRE and SNLE. Note, however, that these algorithms can have different issues and potential pitfalls.","title":"What should I do when my 'posterior samples are outside of the prior support' in SNPE?"},{"location":"faq/question_02/","text":"Can the algorithms deal with invalid data, e.g. NaN or inf? \u00b6 Yes. By default, whenever a simulation returns at least one NaN or inf , it is completely excluded from the training data. In other words, the simulation is simply discarded. In cases where a very large fraction of simulations return NaN or inf , discarding many simulations can be wasteful. There are two options to deal with this: Either, you use the RestrictionEstimator to learn regions in parameter space that do not produce NaN or inf , see here . Alternatively, you can manually substitute the \u2018invalid\u2019 values with a reasonable replacement. I.e., at the end of your simulation code, you search for invalid entries and replace them with a floating point number. Importantly, in order for neural network training work well, the floating point number should still be in a reasonable range, i.e. maybe a few standard deviations outside of \u2018good\u2019 values. If you are running multi-round SNPE, however, things can go fully wrong if invalid data are encountered. In that case, you will get the following warning When invalid simulations are excluded, multi-round SNPE-C can leak into the regions where parameters led to invalid simulations. This can lead to poor results. Hence, if you are running multi-round SNPE and a significant fraction of simulations returns at least one invalid number, we strongly recommend to manually replace the value in your simulation code as described above (or resort to single-round SNPE or use a different method).","title":"Can the algorithms deal with invalid data, e.g. NaN or inf?"},{"location":"faq/question_02/#can-the-algorithms-deal-with-invalid-data-eg-nan-or-inf","text":"Yes. By default, whenever a simulation returns at least one NaN or inf , it is completely excluded from the training data. In other words, the simulation is simply discarded. In cases where a very large fraction of simulations return NaN or inf , discarding many simulations can be wasteful. There are two options to deal with this: Either, you use the RestrictionEstimator to learn regions in parameter space that do not produce NaN or inf , see here . Alternatively, you can manually substitute the \u2018invalid\u2019 values with a reasonable replacement. I.e., at the end of your simulation code, you search for invalid entries and replace them with a floating point number. Importantly, in order for neural network training work well, the floating point number should still be in a reasonable range, i.e. maybe a few standard deviations outside of \u2018good\u2019 values. If you are running multi-round SNPE, however, things can go fully wrong if invalid data are encountered. In that case, you will get the following warning When invalid simulations are excluded, multi-round SNPE-C can leak into the regions where parameters led to invalid simulations. This can lead to poor results. Hence, if you are running multi-round SNPE and a significant fraction of simulations returns at least one invalid number, we strongly recommend to manually replace the value in your simulation code as described above (or resort to single-round SNPE or use a different method).","title":"Can the algorithms deal with invalid data, e.g. NaN or inf?"},{"location":"faq/question_03/","text":"When using multiple workers, I get a pickling error. Can I still use multiprocessing? \u00b6 Yes, but you will have to make a few adjustments to your code. Some background: when using num_workers > 1 , you might experience an error that a certain object from your simulator could not be pickled (an example can be found here ). This can be fixed by forcing sbi to pickle with dill instead of the default cloudpickle . To do so, adjust your code as follows: Install dill : pip install dill At the very beginning of your python script, set the pickler to dill : from joblib.externals.loky import set_loky_pickler set_loky_pickler ( \"dill\" ) Move all imports required by your simulator into the simulator: # Imports specified outside of the simulator will break dill: import torch def my_simulator ( parameters ): return torch . ones ( 1 , 10 ) # Therefore, move the imports into the simulator: def my_simulator ( parameters ): import torch return torch . ones ( 1 , 10 ) Alternative: parallelize yourself \u00b6 You can also write your own code to parallelize simulations with whatever multiprocessing framework you prefer. You can then simulate your data outside of sbi and pass the simulated data as shown in the flexible interface : Some more background \u00b6 sbi uses joblib to parallelize simulations, which in turn uses pickle or cloudpickle to serialize the simulator. Almost all simulators will be picklable with cloudpickle , but we have experienced issues e.g. with neuron simulators, see here .","title":"When using multiple workers, I get a pickling error. Can I still use multiprocessing?"},{"location":"faq/question_03/#when-using-multiple-workers-i-get-a-pickling-error-can-i-still-use-multiprocessing","text":"Yes, but you will have to make a few adjustments to your code. Some background: when using num_workers > 1 , you might experience an error that a certain object from your simulator could not be pickled (an example can be found here ). This can be fixed by forcing sbi to pickle with dill instead of the default cloudpickle . To do so, adjust your code as follows: Install dill : pip install dill At the very beginning of your python script, set the pickler to dill : from joblib.externals.loky import set_loky_pickler set_loky_pickler ( \"dill\" ) Move all imports required by your simulator into the simulator: # Imports specified outside of the simulator will break dill: import torch def my_simulator ( parameters ): return torch . ones ( 1 , 10 ) # Therefore, move the imports into the simulator: def my_simulator ( parameters ): import torch return torch . ones ( 1 , 10 )","title":"When using multiple workers, I get a pickling error. Can I still use multiprocessing?"},{"location":"faq/question_03/#alternative-parallelize-yourself","text":"You can also write your own code to parallelize simulations with whatever multiprocessing framework you prefer. You can then simulate your data outside of sbi and pass the simulated data as shown in the flexible interface :","title":"Alternative: parallelize yourself"},{"location":"faq/question_03/#some-more-background","text":"sbi uses joblib to parallelize simulations, which in turn uses pickle or cloudpickle to serialize the simulator. Almost all simulators will be picklable with cloudpickle , but we have experienced issues e.g. with neuron simulators, see here .","title":"Some more background"},{"location":"faq/question_04/","text":"Can I use the GPU for training the density estimator? \u00b6 TLDR; Yes, by passing device=\"cuda\" and by passing a prior that lives on the device name your passed. But no speed-ups for default density estimators. Yes. When creating the inference object in the flexible interface, you can pass the device as an argument, e.g., inference = SNPE ( prior , device = \"cuda\" , density_estimator = \"maf\" ) The device is set to \"cpu\" by default, and it can be set to anything, as long as it maps to an existing PyTorch CUDA device. sbi will take care of copying the net and the training data to and from the device . Note that the prior must be on the training device already, e.g., when passing device=\"cuda:0\" , make sure to pass a prior object that was created on that device, e.g., prior = torch.distributions.MultivariateNormal(loc=torch.zeros(2, device=\"cuda:0\"), covariance_matrix=torch.eye(2, device=\"cuda:0\")) . Performance \u00b6 Whether or not you reduce your training time when training on a GPU depends on the problem at hand. We provide a couple of default density estimators for SNPE , SNLE and SNRE , e.g., a mixture density network ( density_estimator=\"mdn\" ) or a Masked Autoregressive Flow ( density_estimator=\"maf\" ). For those default density estimators we do not expect a speed up. This is because the underlying neural networks are quite shallow and not tall, e.g., they do not have many parameters or matrix operations that profit a lot from being executed on the GPU. A speed up through training on the GPU will most likely become visible when you are using convolutional modules in your neural networks. E.g., when passing an embedding net for image processing like in this example: https://github.com/mackelab/sbi/blob/main/tutorials/05_embedding_net.ipynb .","title":"Can I use the GPU for training the density estimator?"},{"location":"faq/question_04/#can-i-use-the-gpu-for-training-the-density-estimator","text":"TLDR; Yes, by passing device=\"cuda\" and by passing a prior that lives on the device name your passed. But no speed-ups for default density estimators. Yes. When creating the inference object in the flexible interface, you can pass the device as an argument, e.g., inference = SNPE ( prior , device = \"cuda\" , density_estimator = \"maf\" ) The device is set to \"cpu\" by default, and it can be set to anything, as long as it maps to an existing PyTorch CUDA device. sbi will take care of copying the net and the training data to and from the device . Note that the prior must be on the training device already, e.g., when passing device=\"cuda:0\" , make sure to pass a prior object that was created on that device, e.g., prior = torch.distributions.MultivariateNormal(loc=torch.zeros(2, device=\"cuda:0\"), covariance_matrix=torch.eye(2, device=\"cuda:0\")) .","title":"Can I use the GPU for training the density estimator?"},{"location":"faq/question_04/#performance","text":"Whether or not you reduce your training time when training on a GPU depends on the problem at hand. We provide a couple of default density estimators for SNPE , SNLE and SNRE , e.g., a mixture density network ( density_estimator=\"mdn\" ) or a Masked Autoregressive Flow ( density_estimator=\"maf\" ). For those default density estimators we do not expect a speed up. This is because the underlying neural networks are quite shallow and not tall, e.g., they do not have many parameters or matrix operations that profit a lot from being executed on the GPU. A speed up through training on the GPU will most likely become visible when you are using convolutional modules in your neural networks. E.g., when passing an embedding net for image processing like in this example: https://github.com/mackelab/sbi/blob/main/tutorials/05_embedding_net.ipynb .","title":"Performance"},{"location":"faq/question_05/","text":"How should I save and load objects in sbi ? \u00b6 NeuralPosterior objects are picklable. import pickle # ... run inference posterior = inference . build_posterior () with open ( \"/path/to/my_posterior.pkl\" , \"wb\" ) as handle : pickle . dump ( posterior , handle ) Note: if you try to load a posterior that was saved under sbi v0.14.x or earlier under sbi v0.15.0 or later, you have to add: import sys from sbi.utils import user_input_checks_utils sys . modules [ \"sbi.user_input.user_input_checks_utils\" ] = user_input_checks_utils to your script before loading the posterior. NeuralInference objects are not picklable. There are two workarounds: Pickle with dill (has to be installed with pip install dill first) import dill inference = SNPE ( prior ) # ... run inference with open ( \"path/to/my_inference.pkl\" , \"wb\" ) as handle : dill . dump ( inference ) Delete un-picklable attributes and serialize with pickle. Using this option, you will not be able to use the retrain_from_scratch feature and you can only use the default SummaryWriter . import pickle inference = SNPE ( prior ) # ... run inference posterior = inference . build_posterior () inference . _summary_writer = None inference . _build_neural_net = None with open ( \"/path/to/my_inference.pkl\" , \"wb\" ) as handle : pickle . dump ( inference , handle ) Then, to load: with open ( \"/path/to/my_inference.pkl\" , \"rb\" ) as handle : inference_from_disk = pickle . load ( handle ) inference_from_disk . _summary_writer = inference_from_disk . _default_summary_writer ()","title":"How should I save and load objects in `sbi`?"},{"location":"faq/question_05/#how-should-i-save-and-load-objects-in-sbi","text":"NeuralPosterior objects are picklable. import pickle # ... run inference posterior = inference . build_posterior () with open ( \"/path/to/my_posterior.pkl\" , \"wb\" ) as handle : pickle . dump ( posterior , handle ) Note: if you try to load a posterior that was saved under sbi v0.14.x or earlier under sbi v0.15.0 or later, you have to add: import sys from sbi.utils import user_input_checks_utils sys . modules [ \"sbi.user_input.user_input_checks_utils\" ] = user_input_checks_utils to your script before loading the posterior. NeuralInference objects are not picklable. There are two workarounds: Pickle with dill (has to be installed with pip install dill first) import dill inference = SNPE ( prior ) # ... run inference with open ( \"path/to/my_inference.pkl\" , \"wb\" ) as handle : dill . dump ( inference ) Delete un-picklable attributes and serialize with pickle. Using this option, you will not be able to use the retrain_from_scratch feature and you can only use the default SummaryWriter . import pickle inference = SNPE ( prior ) # ... run inference posterior = inference . build_posterior () inference . _summary_writer = None inference . _build_neural_net = None with open ( \"/path/to/my_inference.pkl\" , \"wb\" ) as handle : pickle . dump ( inference , handle ) Then, to load: with open ( \"/path/to/my_inference.pkl\" , \"rb\" ) as handle : inference_from_disk = pickle . load ( handle ) inference_from_disk . _summary_writer = inference_from_disk . _default_summary_writer ()","title":"How should I save and load objects in sbi?"},{"location":"faq/question_06/","text":"Can I stop neural network training and resume it later? \u00b6 Many clusters have a time limit and sbi might exceed this limit. You can circumvent this problem by using the flexible interface . After simulations are finished, sbi trains a neural network. If this process takes too long, you can stop training and resume it later. The syntax is: inference = SNPE ( prior = prior ) inference = inference . append_simulations ( theta , x ) inference . train ( max_num_epochs = 300 ) # Pick `max_num_epochs` such that it does not exceed the runtime. with open ( \"path/to/my/inference.pkl\" , \"wb\" ) as handle : dill . dump ( inference , handle ) # To resume training: with open ( \"path/to/my/inference.pkl\" , \"rb\" ) as handle : inference_from_disk = dill . load ( handle ) inference_from_disk . train ( resume_training = True , max_num_epochs = 600 ) # Run epochs 301 until 600 (or stop early). posterior = inference_from_disk . build_posterior () Note that the inference object can not be saved with pickle . To save it, you will have to install and use dill . Another solution is described here .","title":"Can I stop neural network training and resume it later?"},{"location":"faq/question_06/#can-i-stop-neural-network-training-and-resume-it-later","text":"Many clusters have a time limit and sbi might exceed this limit. You can circumvent this problem by using the flexible interface . After simulations are finished, sbi trains a neural network. If this process takes too long, you can stop training and resume it later. The syntax is: inference = SNPE ( prior = prior ) inference = inference . append_simulations ( theta , x ) inference . train ( max_num_epochs = 300 ) # Pick `max_num_epochs` such that it does not exceed the runtime. with open ( \"path/to/my/inference.pkl\" , \"wb\" ) as handle : dill . dump ( inference , handle ) # To resume training: with open ( \"path/to/my/inference.pkl\" , \"rb\" ) as handle : inference_from_disk = dill . load ( handle ) inference_from_disk . train ( resume_training = True , max_num_epochs = 600 ) # Run epochs 301 until 600 (or stop early). posterior = inference_from_disk . build_posterior () Note that the inference object can not be saved with pickle . To save it, you will have to install and use dill . Another solution is described here .","title":"Can I stop neural network training and resume it later?"},{"location":"tutorial/00_getting_started/","text":"Getting started with sbi \u00b6 Note, you find the original version of this notebook at https://github.com/mackelab/sbi/blob/main/tutorials/00_getting_started.ipynb in the sbi repository. import torch from sbi import utils as utils from sbi import analysis as analysis from sbi.inference.base import infer Running the inference procedure \u00b6 sbi provides a simple interface to run state-of-the-art algorithms for simulation-based inference. For inference, you need to provide two ingredients: 1) a prior distribution that allows to sample parameter sets. 2) a simulator that takes parameter sets and produces simulation outputs. For example, we can have a 3-dimensional parameter space with a uniform prior between [-1,1] and a simple simulator that for the sake of example adds 1.0 and some Gaussian noise to the parameter set: num_dim = 3 prior = utils . BoxUniform ( low =- 2 * torch . ones ( num_dim ), high = 2 * torch . ones ( num_dim )) def simulator ( parameter_set ): return 1.0 + parameter_set + torch . randn ( parameter_set . shape ) * 0.1 sbi can then run inference: posterior = infer ( simulator , prior , method = 'SNPE' , num_simulations = 1000 ) HBox(children=(FloatProgress(value=0.0, description='Running 1000 simulations.', max=1000.0, style=ProgressSty\u2026 Neural network successfully converged after 175 epochs. Let\u2019s say we have made some observation \\(x\\) : observation = torch . zeros ( 3 ) Given this observation, we can then sample from the posterior \\(p(\\theta|x)\\) , evaluate its log-probability, or plot it. samples = posterior . sample (( 10000 ,), x = observation ) log_probability = posterior . log_prob ( samples , x = observation ) _ = analysis . pairplot ( samples , limits = [[ - 2 , 2 ],[ - 2 , 2 ],[ - 2 , 2 ]], figsize = ( 6 , 6 )) HBox(children=(FloatProgress(value=0.0, description='Drawing 10000 posterior samples', max=10000.0, style=Prog\u2026 Requirements for the simulator, prior, and observation \u00b6 Regardless of the algorithm you need to provide a prior and a simulator for training. Let\u2019s talk about what requirements they need to satisfy. Prior \u00b6 A prior is a distribution object that allows to sample parameter sets. Any class for the prior is allowed as long as it allows to call prior.sample() and prior.log_prob() . Simulator \u00b6 The simulator is a Python callable that takes in a parameter set and outputs data with some (even if very small) stochasticity. Allowed data types and shapes for input and output: the input parameter set and the output have to be either a np.ndarray or a torch.Tensor . the input parameter set should have either shape (1,N) or (N) , and the output must have shape (1,M) or (M) . You can call simulators not written in Python as long as you wrap them in a Python function. Observation \u00b6 Once you have a trained posterior, you will want to evaluate or sample the posterior \\(p(\\theta|x_o)\\) at certain observed values \\(x_o\\) : The allowable data types are either Numpy np.ndarray or a torch torch.Tensor . The shape must be either (1,M) or just (M) . Running different algorithms \u00b6 sbi implements three classes of algorithms that can be used to obtain the posterior distribution: SNPE, SNLE, and SNRE. You can try the different algorithms by simply swapping out the method : posterior = infer ( simulator , prior , method = 'SNPE' , num_simulations = 1000 ) posterior = infer ( simulator , prior , method = 'SNLE' , num_simulations = 1000 ) posterior = infer ( simulator , prior , method = 'SNRE' , num_simulations = 1000 ) You can then infer, sample, evaluate, and plot the posterior as described above.","title":"Getting started"},{"location":"tutorial/00_getting_started/#getting-started-with-sbi","text":"Note, you find the original version of this notebook at https://github.com/mackelab/sbi/blob/main/tutorials/00_getting_started.ipynb in the sbi repository. import torch from sbi import utils as utils from sbi import analysis as analysis from sbi.inference.base import infer","title":"Getting started with sbi"},{"location":"tutorial/00_getting_started/#running-the-inference-procedure","text":"sbi provides a simple interface to run state-of-the-art algorithms for simulation-based inference. For inference, you need to provide two ingredients: 1) a prior distribution that allows to sample parameter sets. 2) a simulator that takes parameter sets and produces simulation outputs. For example, we can have a 3-dimensional parameter space with a uniform prior between [-1,1] and a simple simulator that for the sake of example adds 1.0 and some Gaussian noise to the parameter set: num_dim = 3 prior = utils . BoxUniform ( low =- 2 * torch . ones ( num_dim ), high = 2 * torch . ones ( num_dim )) def simulator ( parameter_set ): return 1.0 + parameter_set + torch . randn ( parameter_set . shape ) * 0.1 sbi can then run inference: posterior = infer ( simulator , prior , method = 'SNPE' , num_simulations = 1000 ) HBox(children=(FloatProgress(value=0.0, description='Running 1000 simulations.', max=1000.0, style=ProgressSty\u2026 Neural network successfully converged after 175 epochs. Let\u2019s say we have made some observation \\(x\\) : observation = torch . zeros ( 3 ) Given this observation, we can then sample from the posterior \\(p(\\theta|x)\\) , evaluate its log-probability, or plot it. samples = posterior . sample (( 10000 ,), x = observation ) log_probability = posterior . log_prob ( samples , x = observation ) _ = analysis . pairplot ( samples , limits = [[ - 2 , 2 ],[ - 2 , 2 ],[ - 2 , 2 ]], figsize = ( 6 , 6 )) HBox(children=(FloatProgress(value=0.0, description='Drawing 10000 posterior samples', max=10000.0, style=Prog\u2026","title":"Running the inference procedure"},{"location":"tutorial/00_getting_started/#requirements-for-the-simulator-prior-and-observation","text":"Regardless of the algorithm you need to provide a prior and a simulator for training. Let\u2019s talk about what requirements they need to satisfy.","title":"Requirements for the simulator, prior, and observation"},{"location":"tutorial/00_getting_started/#prior","text":"A prior is a distribution object that allows to sample parameter sets. Any class for the prior is allowed as long as it allows to call prior.sample() and prior.log_prob() .","title":"Prior"},{"location":"tutorial/00_getting_started/#simulator","text":"The simulator is a Python callable that takes in a parameter set and outputs data with some (even if very small) stochasticity. Allowed data types and shapes for input and output: the input parameter set and the output have to be either a np.ndarray or a torch.Tensor . the input parameter set should have either shape (1,N) or (N) , and the output must have shape (1,M) or (M) . You can call simulators not written in Python as long as you wrap them in a Python function.","title":"Simulator"},{"location":"tutorial/00_getting_started/#observation","text":"Once you have a trained posterior, you will want to evaluate or sample the posterior \\(p(\\theta|x_o)\\) at certain observed values \\(x_o\\) : The allowable data types are either Numpy np.ndarray or a torch torch.Tensor . The shape must be either (1,M) or just (M) .","title":"Observation"},{"location":"tutorial/00_getting_started/#running-different-algorithms","text":"sbi implements three classes of algorithms that can be used to obtain the posterior distribution: SNPE, SNLE, and SNRE. You can try the different algorithms by simply swapping out the method : posterior = infer ( simulator , prior , method = 'SNPE' , num_simulations = 1000 ) posterior = infer ( simulator , prior , method = 'SNLE' , num_simulations = 1000 ) posterior = infer ( simulator , prior , method = 'SNRE' , num_simulations = 1000 ) You can then infer, sample, evaluate, and plot the posterior as described above.","title":"Running different algorithms"},{"location":"tutorial/01_gaussian_amortized/","text":"Amortized posterior inference on Gaussian example \u00b6 Note, you find the original version of this notebook at https://github.com/mackelab/sbi/blob/main/tutorials/01_gaussian_amortized.ipynb in the sbi repository. In this tutorial, we will demonstrate how sbi can infer an amortized posterior for a simple toy model with a uniform prior and Gaussian likelihood. import torch import numpy as np from sbi import utils as utils from sbi import analysis as analysis from sbi.inference.base import infer Defining prior, simulator, and running inference \u00b6 Say we have 3-dimensional parameter space, and the prior is uniformly distributed between -2 and 2 in each dimension, i.e. \\(x\\in [-2,2], y\\in [-2,2], z \\in [-2,2]\\) . num_dim = 3 prior = utils . BoxUniform ( low =- 2 * torch . ones ( num_dim ), high = 2 * torch . ones ( num_dim )) Our simulator takes the input parameters, adds 1.0 in each dimension, and then adds some Gaussian noise: def linear_gaussian ( theta ): return theta + 1.0 + torch . randn_like ( theta ) * 0.1 We can then run inference: posterior = infer ( linear_gaussian , prior , 'SNPE' , num_simulations = 1000 ) HBox(children=(FloatProgress(value=0.0, description='Running 1000 simulations.', max=1000.0, style=ProgressSty\u2026 Neural network successfully converged after 150 epochs. Amortized inference \u00b6 Note that we have not yet provided an observation to the inference procedure. In fact, we can evaluate the posterior for different observations without having to re-run inference. This is called amortization. An amortized posterior is one that is not focused on any particular observation. Naturally, if the diversity of observations is large, any of the inference methods will need to run a sufficient number of simulations for the resulting posterior to perform well across these diverse observations. Let\u2019s say we have two observations x_o_1 = [0,0,0] and x_o_2 = [2,2,2] : x_o_1 = torch . zeros ( 3 ,) x_o_2 = 2.0 * torch . ones ( 3 ,) We can draw samples from the posterior given x_o_1 and then plot them: posterior_samples_1 = posterior . sample (( 10000 ,), x = x_o_1 ) # plot posterior samples _ = analysis . pairplot ( posterior_samples_1 , limits = [[ - 2 , 2 ],[ - 2 , 2 ],[ - 2 , 2 ]], figsize = ( 5 , 5 )) HBox(children=(FloatProgress(value=0.0, description='Drawing 10000 posterior samples', max=10000.0, style=Prog\u2026 As it can be seen, the posterior samples are centered around [-1,-1,-1] in each dimension. This makes sense because the simulator always adds 1.0 in each dimension and we have observed x_o_1 = [0,0,0] . Since the learned posterior is amortized, we can also draw samples from the posterior given the second observation without having to re-run inference: posterior_samples_2 = posterior . sample (( 10000 ,), x = x_o_2 ) # plot posterior samples _ = analysis . pairplot ( posterior_samples_2 , limits = [[ - 2 , 2 ],[ - 2 , 2 ],[ - 2 , 2 ]], figsize = ( 5 , 5 )) HBox(children=(FloatProgress(value=0.0, description='Drawing 10000 posterior samples', max=10000.0, style=Prog\u2026 So, if we observed x_o_2 = [2,2,2] , the posterior is centered around [1,1,1] \u2013 again, this makes sense because the simulator adds 1.0 in each dimension.","title":"Amortized inference"},{"location":"tutorial/01_gaussian_amortized/#amortized-posterior-inference-on-gaussian-example","text":"Note, you find the original version of this notebook at https://github.com/mackelab/sbi/blob/main/tutorials/01_gaussian_amortized.ipynb in the sbi repository. In this tutorial, we will demonstrate how sbi can infer an amortized posterior for a simple toy model with a uniform prior and Gaussian likelihood. import torch import numpy as np from sbi import utils as utils from sbi import analysis as analysis from sbi.inference.base import infer","title":"Amortized posterior inference on Gaussian example"},{"location":"tutorial/01_gaussian_amortized/#defining-prior-simulator-and-running-inference","text":"Say we have 3-dimensional parameter space, and the prior is uniformly distributed between -2 and 2 in each dimension, i.e. \\(x\\in [-2,2], y\\in [-2,2], z \\in [-2,2]\\) . num_dim = 3 prior = utils . BoxUniform ( low =- 2 * torch . ones ( num_dim ), high = 2 * torch . ones ( num_dim )) Our simulator takes the input parameters, adds 1.0 in each dimension, and then adds some Gaussian noise: def linear_gaussian ( theta ): return theta + 1.0 + torch . randn_like ( theta ) * 0.1 We can then run inference: posterior = infer ( linear_gaussian , prior , 'SNPE' , num_simulations = 1000 ) HBox(children=(FloatProgress(value=0.0, description='Running 1000 simulations.', max=1000.0, style=ProgressSty\u2026 Neural network successfully converged after 150 epochs.","title":"Defining prior, simulator, and running inference"},{"location":"tutorial/01_gaussian_amortized/#amortized-inference","text":"Note that we have not yet provided an observation to the inference procedure. In fact, we can evaluate the posterior for different observations without having to re-run inference. This is called amortization. An amortized posterior is one that is not focused on any particular observation. Naturally, if the diversity of observations is large, any of the inference methods will need to run a sufficient number of simulations for the resulting posterior to perform well across these diverse observations. Let\u2019s say we have two observations x_o_1 = [0,0,0] and x_o_2 = [2,2,2] : x_o_1 = torch . zeros ( 3 ,) x_o_2 = 2.0 * torch . ones ( 3 ,) We can draw samples from the posterior given x_o_1 and then plot them: posterior_samples_1 = posterior . sample (( 10000 ,), x = x_o_1 ) # plot posterior samples _ = analysis . pairplot ( posterior_samples_1 , limits = [[ - 2 , 2 ],[ - 2 , 2 ],[ - 2 , 2 ]], figsize = ( 5 , 5 )) HBox(children=(FloatProgress(value=0.0, description='Drawing 10000 posterior samples', max=10000.0, style=Prog\u2026 As it can be seen, the posterior samples are centered around [-1,-1,-1] in each dimension. This makes sense because the simulator always adds 1.0 in each dimension and we have observed x_o_1 = [0,0,0] . Since the learned posterior is amortized, we can also draw samples from the posterior given the second observation without having to re-run inference: posterior_samples_2 = posterior . sample (( 10000 ,), x = x_o_2 ) # plot posterior samples _ = analysis . pairplot ( posterior_samples_2 , limits = [[ - 2 , 2 ],[ - 2 , 2 ],[ - 2 , 2 ]], figsize = ( 5 , 5 )) HBox(children=(FloatProgress(value=0.0, description='Drawing 10000 posterior samples', max=10000.0, style=Prog\u2026 So, if we observed x_o_2 = [2,2,2] , the posterior is centered around [1,1,1] \u2013 again, this makes sense because the simulator adds 1.0 in each dimension.","title":"Amortized inference"},{"location":"tutorial/02_flexible_interface/","text":"The flexible interface \u00b6 In the previous tutorial, we have demonstrated how sbi can be used to run simulation-based inference with just a single line of code. In addition to this simple interface, sbi also provides a flexible interface which provides several additional features implemented in sbi . Note, you find the original version of this notebook at https://github.com/mackelab/sbi/blob/main/tutorials/02_flexible_interface.ipynb in the sbi repository. Features \u00b6 The flexible interface offers at least the following features: performing sequential posterior estimation by focusing on a particular observation over multiple rounds. This can decrease the number of simulations one has to run, but the inference procedure is no longer amortized ( tutorial ). specify your own density estimator, or change hyperparameters of existing ones (e.g. number of hidden units for NSF ) ( tutorial ). use an embedding_net to learn summary features from high-dimensional simulation outputs ( tutorial ). provide presimulated data run simulations in batches, which can speed up simulations. when it makes sense, choose between different methods to sample from the posterior. use calibration kernels as proposed by Lueckmann, Goncalves et al. 2017 . Main syntax \u00b6 The main syntax for the flexible interface was changed in sbi version 0.14.0 (see Github for details). Below, we show the syntax of the flexible interface until and after v0.13.2. Syntax until v0.13.2 \u00b6 from sbi.inference import SNPE , prepare_for_sbi simulator , prior = prepare_for_sbi ( simulator , prior ) inference = SNPE ( simulator , prior ) # Simulate, train, and build posterior. posterior = inference ( num_simulation = 1000 ) Syntax from v0.14.0 onwards \u00b6 from sbi.inference import SNPE , prepare_for_sbi , simulate_for_sbi simulator , prior = prepare_for_sbi ( simulator , prior ) inference = SNPE ( prior ) theta , x = simulate_for_sbi ( simulator , proposal = prior , num_simulations = 1000 ) density_estimator = inference . append_simulations ( theta , x ) . train () posterior = inference . build_posterior ( density_estimator ) # MCMC kwargs go here. Linear Gaussian example \u00b6 We will show an example of how we can use the flexible interface (v0.14.0 onwards) to infer the posterior for an example with a Gaussian likelihood (same example as before). First, we import the inference method we want to use ( SNPE , SNLE , or SNRE ) and other helper functions. import torch from sbi.inference import SNPE , prepare_for_sbi , simulate_for_sbi from sbi.utils.get_nn_models import posterior_nn from sbi import utils as utils from sbi import analysis as analysis Next, we define the prior and simulator: num_dim = 3 prior = utils . BoxUniform ( low =- 2 * torch . ones ( num_dim ), high = 2 * torch . ones ( num_dim )) def linear_gaussian ( theta ): return theta + 1.0 + torch . randn_like ( theta ) * 0.1 In the flexible interface, you have to ensure that your simulator and prior adhere the requirements of sbi . You can do so with the prepare_for_sbi() function. simulator , prior = prepare_for_sbi ( linear_gaussian , prior ) Then, we instantiate the inference object: inference = SNPE ( prior = prior ) Next, we run simulations. You can do so either by yourself by sampling from the prior and running the simulator (e.g. on a compute cluster), or you can use a helper function provided by sbi called simulate_for_sbi . This function allows to parallelize your code with joblib . theta , x = simulate_for_sbi ( simulator , proposal = prior , num_simulations = 500 ) HBox(children=(FloatProgress(value=0.0, description='Running 500 simulations.', max=500.0, style=ProgressStyle\u2026 We then pass the simulated data to the inference object. theta and x should both be tensors of type float32. inference = inference . append_simulations ( theta , x ) Next, we train the neural density estimator. density_estimator = inference . train () Neural network successfully converged after 191 epochs. Lastly, we can use this density estimator to build the posterior: posterior = inference . build_posterior ( density_estimator ) Once we have obtained the posterior, we can .sample() , .log_prob() , or .pairplot() in the same way as for the simple interface. x_o = torch . zeros ( 3 ,) posterior_samples = posterior . sample (( 10000 ,), x = x_o ) # plot posterior samples _ = analysis . pairplot ( posterior_samples , limits = [[ - 2 , 2 ],[ - 2 , 2 ],[ - 2 , 2 ]], figsize = ( 5 , 5 )) HBox(children=(FloatProgress(value=0.0, description='Drawing 10000 posterior samples', max=10000.0, style=Prog\u2026 We can always print the posterior to know how it was trained: print ( posterior ) Posterior conditional density p(\u03b8|x) (amortized). This DirectPosterior-object was obtained with a SNPE-class method using a flow. It allows to .sample() and .log_prob() the posterior and wraps the output of the .net to avoid leakage into regions with 0 prior probability.","title":"Flexible interface"},{"location":"tutorial/02_flexible_interface/#the-flexible-interface","text":"In the previous tutorial, we have demonstrated how sbi can be used to run simulation-based inference with just a single line of code. In addition to this simple interface, sbi also provides a flexible interface which provides several additional features implemented in sbi . Note, you find the original version of this notebook at https://github.com/mackelab/sbi/blob/main/tutorials/02_flexible_interface.ipynb in the sbi repository.","title":"The flexible interface"},{"location":"tutorial/02_flexible_interface/#features","text":"The flexible interface offers at least the following features: performing sequential posterior estimation by focusing on a particular observation over multiple rounds. This can decrease the number of simulations one has to run, but the inference procedure is no longer amortized ( tutorial ). specify your own density estimator, or change hyperparameters of existing ones (e.g. number of hidden units for NSF ) ( tutorial ). use an embedding_net to learn summary features from high-dimensional simulation outputs ( tutorial ). provide presimulated data run simulations in batches, which can speed up simulations. when it makes sense, choose between different methods to sample from the posterior. use calibration kernels as proposed by Lueckmann, Goncalves et al. 2017 .","title":"Features"},{"location":"tutorial/02_flexible_interface/#main-syntax","text":"The main syntax for the flexible interface was changed in sbi version 0.14.0 (see Github for details). Below, we show the syntax of the flexible interface until and after v0.13.2.","title":"Main syntax"},{"location":"tutorial/02_flexible_interface/#syntax-until-v0132","text":"from sbi.inference import SNPE , prepare_for_sbi simulator , prior = prepare_for_sbi ( simulator , prior ) inference = SNPE ( simulator , prior ) # Simulate, train, and build posterior. posterior = inference ( num_simulation = 1000 )","title":"Syntax until v0.13.2"},{"location":"tutorial/02_flexible_interface/#syntax-from-v0140-onwards","text":"from sbi.inference import SNPE , prepare_for_sbi , simulate_for_sbi simulator , prior = prepare_for_sbi ( simulator , prior ) inference = SNPE ( prior ) theta , x = simulate_for_sbi ( simulator , proposal = prior , num_simulations = 1000 ) density_estimator = inference . append_simulations ( theta , x ) . train () posterior = inference . build_posterior ( density_estimator ) # MCMC kwargs go here.","title":"Syntax from v0.14.0 onwards"},{"location":"tutorial/02_flexible_interface/#linear-gaussian-example","text":"We will show an example of how we can use the flexible interface (v0.14.0 onwards) to infer the posterior for an example with a Gaussian likelihood (same example as before). First, we import the inference method we want to use ( SNPE , SNLE , or SNRE ) and other helper functions. import torch from sbi.inference import SNPE , prepare_for_sbi , simulate_for_sbi from sbi.utils.get_nn_models import posterior_nn from sbi import utils as utils from sbi import analysis as analysis Next, we define the prior and simulator: num_dim = 3 prior = utils . BoxUniform ( low =- 2 * torch . ones ( num_dim ), high = 2 * torch . ones ( num_dim )) def linear_gaussian ( theta ): return theta + 1.0 + torch . randn_like ( theta ) * 0.1 In the flexible interface, you have to ensure that your simulator and prior adhere the requirements of sbi . You can do so with the prepare_for_sbi() function. simulator , prior = prepare_for_sbi ( linear_gaussian , prior ) Then, we instantiate the inference object: inference = SNPE ( prior = prior ) Next, we run simulations. You can do so either by yourself by sampling from the prior and running the simulator (e.g. on a compute cluster), or you can use a helper function provided by sbi called simulate_for_sbi . This function allows to parallelize your code with joblib . theta , x = simulate_for_sbi ( simulator , proposal = prior , num_simulations = 500 ) HBox(children=(FloatProgress(value=0.0, description='Running 500 simulations.', max=500.0, style=ProgressStyle\u2026 We then pass the simulated data to the inference object. theta and x should both be tensors of type float32. inference = inference . append_simulations ( theta , x ) Next, we train the neural density estimator. density_estimator = inference . train () Neural network successfully converged after 191 epochs. Lastly, we can use this density estimator to build the posterior: posterior = inference . build_posterior ( density_estimator ) Once we have obtained the posterior, we can .sample() , .log_prob() , or .pairplot() in the same way as for the simple interface. x_o = torch . zeros ( 3 ,) posterior_samples = posterior . sample (( 10000 ,), x = x_o ) # plot posterior samples _ = analysis . pairplot ( posterior_samples , limits = [[ - 2 , 2 ],[ - 2 , 2 ],[ - 2 , 2 ]], figsize = ( 5 , 5 )) HBox(children=(FloatProgress(value=0.0, description='Drawing 10000 posterior samples', max=10000.0, style=Prog\u2026 We can always print the posterior to know how it was trained: print ( posterior ) Posterior conditional density p(\u03b8|x) (amortized). This DirectPosterior-object was obtained with a SNPE-class method using a flow. It allows to .sample() and .log_prob() the posterior and wraps the output of the .net to avoid leakage into regions with 0 prior probability.","title":"Linear Gaussian example"},{"location":"tutorial/03_embedding_net/","text":"Learning summary statistics with a neural net \u00b6 When doing simulation-based inference, it is very important to use well-chosen summary statistics for describing the data generated by the simulator. Very often, these statistics take into account previous domain knowledge. For instance, in the case of the Hodgkin-Huxley model from this tutorial , the summary statistics are defined via the function defined in here , which takes a 120 ms recording as input (a 12000-dimensional input vector) and outputs a 7-dimensional feature vector containing different statistical descriptors of the recording (e.g., number of spikes, average value, etc.). However, in some occasions, it might be of interest to actually learn from the data which summary statistics to use. sbi offers functionality to learn summary statistics from (potentially high-dimensional) simulation outputs with a neural network. In sbi , this neural network is referred to as embedding_net . If an embedding_net is specified, the simulation outputs are passed through the embedding_net , whose outputs are then passed to the neural density estimator. The parameters of the embedding_net are updated together with the parameters of the neural density estimator. NB: only SNPE and SNRE methods can use an embedding_net to learn summary statistics from simulation outputs. SNLE does not offer this functionality since the simulation outputs \\(x\\) are the outputs of the neural density estimator in SNLE . In the example that follows, we illustrate a situation where the data points generated by the simulator model are high-dimensional (32 by 32 images) and we use a convolutional neural network as summary statistics extractor. Note, you find the original version of this notebook at https://github.com/mackelab/sbi/blob/main/tutorials/05_embedding_net.ipynb in the sbi repository. First of all, we import all the packages required for running the tutorial import matplotlib.pyplot as plt import numpy as np import torch import torch.nn as nn import torch.nn.functional as F from sbi import utils from sbi import inference import numpy as np # set seed for numpy and torch seed = 42 np . random . seed ( seed ) torch . manual_seed ( seed ) The simulator model \u00b6 The simulator model that we consider has two parameters: \\(r\\) and \\(\\theta\\) . On each run, it generates 100 two-dimensional points centered around \\((r \\cos(\\theta), r \\sin(\\theta))\\) and perturbed by a Gaussian noise with variance 0.01. Instead of simply outputting the \\((x,y)\\) coordinates of each data point, the model generates a grayscale image of the scattered points with dimensions 32 by 32. This image is further perturbed by an uniform noise with values betweeen 0 and 0.2. The code below defines such model. def simulator_model ( parameter , return_points = False ): \"\"\" Simulator model with two-dimensional input parameter and 1024-dimensional output This simulator serves as a basic example for using a neural net for learning summary features. It has only two input parameters but generates high-dimensional output vectors. The data is generated as follows: (-) Input: parameter = [r, theta] (1) Generate 100 two-dimensional points centered around (r cos(theta),r sin(theta)) and perturbed by a Gaussian noise with variance 0.01 (2) Create a grayscale image I of the scattered points with dimensions 32 by 32 (3) Perturb I with an uniform noise with values betweeen 0 and 0.2 (-) Output: I Parameters ---------- parameter : array-like, shape (2) The two input parameters of the model, ordered as [r, theta] return_points : bool (default: False) Whether the simulator should return the coordinates of the simulated data points as well Returns ------- I: torch tensor, shape (1, 1024) Output flattened image (optional) points: array-like, shape (100, 2) Coordinates of the 2D simulated data points \"\"\" r = parameter [ 0 ] theta = parameter [ 1 ] sigma_points = 0.10 npoints = 100 points = [] for _ in range ( npoints ): x = r * np . cos ( theta ) + sigma_points * np . random . randn () y = r * np . sin ( theta ) + sigma_points * np . random . randn () points . append ([ x , y ]) points = np . array ( points ) nx = 32 ny = 32 sigma_image = 0.20 I = np . zeros (( nx , ny )) for point in points : pi = int (( point [ 0 ] - ( - 1 )) / (( + 1 ) - ( - 1 )) * nx ) pj = int (( point [ 1 ] - ( - 1 )) / (( + 1 ) - ( - 1 )) * ny ) if ( pi < nx ) and ( pj < ny ): I [ pi , pj ] = 1 I = I + sigma_image * np . random . rand ( nx , ny ) I = I . T I = I . reshape ( 1 , - 1 ) I = torch . tensor ( I , dtype = torch . get_default_dtype ()) if return_points : return I , points else : return I The figure below shows an example of the output of the simulator when \\(r = 0.70\\) and \\(\\theta = \\pi/4\\) # simulate samples true_parameter = torch . tensor ([ 0.70 , np . pi / 4 ]) x_observed , x_points = simulator_model ( true_parameter , return_points = True ) # plot the observation fig , ax = plt . subplots ( facecolor = 'white' , figsize = ( 11.15 , 5.61 ), ncols = 2 , constrained_layout = True ) circle = plt . Circle (( 0 , 0 ), 1.0 , color = 'k' , ls = '--' , lw = 0.8 , fill = False ) ax [ 0 ] . add_artist ( circle ) ax [ 0 ] . scatter ( x_points [:, 0 ], x_points [:, 1 ], s = 20 ) ax [ 0 ] . set_xlabel ( 'x' ) ax [ 0 ] . set_ylabel ( 'y' ) ax [ 0 ] . set_xlim ( - 1 , + 1 ) ax [ 0 ] . set_xticks ([ - 1 , 0.0 , + 1.0 ]) ax [ 0 ] . set_ylim ( - 1 , + 1 ) ax [ 0 ] . set_yticks ([ - 1 , 0.0 , + 1.0 ]) ax [ 0 ] . set_title ( r 'original simulated points with $r = 0.70$ and $\\theta = \\pi/4$' ) ax [ 1 ] . imshow ( x_observed . view ( 32 , 32 ), origin = 'lower' , cmap = 'gray' ) ax [ 1 ] . set_xticks ([]); ax [ 1 ] . set_yticks ([]) ax [ 1 ] . set_title ( 'noisy observed data (gray image with 32 x 32 pixels)' ) Defining an embedding_net \u00b6 An inference procedure applied to the output data from this simulator model determines the posterior distribution of \\(r\\) and \\(\\theta\\) given an observation of \\(x\\) , which lives in a 1024 dimensional space (32 x 32 = 1024). To avoid working directly on these high-dimensional vectors, one can use a convolutional neural network (CNN) that takes the 32x32 images as input and encodes them into 8-dimensional feature vectors. This CNN is trained along with the neural density estimator of the inference procedure and serves as an automatic summary statistics extractor. We define and instantiate the CNN as follows: class SummaryNet ( nn . Module ): def __init__ ( self ): super () . __init__ () # 2D convolutional layer self . conv1 = nn . Conv2d ( in_channels = 1 , out_channels = 6 , kernel_size = 5 , padding = 2 ) # Maxpool layer that reduces 32x32 image to 4x4 self . pool = nn . MaxPool2d ( kernel_size = 8 , stride = 8 ) # Fully connected layer taking as input the 6 flattened output arrays from the maxpooling layer self . fc = nn . Linear ( in_features = 6 * 4 * 4 , out_features = 8 ) def forward ( self , x ): x = x . view ( - 1 , 1 , 32 , 32 ) x = self . pool ( F . relu ( self . conv1 ( x ))) x = x . view ( - 1 , 6 * 4 * 4 ) x = F . relu ( self . fc ( x )) return x embedding_net = SummaryNet () The inference procedure \u00b6 With the embedding_net defined and instantiated, we can follow the usual workflow of an inference procedure in sbi . The embedding_net object appears as an input argument when instantiating the neural density estimator with utils.posterior_nn . # set prior distribution for the parameters prior = utils . BoxUniform ( low = torch . tensor ([ 0.0 , 0.0 ]), high = torch . tensor ([ 1.0 , 2 * np . pi ])) # make a SBI-wrapper on the simulator object for compatibility simulator_wrapper , prior = inference . prepare_for_sbi ( simulator_model , prior ) # instantiate the neural density estimator neural_posterior = utils . posterior_nn ( model = 'maf' , embedding_net = embedding_net , hidden_features = 10 , num_transforms = 2 ) # setup the inference procedure with the SNPE-C procedure inference = inference . SNPE ( simulator_wrapper , prior , density_estimator = neural_posterior , show_progress_bars = True ) # run the inference procedure on one round and 10000 simulated data points posterior = inference ( num_simulations = 10000 ) Visualizing the results \u00b6 We now generate 50000 samples of the posterior distribution of \\(r\\) and \\(\\theta\\) when observing an input data point \\(x\\) generated from the simulator model with \\(r = 0.70\\) and \\(\\theta = \\pi/4\\) . # generate posterior samples true_parameter = torch . tensor ([ 0.70 , np . pi / 4 ]) x_observed = simulator_model ( true_parameter ) samples = posterior . set_default_x ( x_observed ) . sample (( 50000 ,)) The figure below shows the statistics of the generated samples. # create the figure fig , ax = utils . pairplot ( samples , points = true_parameter , labels = [ 'r' , r '$\\theta$' ], limits = [[ 0 , 1 ], [ 0 , 2 * np . pi ]], points_colors = 'r' , points_offdiag = { 'markersize' : 6 }, fig_size = [ 7.5 , 6.4 ])","title":"Learning summary statistics with a neural net"},{"location":"tutorial/03_embedding_net/#learning-summary-statistics-with-a-neural-net","text":"When doing simulation-based inference, it is very important to use well-chosen summary statistics for describing the data generated by the simulator. Very often, these statistics take into account previous domain knowledge. For instance, in the case of the Hodgkin-Huxley model from this tutorial , the summary statistics are defined via the function defined in here , which takes a 120 ms recording as input (a 12000-dimensional input vector) and outputs a 7-dimensional feature vector containing different statistical descriptors of the recording (e.g., number of spikes, average value, etc.). However, in some occasions, it might be of interest to actually learn from the data which summary statistics to use. sbi offers functionality to learn summary statistics from (potentially high-dimensional) simulation outputs with a neural network. In sbi , this neural network is referred to as embedding_net . If an embedding_net is specified, the simulation outputs are passed through the embedding_net , whose outputs are then passed to the neural density estimator. The parameters of the embedding_net are updated together with the parameters of the neural density estimator. NB: only SNPE and SNRE methods can use an embedding_net to learn summary statistics from simulation outputs. SNLE does not offer this functionality since the simulation outputs \\(x\\) are the outputs of the neural density estimator in SNLE . In the example that follows, we illustrate a situation where the data points generated by the simulator model are high-dimensional (32 by 32 images) and we use a convolutional neural network as summary statistics extractor. Note, you find the original version of this notebook at https://github.com/mackelab/sbi/blob/main/tutorials/05_embedding_net.ipynb in the sbi repository. First of all, we import all the packages required for running the tutorial import matplotlib.pyplot as plt import numpy as np import torch import torch.nn as nn import torch.nn.functional as F from sbi import utils from sbi import inference import numpy as np # set seed for numpy and torch seed = 42 np . random . seed ( seed ) torch . manual_seed ( seed )","title":"Learning summary statistics with a neural net"},{"location":"tutorial/03_embedding_net/#the-simulator-model","text":"The simulator model that we consider has two parameters: \\(r\\) and \\(\\theta\\) . On each run, it generates 100 two-dimensional points centered around \\((r \\cos(\\theta), r \\sin(\\theta))\\) and perturbed by a Gaussian noise with variance 0.01. Instead of simply outputting the \\((x,y)\\) coordinates of each data point, the model generates a grayscale image of the scattered points with dimensions 32 by 32. This image is further perturbed by an uniform noise with values betweeen 0 and 0.2. The code below defines such model. def simulator_model ( parameter , return_points = False ): \"\"\" Simulator model with two-dimensional input parameter and 1024-dimensional output This simulator serves as a basic example for using a neural net for learning summary features. It has only two input parameters but generates high-dimensional output vectors. The data is generated as follows: (-) Input: parameter = [r, theta] (1) Generate 100 two-dimensional points centered around (r cos(theta),r sin(theta)) and perturbed by a Gaussian noise with variance 0.01 (2) Create a grayscale image I of the scattered points with dimensions 32 by 32 (3) Perturb I with an uniform noise with values betweeen 0 and 0.2 (-) Output: I Parameters ---------- parameter : array-like, shape (2) The two input parameters of the model, ordered as [r, theta] return_points : bool (default: False) Whether the simulator should return the coordinates of the simulated data points as well Returns ------- I: torch tensor, shape (1, 1024) Output flattened image (optional) points: array-like, shape (100, 2) Coordinates of the 2D simulated data points \"\"\" r = parameter [ 0 ] theta = parameter [ 1 ] sigma_points = 0.10 npoints = 100 points = [] for _ in range ( npoints ): x = r * np . cos ( theta ) + sigma_points * np . random . randn () y = r * np . sin ( theta ) + sigma_points * np . random . randn () points . append ([ x , y ]) points = np . array ( points ) nx = 32 ny = 32 sigma_image = 0.20 I = np . zeros (( nx , ny )) for point in points : pi = int (( point [ 0 ] - ( - 1 )) / (( + 1 ) - ( - 1 )) * nx ) pj = int (( point [ 1 ] - ( - 1 )) / (( + 1 ) - ( - 1 )) * ny ) if ( pi < nx ) and ( pj < ny ): I [ pi , pj ] = 1 I = I + sigma_image * np . random . rand ( nx , ny ) I = I . T I = I . reshape ( 1 , - 1 ) I = torch . tensor ( I , dtype = torch . get_default_dtype ()) if return_points : return I , points else : return I The figure below shows an example of the output of the simulator when \\(r = 0.70\\) and \\(\\theta = \\pi/4\\) # simulate samples true_parameter = torch . tensor ([ 0.70 , np . pi / 4 ]) x_observed , x_points = simulator_model ( true_parameter , return_points = True ) # plot the observation fig , ax = plt . subplots ( facecolor = 'white' , figsize = ( 11.15 , 5.61 ), ncols = 2 , constrained_layout = True ) circle = plt . Circle (( 0 , 0 ), 1.0 , color = 'k' , ls = '--' , lw = 0.8 , fill = False ) ax [ 0 ] . add_artist ( circle ) ax [ 0 ] . scatter ( x_points [:, 0 ], x_points [:, 1 ], s = 20 ) ax [ 0 ] . set_xlabel ( 'x' ) ax [ 0 ] . set_ylabel ( 'y' ) ax [ 0 ] . set_xlim ( - 1 , + 1 ) ax [ 0 ] . set_xticks ([ - 1 , 0.0 , + 1.0 ]) ax [ 0 ] . set_ylim ( - 1 , + 1 ) ax [ 0 ] . set_yticks ([ - 1 , 0.0 , + 1.0 ]) ax [ 0 ] . set_title ( r 'original simulated points with $r = 0.70$ and $\\theta = \\pi/4$' ) ax [ 1 ] . imshow ( x_observed . view ( 32 , 32 ), origin = 'lower' , cmap = 'gray' ) ax [ 1 ] . set_xticks ([]); ax [ 1 ] . set_yticks ([]) ax [ 1 ] . set_title ( 'noisy observed data (gray image with 32 x 32 pixels)' )","title":"The simulator model"},{"location":"tutorial/03_embedding_net/#defining-an-embedding_net","text":"An inference procedure applied to the output data from this simulator model determines the posterior distribution of \\(r\\) and \\(\\theta\\) given an observation of \\(x\\) , which lives in a 1024 dimensional space (32 x 32 = 1024). To avoid working directly on these high-dimensional vectors, one can use a convolutional neural network (CNN) that takes the 32x32 images as input and encodes them into 8-dimensional feature vectors. This CNN is trained along with the neural density estimator of the inference procedure and serves as an automatic summary statistics extractor. We define and instantiate the CNN as follows: class SummaryNet ( nn . Module ): def __init__ ( self ): super () . __init__ () # 2D convolutional layer self . conv1 = nn . Conv2d ( in_channels = 1 , out_channels = 6 , kernel_size = 5 , padding = 2 ) # Maxpool layer that reduces 32x32 image to 4x4 self . pool = nn . MaxPool2d ( kernel_size = 8 , stride = 8 ) # Fully connected layer taking as input the 6 flattened output arrays from the maxpooling layer self . fc = nn . Linear ( in_features = 6 * 4 * 4 , out_features = 8 ) def forward ( self , x ): x = x . view ( - 1 , 1 , 32 , 32 ) x = self . pool ( F . relu ( self . conv1 ( x ))) x = x . view ( - 1 , 6 * 4 * 4 ) x = F . relu ( self . fc ( x )) return x embedding_net = SummaryNet ()","title":"Defining an embedding_net"},{"location":"tutorial/03_embedding_net/#the-inference-procedure","text":"With the embedding_net defined and instantiated, we can follow the usual workflow of an inference procedure in sbi . The embedding_net object appears as an input argument when instantiating the neural density estimator with utils.posterior_nn . # set prior distribution for the parameters prior = utils . BoxUniform ( low = torch . tensor ([ 0.0 , 0.0 ]), high = torch . tensor ([ 1.0 , 2 * np . pi ])) # make a SBI-wrapper on the simulator object for compatibility simulator_wrapper , prior = inference . prepare_for_sbi ( simulator_model , prior ) # instantiate the neural density estimator neural_posterior = utils . posterior_nn ( model = 'maf' , embedding_net = embedding_net , hidden_features = 10 , num_transforms = 2 ) # setup the inference procedure with the SNPE-C procedure inference = inference . SNPE ( simulator_wrapper , prior , density_estimator = neural_posterior , show_progress_bars = True ) # run the inference procedure on one round and 10000 simulated data points posterior = inference ( num_simulations = 10000 )","title":"The inference procedure"},{"location":"tutorial/03_embedding_net/#visualizing-the-results","text":"We now generate 50000 samples of the posterior distribution of \\(r\\) and \\(\\theta\\) when observing an input data point \\(x\\) generated from the simulator model with \\(r = 0.70\\) and \\(\\theta = \\pi/4\\) . # generate posterior samples true_parameter = torch . tensor ([ 0.70 , np . pi / 4 ]) x_observed = simulator_model ( true_parameter ) samples = posterior . set_default_x ( x_observed ) . sample (( 50000 ,)) The figure below shows the statistics of the generated samples. # create the figure fig , ax = utils . pairplot ( samples , points = true_parameter , labels = [ 'r' , r '$\\theta$' ], limits = [[ 0 , 1 ], [ 0 , 2 * np . pi ]], points_colors = 'r' , points_offdiag = { 'markersize' : 6 }, fig_size = [ 7.5 , 6.4 ])","title":"Visualizing the results"},{"location":"tutorial/03_multiround_inference/","text":"Multi-round inference \u00b6 In the previous tutorials, we have inferred the posterior using single-round inference . In single-round inference , we draw parameters from the prior, simulate the corresponding data, and then train a neural network to obtain the posterior. However, if one is interested in only one particular observation x_o sampling from the prior can be inefficient in the number of simulations because one is effectively learning a posterior estimate for all observations in the prior space. In this tutorial, we show how one can alleviate this issue by performing multi-round inference with sbi . Multi-round inference also starts by drawing parameters from the prior, simulating them, and training a neural network to estimate the posterior distribution. Afterwards, however, it continues inference in multiple rounds, focusing on a particular observation x_o . In each new round of inference, it draws samples from the obtained posterior distribution conditioned at x_o (instead of from the prior), simulates these, and trains the network again. This process can be repeated arbitrarily often to get increasingly good approximations to the true posterior distribution at x_o . Running multi-round inference can be more efficient in the number of simulations, but it will lead to the posterior no longer being amortized (i.e. it will be accurate only for a specific observation x_o , not for any x ). Note, you can find the original version of this notebook at https://github.com/mackelab/sbi/blob/main/tutorials/03_multiround_inference.ipynb in the sbi repository. Main syntax \u00b6 # 2 rounds: first round simulates from the prior, second round simulates parameter set # that were sampled from the obtained posterior. num_rounds = 2 # The specific observation we want to focus the inference on. x_o = torch . zeros ( 3 ,) posteriors = [] proposal = prior for _ in range ( num_rounds ): theta , x = simulate_for_sbi ( simulator , proposal , num_simulations = 500 ) # In `SNLE` and `SNRE`, you should not pass the `proposal` to `.append_simulations()` density_estimator = inference . append_simulations ( theta , x , proposal = proposal ) . train () posterior = inference . build_posterior ( density_estimator ) posteriors . append ( posterior ) proposal = posterior . set_default_x ( x_o ) Linear Gaussian example \u00b6 Below, we give a full example of inferring the posterior distribution over multiple rounds. import torch from sbi.inference import SNPE , prepare_for_sbi , simulate_for_sbi from sbi.utils.get_nn_models import posterior_nn from sbi import utils as utils from sbi import analysis as analysis torch . manual_seed ( 0 ) <torch._C.Generator at 0x7f094007e490> First, we define a simple prior and simulator and ensure that they comply with sbi by using prepare_for_sbi : num_dim = 3 prior = utils . BoxUniform ( low =- 2 * torch . ones ( num_dim ), high = 2 * torch . ones ( num_dim )) def linear_gaussian ( theta ): return theta + 1.0 + torch . randn_like ( theta ) * 0.1 simulator , prior = prepare_for_sbi ( linear_gaussian , prior ) Then, we instantiate the inference object: inference = SNPE ( prior = prior ) And we can run inference. In this example, we will run inference over 2 rounds, potentially leading to a more focused posterior around the observation x_o . num_rounds = 2 x_o = torch . zeros ( 3 ,) posteriors = [] proposal = prior for _ in range ( num_rounds ): theta , x = simulate_for_sbi ( simulator , proposal , num_simulations = 500 ) density_estimator = inference . append_simulations ( theta , x , proposal = proposal ) . train () posterior = inference . build_posterior ( density_estimator ) posteriors . append ( posterior ) proposal = posterior . set_default_x ( x_o ) HBox(children=(FloatProgress(value=0.0, description='Running 500 simulations.', max=500.0, style=ProgressStyle\u2026 Neural network successfully converged after 194 epochs. HBox(children=(FloatProgress(value=0.0, description='Drawing 500 posterior samples', max=500.0, style=Progress\u2026 HBox(children=(FloatProgress(value=0.0, description='Running 500 simulations.', max=500.0, style=ProgressStyle\u2026 Using SNPE-C with atomic loss Neural network successfully converged after 41 epochs. Note that, for num_rounds>1 , the posterior is no longer amortized: it will give good results when sampled around x=observation , but possibly bad results for other x . Once we have obtained the posterior, we can .sample() , .log_prob() , or .pairplot() in the same way as for the simple interface. posterior_samples = posterior . sample (( 10000 ,), x = x_o ) # plot posterior samples _ = analysis . pairplot ( posterior_samples , limits = [[ - 2 , 2 ],[ - 2 , 2 ],[ - 2 , 2 ]], figsize = ( 5 , 5 )) HBox(children=(FloatProgress(value=0.0, description='Drawing 10000 posterior samples', max=10000.0, style=Prog\u2026 We can always print the posterior to know how it was trained: print ( posterior ) Posterior conditional density p(\u03b8|x) (multi-round). Evaluates and samples by default at x=[[0.0, 0.0, 0.0]]. This DirectPosterior-object was obtained with a SNPE-class method using a flow. It allows to .sample() and .log_prob() the posterior and wraps the output of the .net to avoid leakage into regions with 0 prior probability.","title":"Multi-round inference"},{"location":"tutorial/03_multiround_inference/#multi-round-inference","text":"In the previous tutorials, we have inferred the posterior using single-round inference . In single-round inference , we draw parameters from the prior, simulate the corresponding data, and then train a neural network to obtain the posterior. However, if one is interested in only one particular observation x_o sampling from the prior can be inefficient in the number of simulations because one is effectively learning a posterior estimate for all observations in the prior space. In this tutorial, we show how one can alleviate this issue by performing multi-round inference with sbi . Multi-round inference also starts by drawing parameters from the prior, simulating them, and training a neural network to estimate the posterior distribution. Afterwards, however, it continues inference in multiple rounds, focusing on a particular observation x_o . In each new round of inference, it draws samples from the obtained posterior distribution conditioned at x_o (instead of from the prior), simulates these, and trains the network again. This process can be repeated arbitrarily often to get increasingly good approximations to the true posterior distribution at x_o . Running multi-round inference can be more efficient in the number of simulations, but it will lead to the posterior no longer being amortized (i.e. it will be accurate only for a specific observation x_o , not for any x ). Note, you can find the original version of this notebook at https://github.com/mackelab/sbi/blob/main/tutorials/03_multiround_inference.ipynb in the sbi repository.","title":"Multi-round inference"},{"location":"tutorial/03_multiround_inference/#main-syntax","text":"# 2 rounds: first round simulates from the prior, second round simulates parameter set # that were sampled from the obtained posterior. num_rounds = 2 # The specific observation we want to focus the inference on. x_o = torch . zeros ( 3 ,) posteriors = [] proposal = prior for _ in range ( num_rounds ): theta , x = simulate_for_sbi ( simulator , proposal , num_simulations = 500 ) # In `SNLE` and `SNRE`, you should not pass the `proposal` to `.append_simulations()` density_estimator = inference . append_simulations ( theta , x , proposal = proposal ) . train () posterior = inference . build_posterior ( density_estimator ) posteriors . append ( posterior ) proposal = posterior . set_default_x ( x_o )","title":"Main syntax"},{"location":"tutorial/03_multiround_inference/#linear-gaussian-example","text":"Below, we give a full example of inferring the posterior distribution over multiple rounds. import torch from sbi.inference import SNPE , prepare_for_sbi , simulate_for_sbi from sbi.utils.get_nn_models import posterior_nn from sbi import utils as utils from sbi import analysis as analysis torch . manual_seed ( 0 ) <torch._C.Generator at 0x7f094007e490> First, we define a simple prior and simulator and ensure that they comply with sbi by using prepare_for_sbi : num_dim = 3 prior = utils . BoxUniform ( low =- 2 * torch . ones ( num_dim ), high = 2 * torch . ones ( num_dim )) def linear_gaussian ( theta ): return theta + 1.0 + torch . randn_like ( theta ) * 0.1 simulator , prior = prepare_for_sbi ( linear_gaussian , prior ) Then, we instantiate the inference object: inference = SNPE ( prior = prior ) And we can run inference. In this example, we will run inference over 2 rounds, potentially leading to a more focused posterior around the observation x_o . num_rounds = 2 x_o = torch . zeros ( 3 ,) posteriors = [] proposal = prior for _ in range ( num_rounds ): theta , x = simulate_for_sbi ( simulator , proposal , num_simulations = 500 ) density_estimator = inference . append_simulations ( theta , x , proposal = proposal ) . train () posterior = inference . build_posterior ( density_estimator ) posteriors . append ( posterior ) proposal = posterior . set_default_x ( x_o ) HBox(children=(FloatProgress(value=0.0, description='Running 500 simulations.', max=500.0, style=ProgressStyle\u2026 Neural network successfully converged after 194 epochs. HBox(children=(FloatProgress(value=0.0, description='Drawing 500 posterior samples', max=500.0, style=Progress\u2026 HBox(children=(FloatProgress(value=0.0, description='Running 500 simulations.', max=500.0, style=ProgressStyle\u2026 Using SNPE-C with atomic loss Neural network successfully converged after 41 epochs. Note that, for num_rounds>1 , the posterior is no longer amortized: it will give good results when sampled around x=observation , but possibly bad results for other x . Once we have obtained the posterior, we can .sample() , .log_prob() , or .pairplot() in the same way as for the simple interface. posterior_samples = posterior . sample (( 10000 ,), x = x_o ) # plot posterior samples _ = analysis . pairplot ( posterior_samples , limits = [[ - 2 , 2 ],[ - 2 , 2 ],[ - 2 , 2 ]], figsize = ( 5 , 5 )) HBox(children=(FloatProgress(value=0.0, description='Drawing 10000 posterior samples', max=10000.0, style=Prog\u2026 We can always print the posterior to know how it was trained: print ( posterior ) Posterior conditional density p(\u03b8|x) (multi-round). Evaluates and samples by default at x=[[0.0, 0.0, 0.0]]. This DirectPosterior-object was obtained with a SNPE-class method using a flow. It allows to .sample() and .log_prob() the posterior and wraps the output of the .net to avoid leakage into regions with 0 prior probability.","title":"Linear Gaussian example"},{"location":"tutorial/04_density_estimators/","text":"Customizing the density estimator \u00b6 sbi allows to specify a custom density estimator for each of the implemented methods. For all options, check the API reference here . Changing the type of density estimator \u00b6 One option is to use one of set of preconfigured density estimators by passing a string in the density_estimator keyword argument to the inference object ( SNPE or SNLE ), e.g., \u201cmaf\u201d to use a Masked Autoregressive Flow, of \u201cnsf\u201d to use a Neural Spline Flow with default hyperparameters. inference = SNPE ( prior = prior , density_estimator = 'maf' ) In the case of SNRE , the argument is called classifier : inference = SNRE ( prior = prior , classifier = 'resnet' ) Changing hyperparameters of density estimators \u00b6 Alternatively, you can use a set of utils functions to configure a density estimator yourself, e.g., use a MAF with hyperparameters chosen for your problem at hand. Here, because we want to use SN*P*E, we specifiy a neural network targeting the posterior (using the utils function posterior_nn ). In this example, we will create a neural spline flow ( 'nsf' ) with 60 hidden units and 3 transform layers: from sbi.utils.get_nn_models import posterior_nn # For SNLE: likelihood_nn(). For SNRE: classifier_nn() density_estimator_build_fun = posterior_nn ( model = 'nsf' , hidden_features = 60 , num_transforms = 3 ) inference = SNPE ( prior = prior , density_estimator = density_estimator_build_fun ) It is also possible to pass an embedding_net to posterior_nn() which learn summary statistics from high-dimensional simulation outputs. You can find a more detailed tutorial on this here . Building new density estimators from scratch \u00b6 Finally, it is also possible to implement your own density estimator from scratch, e.g., including embedding nets to preprocess data, or to a density estimator architecture of your choice. For this, the density_estimator argument needs to be a function that takes theta and x batches as arguments to then construct the density estimator after the first set of simulations was generated. Our utils functions in sbi/utils/get_nn_models.py return such a function.","title":"Custom density estimators"},{"location":"tutorial/04_density_estimators/#customizing-the-density-estimator","text":"sbi allows to specify a custom density estimator for each of the implemented methods. For all options, check the API reference here .","title":"Customizing the density estimator"},{"location":"tutorial/04_density_estimators/#changing-the-type-of-density-estimator","text":"One option is to use one of set of preconfigured density estimators by passing a string in the density_estimator keyword argument to the inference object ( SNPE or SNLE ), e.g., \u201cmaf\u201d to use a Masked Autoregressive Flow, of \u201cnsf\u201d to use a Neural Spline Flow with default hyperparameters. inference = SNPE ( prior = prior , density_estimator = 'maf' ) In the case of SNRE , the argument is called classifier : inference = SNRE ( prior = prior , classifier = 'resnet' )","title":"Changing the type of density estimator"},{"location":"tutorial/04_density_estimators/#changing-hyperparameters-of-density-estimators","text":"Alternatively, you can use a set of utils functions to configure a density estimator yourself, e.g., use a MAF with hyperparameters chosen for your problem at hand. Here, because we want to use SN*P*E, we specifiy a neural network targeting the posterior (using the utils function posterior_nn ). In this example, we will create a neural spline flow ( 'nsf' ) with 60 hidden units and 3 transform layers: from sbi.utils.get_nn_models import posterior_nn # For SNLE: likelihood_nn(). For SNRE: classifier_nn() density_estimator_build_fun = posterior_nn ( model = 'nsf' , hidden_features = 60 , num_transforms = 3 ) inference = SNPE ( prior = prior , density_estimator = density_estimator_build_fun ) It is also possible to pass an embedding_net to posterior_nn() which learn summary statistics from high-dimensional simulation outputs. You can find a more detailed tutorial on this here .","title":"Changing hyperparameters of density estimators"},{"location":"tutorial/04_density_estimators/#building-new-density-estimators-from-scratch","text":"Finally, it is also possible to implement your own density estimator from scratch, e.g., including embedding nets to preprocess data, or to a density estimator architecture of your choice. For this, the density_estimator argument needs to be a function that takes theta and x batches as arguments to then construct the density estimator after the first set of simulations was generated. Our utils functions in sbi/utils/get_nn_models.py return such a function.","title":"Building new density estimators from scratch"},{"location":"tutorial/05_embedding_net/","text":"Learning summary statistics with a neural net \u00b6 When doing simulation-based inference, it is very important to use well-chosen summary statistics for describing the data generated by the simulator. Usually, these statistics take into account domain knowledge. For instance, in the example of the Hodgkin-Huxley model , the summary statistics are defined by a function which takes a 120 ms recording as input (a 12000-dimensional input vector) and outputs a 7-dimensional feature vector containing different statistical descriptors of the recording (e.g., number of spikes, average value, etc.). However, in other cases, it might be of interest to actually learn from the data which summary statistics to use, e.g., because the raw data is highly complex and domain knowledge is not available or not applicable. sbi offers functionality to learn summary statistics from (potentially high-dimensional) simulation outputs with a neural network. In sbi , this neural network is referred to as embedding_net . If an embedding_net is specified, the simulation outputs are passed through the embedding_net , whose outputs are then passed to the neural density estimator. The parameters of the embedding_net are learned together with the parameters of the neural density estimator. NB: only SNPE and SNRE methods can use an embedding_net to learn summary statistics from simulation outputs. SNLE does not offer this functionality since the simulation outputs \\(x\\) are the outputs of the neural density estimator. In the example that follows, we illustrate a situation where the data points generated by the simulator model are high-dimensional (32 by 32 images) and we use a convolutional neural network as summary statistics extractor. Note, you find the original version of this notebook at https://github.com/mackelab/sbi/blob/main/tutorials/05_embedding_net.ipynb in the sbi repository. First of all, we import all the packages required for running the tutorial import matplotlib.pyplot as plt import numpy as np import torch import torch.nn as nn import torch.nn.functional as F from sbi import utils from sbi import analysis from sbi import inference from sbi.inference import SNPE , simulate_for_sbi , prepare_for_sbi import numpy as np # set seed for numpy and torch seed = 42 np . random . seed ( seed ) torch . manual_seed ( seed ) <torch._C.Generator at 0x128d04990> The simulator model \u00b6 The simulator model that we consider has two parameters: \\(r\\) and \\(\\theta\\) . On each run, it generates 100 two-dimensional points centered around \\((r \\cos(\\theta), r \\sin(\\theta))\\) and perturbed by a Gaussian noise with variance 0.01. Instead of simply outputting the \\((x,y)\\) coordinates of each data point, the model generates a grayscale image of the scattered points with dimensions 32 by 32. This image is further perturbed by an uniform noise with values betweeen 0 and 0.2. The code below defines such model. def simulator_model ( parameter , return_points = False ): \"\"\" Simulator model with two-dimensional input parameter and 1024-dimensional output This simulator serves as a basic example for using a neural net for learning summary features. It has only two input parameters but generates high-dimensional output vectors. The data is generated as follows: (-) Input: parameter = [r, theta] (1) Generate 100 two-dimensional points centered around (r cos(theta),r sin(theta)) and perturbed by a Gaussian noise with variance 0.01 (2) Create a grayscale image I of the scattered points with dimensions 32 by 32 (3) Perturb I with an uniform noise with values betweeen 0 and 0.2 (-) Output: I Parameters ---------- parameter : array-like, shape (2) The two input parameters of the model, ordered as [r, theta] return_points : bool (default: False) Whether the simulator should return the coordinates of the simulated data points as well Returns ------- I: torch tensor, shape (1, 1024) Output flattened image (optional) points: array-like, shape (100, 2) Coordinates of the 2D simulated data points \"\"\" r = parameter [ 0 ] theta = parameter [ 1 ] sigma_points = 0.10 npoints = 100 points = [] for _ in range ( npoints ): x = r * np . cos ( theta ) + sigma_points * np . random . randn () y = r * np . sin ( theta ) + sigma_points * np . random . randn () points . append ([ x , y ]) points = np . array ( points ) nx = 32 ny = 32 sigma_image = 0.20 I = np . zeros (( nx , ny )) for point in points : pi = int (( point [ 0 ] - ( - 1 )) / (( + 1 ) - ( - 1 )) * nx ) pj = int (( point [ 1 ] - ( - 1 )) / (( + 1 ) - ( - 1 )) * ny ) if ( pi < nx ) and ( pj < ny ): I [ pi , pj ] = 1 I = I + sigma_image * np . random . rand ( nx , ny ) I = I . T I = I . reshape ( 1 , - 1 ) I = torch . tensor ( I , dtype = torch . get_default_dtype ()) if return_points : return I , points else : return I The figure below shows an example of the output of the simulator when \\(r = 0.70\\) and \\(\\theta = \\pi/4\\) # simulate samples true_parameter = torch . tensor ([ 0.70 , np . pi / 4 ]) x_observed , x_points = simulator_model ( true_parameter , return_points = True ) # plot the observation fig , ax = plt . subplots ( facecolor = 'white' , figsize = ( 11.15 , 5.61 ), ncols = 2 , constrained_layout = True ) circle = plt . Circle (( 0 , 0 ), 1.0 , color = 'k' , ls = '--' , lw = 0.8 , fill = False ) ax [ 0 ] . add_artist ( circle ) ax [ 0 ] . scatter ( x_points [:, 0 ], x_points [:, 1 ], s = 20 ) ax [ 0 ] . set_xlabel ( 'x' ) ax [ 0 ] . set_ylabel ( 'y' ) ax [ 0 ] . set_xlim ( - 1 , + 1 ) ax [ 0 ] . set_xticks ([ - 1 , 0.0 , + 1.0 ]) ax [ 0 ] . set_ylim ( - 1 , + 1 ) ax [ 0 ] . set_yticks ([ - 1 , 0.0 , + 1.0 ]) ax [ 0 ] . set_title ( r 'original simulated points with $r = 0.70$ and $\\theta = \\pi/4$' ) ax [ 1 ] . imshow ( x_observed . view ( 32 , 32 ), origin = 'lower' , cmap = 'gray' ) ax [ 1 ] . set_xticks ([]); ax [ 1 ] . set_yticks ([]) ax [ 1 ] . set_title ( 'noisy observed data (gray image with 32 x 32 pixels)' ) Text(0.5, 1.0, 'noisy observed data (gray image with 32 x 32 pixels)') Defining an embedding_net \u00b6 An inference procedure applied to the output data from this simulator model determines the posterior distribution of \\(r\\) and \\(\\theta\\) given an observation of \\(x\\) , which lives in a 1024 dimensional space (32 x 32 = 1024). To avoid working directly on these high-dimensional vectors, one can use a convolutional neural network (CNN) that takes the 32x32 images as input and encodes them into 8-dimensional feature vectors. This CNN is trained along with the neural density estimator of the inference procedure and serves as an automatic summary statistics extractor. We define and instantiate the CNN as follows: class SummaryNet ( nn . Module ): def __init__ ( self ): super () . __init__ () # 2D convolutional layer self . conv1 = nn . Conv2d ( in_channels = 1 , out_channels = 6 , kernel_size = 5 , padding = 2 ) # Maxpool layer that reduces 32x32 image to 4x4 self . pool = nn . MaxPool2d ( kernel_size = 8 , stride = 8 ) # Fully connected layer taking as input the 6 flattened output arrays from the maxpooling layer self . fc = nn . Linear ( in_features = 6 * 4 * 4 , out_features = 8 ) def forward ( self , x ): x = x . view ( - 1 , 1 , 32 , 32 ) x = self . pool ( F . relu ( self . conv1 ( x ))) x = x . view ( - 1 , 6 * 4 * 4 ) x = F . relu ( self . fc ( x )) return x embedding_net = SummaryNet () The inference procedure \u00b6 With the embedding_net defined and instantiated, we can follow the usual workflow of an inference procedure in sbi . The embedding_net object appears as an input argument when instantiating the neural density estimator with utils.posterior_nn . # set prior distribution for the parameters prior = utils . BoxUniform ( low = torch . tensor ([ 0.0 , 0.0 ]), high = torch . tensor ([ 1.0 , 2 * np . pi ])) # make a SBI-wrapper on the simulator object for compatibility simulator_wrapper , prior = prepare_for_sbi ( simulator_model , prior ) # instantiate the neural density estimator neural_posterior = utils . posterior_nn ( model = 'maf' , embedding_net = embedding_net , hidden_features = 10 , num_transforms = 2 ) # setup the inference procedure with the SNPE-C procedure inference = SNPE ( prior = prior , density_estimator = neural_posterior ) # run the inference procedure on one round and 10000 simulated data points theta , x = simulate_for_sbi ( simulator_wrapper , prior , num_simulations = 10000 ) density_estimator = inference . append_simulations ( theta , x ) . train () posterior = inference . build_posterior ( density_estimator ) HBox(children=(FloatProgress(value=0.0, description='Running 10000 simulations.', max=10000.0, style=ProgressS\u2026 Visualizing the results \u00b6 We now generate 50000 samples of the posterior distribution of \\(r\\) and \\(\\theta\\) when observing an input data point \\(x\\) generated from the simulator model with \\(r = 0.70\\) and \\(\\theta = \\pi/4\\) . # generate posterior samples true_parameter = torch . tensor ([ 0.70 , np . pi / 4 ]) x_observed = simulator_model ( true_parameter ) samples = posterior . set_default_x ( x_observed ) . sample (( 50000 ,)) The figure below shows the statistics of the generated samples. # create the figure fig , ax = analysis . pairplot ( samples , points = true_parameter , labels = [ 'r' , r '$\\theta$' ], limits = [[ 0 , 1 ], [ 0 , 2 * np . pi ]], points_colors = 'r' , points_offdiag = { 'markersize' : 6 }, figsize = [ 7.5 , 6.4 ])","title":"Learning summary statistics"},{"location":"tutorial/05_embedding_net/#learning-summary-statistics-with-a-neural-net","text":"When doing simulation-based inference, it is very important to use well-chosen summary statistics for describing the data generated by the simulator. Usually, these statistics take into account domain knowledge. For instance, in the example of the Hodgkin-Huxley model , the summary statistics are defined by a function which takes a 120 ms recording as input (a 12000-dimensional input vector) and outputs a 7-dimensional feature vector containing different statistical descriptors of the recording (e.g., number of spikes, average value, etc.). However, in other cases, it might be of interest to actually learn from the data which summary statistics to use, e.g., because the raw data is highly complex and domain knowledge is not available or not applicable. sbi offers functionality to learn summary statistics from (potentially high-dimensional) simulation outputs with a neural network. In sbi , this neural network is referred to as embedding_net . If an embedding_net is specified, the simulation outputs are passed through the embedding_net , whose outputs are then passed to the neural density estimator. The parameters of the embedding_net are learned together with the parameters of the neural density estimator. NB: only SNPE and SNRE methods can use an embedding_net to learn summary statistics from simulation outputs. SNLE does not offer this functionality since the simulation outputs \\(x\\) are the outputs of the neural density estimator. In the example that follows, we illustrate a situation where the data points generated by the simulator model are high-dimensional (32 by 32 images) and we use a convolutional neural network as summary statistics extractor. Note, you find the original version of this notebook at https://github.com/mackelab/sbi/blob/main/tutorials/05_embedding_net.ipynb in the sbi repository. First of all, we import all the packages required for running the tutorial import matplotlib.pyplot as plt import numpy as np import torch import torch.nn as nn import torch.nn.functional as F from sbi import utils from sbi import analysis from sbi import inference from sbi.inference import SNPE , simulate_for_sbi , prepare_for_sbi import numpy as np # set seed for numpy and torch seed = 42 np . random . seed ( seed ) torch . manual_seed ( seed ) <torch._C.Generator at 0x128d04990>","title":"Learning summary statistics with a neural net"},{"location":"tutorial/05_embedding_net/#the-simulator-model","text":"The simulator model that we consider has two parameters: \\(r\\) and \\(\\theta\\) . On each run, it generates 100 two-dimensional points centered around \\((r \\cos(\\theta), r \\sin(\\theta))\\) and perturbed by a Gaussian noise with variance 0.01. Instead of simply outputting the \\((x,y)\\) coordinates of each data point, the model generates a grayscale image of the scattered points with dimensions 32 by 32. This image is further perturbed by an uniform noise with values betweeen 0 and 0.2. The code below defines such model. def simulator_model ( parameter , return_points = False ): \"\"\" Simulator model with two-dimensional input parameter and 1024-dimensional output This simulator serves as a basic example for using a neural net for learning summary features. It has only two input parameters but generates high-dimensional output vectors. The data is generated as follows: (-) Input: parameter = [r, theta] (1) Generate 100 two-dimensional points centered around (r cos(theta),r sin(theta)) and perturbed by a Gaussian noise with variance 0.01 (2) Create a grayscale image I of the scattered points with dimensions 32 by 32 (3) Perturb I with an uniform noise with values betweeen 0 and 0.2 (-) Output: I Parameters ---------- parameter : array-like, shape (2) The two input parameters of the model, ordered as [r, theta] return_points : bool (default: False) Whether the simulator should return the coordinates of the simulated data points as well Returns ------- I: torch tensor, shape (1, 1024) Output flattened image (optional) points: array-like, shape (100, 2) Coordinates of the 2D simulated data points \"\"\" r = parameter [ 0 ] theta = parameter [ 1 ] sigma_points = 0.10 npoints = 100 points = [] for _ in range ( npoints ): x = r * np . cos ( theta ) + sigma_points * np . random . randn () y = r * np . sin ( theta ) + sigma_points * np . random . randn () points . append ([ x , y ]) points = np . array ( points ) nx = 32 ny = 32 sigma_image = 0.20 I = np . zeros (( nx , ny )) for point in points : pi = int (( point [ 0 ] - ( - 1 )) / (( + 1 ) - ( - 1 )) * nx ) pj = int (( point [ 1 ] - ( - 1 )) / (( + 1 ) - ( - 1 )) * ny ) if ( pi < nx ) and ( pj < ny ): I [ pi , pj ] = 1 I = I + sigma_image * np . random . rand ( nx , ny ) I = I . T I = I . reshape ( 1 , - 1 ) I = torch . tensor ( I , dtype = torch . get_default_dtype ()) if return_points : return I , points else : return I The figure below shows an example of the output of the simulator when \\(r = 0.70\\) and \\(\\theta = \\pi/4\\) # simulate samples true_parameter = torch . tensor ([ 0.70 , np . pi / 4 ]) x_observed , x_points = simulator_model ( true_parameter , return_points = True ) # plot the observation fig , ax = plt . subplots ( facecolor = 'white' , figsize = ( 11.15 , 5.61 ), ncols = 2 , constrained_layout = True ) circle = plt . Circle (( 0 , 0 ), 1.0 , color = 'k' , ls = '--' , lw = 0.8 , fill = False ) ax [ 0 ] . add_artist ( circle ) ax [ 0 ] . scatter ( x_points [:, 0 ], x_points [:, 1 ], s = 20 ) ax [ 0 ] . set_xlabel ( 'x' ) ax [ 0 ] . set_ylabel ( 'y' ) ax [ 0 ] . set_xlim ( - 1 , + 1 ) ax [ 0 ] . set_xticks ([ - 1 , 0.0 , + 1.0 ]) ax [ 0 ] . set_ylim ( - 1 , + 1 ) ax [ 0 ] . set_yticks ([ - 1 , 0.0 , + 1.0 ]) ax [ 0 ] . set_title ( r 'original simulated points with $r = 0.70$ and $\\theta = \\pi/4$' ) ax [ 1 ] . imshow ( x_observed . view ( 32 , 32 ), origin = 'lower' , cmap = 'gray' ) ax [ 1 ] . set_xticks ([]); ax [ 1 ] . set_yticks ([]) ax [ 1 ] . set_title ( 'noisy observed data (gray image with 32 x 32 pixels)' ) Text(0.5, 1.0, 'noisy observed data (gray image with 32 x 32 pixels)')","title":"The simulator model"},{"location":"tutorial/05_embedding_net/#defining-an-embedding_net","text":"An inference procedure applied to the output data from this simulator model determines the posterior distribution of \\(r\\) and \\(\\theta\\) given an observation of \\(x\\) , which lives in a 1024 dimensional space (32 x 32 = 1024). To avoid working directly on these high-dimensional vectors, one can use a convolutional neural network (CNN) that takes the 32x32 images as input and encodes them into 8-dimensional feature vectors. This CNN is trained along with the neural density estimator of the inference procedure and serves as an automatic summary statistics extractor. We define and instantiate the CNN as follows: class SummaryNet ( nn . Module ): def __init__ ( self ): super () . __init__ () # 2D convolutional layer self . conv1 = nn . Conv2d ( in_channels = 1 , out_channels = 6 , kernel_size = 5 , padding = 2 ) # Maxpool layer that reduces 32x32 image to 4x4 self . pool = nn . MaxPool2d ( kernel_size = 8 , stride = 8 ) # Fully connected layer taking as input the 6 flattened output arrays from the maxpooling layer self . fc = nn . Linear ( in_features = 6 * 4 * 4 , out_features = 8 ) def forward ( self , x ): x = x . view ( - 1 , 1 , 32 , 32 ) x = self . pool ( F . relu ( self . conv1 ( x ))) x = x . view ( - 1 , 6 * 4 * 4 ) x = F . relu ( self . fc ( x )) return x embedding_net = SummaryNet ()","title":"Defining an embedding_net"},{"location":"tutorial/05_embedding_net/#the-inference-procedure","text":"With the embedding_net defined and instantiated, we can follow the usual workflow of an inference procedure in sbi . The embedding_net object appears as an input argument when instantiating the neural density estimator with utils.posterior_nn . # set prior distribution for the parameters prior = utils . BoxUniform ( low = torch . tensor ([ 0.0 , 0.0 ]), high = torch . tensor ([ 1.0 , 2 * np . pi ])) # make a SBI-wrapper on the simulator object for compatibility simulator_wrapper , prior = prepare_for_sbi ( simulator_model , prior ) # instantiate the neural density estimator neural_posterior = utils . posterior_nn ( model = 'maf' , embedding_net = embedding_net , hidden_features = 10 , num_transforms = 2 ) # setup the inference procedure with the SNPE-C procedure inference = SNPE ( prior = prior , density_estimator = neural_posterior ) # run the inference procedure on one round and 10000 simulated data points theta , x = simulate_for_sbi ( simulator_wrapper , prior , num_simulations = 10000 ) density_estimator = inference . append_simulations ( theta , x ) . train () posterior = inference . build_posterior ( density_estimator ) HBox(children=(FloatProgress(value=0.0, description='Running 10000 simulations.', max=10000.0, style=ProgressS\u2026","title":"The inference procedure"},{"location":"tutorial/05_embedding_net/#visualizing-the-results","text":"We now generate 50000 samples of the posterior distribution of \\(r\\) and \\(\\theta\\) when observing an input data point \\(x\\) generated from the simulator model with \\(r = 0.70\\) and \\(\\theta = \\pi/4\\) . # generate posterior samples true_parameter = torch . tensor ([ 0.70 , np . pi / 4 ]) x_observed = simulator_model ( true_parameter ) samples = posterior . set_default_x ( x_observed ) . sample (( 50000 ,)) The figure below shows the statistics of the generated samples. # create the figure fig , ax = analysis . pairplot ( samples , points = true_parameter , labels = [ 'r' , r '$\\theta$' ], limits = [[ 0 , 1 ], [ 0 , 2 * np . pi ]], points_colors = 'r' , points_offdiag = { 'markersize' : 6 }, figsize = [ 7.5 , 6.4 ])","title":"Visualizing the results"},{"location":"tutorial/07_conditional_distributions/","text":"Analysing variability and compensation mechansims with conditional distributions \u00b6 A central advantage of sbi over parameter search methods such as genetic algorithms is that the posterior captures all models that can reproduce experimental data. This allows us to analyse whether parameters can be variable or have to be narrowly tuned, and to analyse compensation mechanisms between different parameters. See also Marder and Taylor, 2011 for further motivation to identify all models that capture experimental data. In this tutorial, we will show how one can use the posterior distribution to identify whether parameters can be variable or have to be finely tuned, and how we can use the posterior to find potential compensation mechanisms between model parameters. To investigate this, we will extract conditional distributions from the posterior inferred with sbi . Note, you can find the original version of this notebook at https://github.com/mackelab/sbi/blob/main/tutorials/07_conditional_distributions.ipynb in the sbi repository. Main syntax \u00b6 from sbi.analysis import conditional_pairplot , conditional_corrcoeff # Plot slices through posterior, i.e. conditionals. _ = conditional_pairplot ( density = posterior , condition = posterior . sample (( 1 ,)), limits = torch . tensor ([[ - 2. , 2. ], [ - 2. , 2. ]]), ) # Compute the matrix of correlation coefficients of the slices. cond_coeff_mat = conditional_corrcoeff ( density = posterior , condition = posterior . sample (( 1 ,)), limits = torch . tensor ([[ - 2. , 2. ], [ - 2. , 2. ]]), ) plt . imshow ( cond_coeff_mat , clim = [ - 1 , 1 ]) Analysing variability and compensation mechanisms in a toy example \u00b6 Below, we use a simple toy example to demonstrate the above described features. For an application of these features to a neuroscience problem, see figure 6 in Gon\u00e7alves, Lueckmann, Deistler et al., 2019 . from sbi import utils as utils from sbi.analysis import pairplot , conditional_pairplot , conditional_corrcoeff import torch import numpy as np import matplotlib.pyplot as plt from mpl_toolkits.mplot3d import Axes3D from matplotlib import animation , rc from IPython.display import HTML , Image _ = torch . manual_seed ( 0 ) Let\u2019s say we have used SNPE to obtain a posterior distribution over three parameters. In this tutorial, we just load the posterior from a file: from toy_posterior_for_07_cc import ExamplePosterior posterior = ExamplePosterior () First, we specify the experimental observation \\(x_o\\) at which we want to evaluate and sample the posterior \\(p(\\theta|x_o)\\) : x_o = torch . ones ( 1 , 20 ) # simulator output was 20-dimensional posterior . set_default_x ( x_o ) As always, we can inspect the posterior marginals with the pairplot() function: posterior_samples = posterior . sample (( 5000 ,)) fig , ax = pairplot ( samples = posterior_samples , limits = torch . tensor ([[ - 2. , 2. ]] * 3 ), upper = [ 'kde' ], diag = [ 'kde' ], figsize = ( 5 , 5 ) ) The 1D and 2D marginals of the posterior fill almost the entire parameter space! Also, the Pearson correlation coefficient matrix of the marginal shows rather weak interactions (low correlations): corr_matrix_marginal = np . corrcoef ( posterior_samples . T ) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 4 , 4 )) im = plt . imshow ( corr_matrix_marginal , clim = [ - 1 , 1 ], cmap = 'PiYG' ) _ = fig . colorbar ( im ) It might be tempting to conclude that the experimental data barely constrains our parameters and that almost all parameter combinations can reproduce the experimental data. As we will show below, this is not the case. Because our toy posterior has only three parameters, we can plot posterior samples in a 3D plot: rc ( 'animation' , html = 'html5' ) # First set up the figure, the axis, and the plot element we want to animate fig = plt . figure ( figsize = ( 6 , 6 )) ax = fig . add_subplot ( 111 , projection = '3d' ) ax . set_xlim (( - 2 , 2 )) ax . set_ylim (( - 2 , 2 )) def init (): line , = ax . plot ([], [], lw = 2 ) line . set_data ([], []) return ( line ,) def animate ( angle ): num_samples_vis = 1000 line = ax . scatter ( posterior_samples [: num_samples_vis , 0 ], posterior_samples [: num_samples_vis , 1 ], posterior_samples [: num_samples_vis , 2 ], zdir = 'z' , s = 15 , c = '#2171b5' , depthshade = False ) ax . view_init ( 20 , angle ) return ( line ,) anim = animation . FuncAnimation ( fig , animate , init_func = init , frames = range ( 0 , 360 , 5 ), interval = 150 , blit = True ) plt . close () HTML ( anim . to_html5_video ()) Your browser does not support the video tag. Clearly, the range of admissible parameters is constrained to a narrow region in parameter space, which had not been evident from the marginals. If the posterior has more than three dimensions, inspecting all dimensions at once will not be possible anymore. One way to still reveal structures in high-dimensional posteriors is to inspect 2D-slices through the posterior. In sbi , this can be done with the conditional_pairplot() function, which computes the conditional distributions within the posterior. We can slice (i.e. condition) the posterior at any location, given by the condition . In the plot below, for all upper diagonal plots, we keep all but two parameters constant at values sampled from the posterior, and inspect what combinations of the remaining two parameters can reproduce experimental data. For the plots on the diagonal (the 1D conditionals), we keep all but one parameter constant. condition = posterior . sample (( 1 ,)) _ = conditional_pairplot ( density = posterior , condition = condition , limits = torch . tensor ([[ - 2. , 2. ]] * 3 ), figsize = ( 5 , 5 ) ) This plot looks completely different from the marginals obtained with pairplot() . As it can be seen on the diagonal plots, if all parameters but one are kept constant, the remaining parameter has to be tuned to a narrow region in parameter space. In addition, the upper diagonal plots show strong correlations: deviations in one parameter can be compensated through changes in another parameter. We can summarize these correlations in a conditional correlation matrix, which computes the Pearson correlation coefficient of each of these pairwise plots. This matrix (below) shows strong correlations between many parameters, which can be interpreted as potential compensation mechansims: cond_coeff_mat = conditional_corrcoeff ( density = posterior , condition = condition , limits = torch . tensor ([[ - 2. , 2. ]] * 3 ), ) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 4 , 4 )) im = plt . imshow ( cond_coeff_mat , clim = [ - 1 , 1 ], cmap = 'PiYG' ) _ = fig . colorbar ( im ) So far, we have investigated the conditional distribution only at a specific condition sampled from the posterior. In many applications, it makes sense to repeat the above analyses with a different condition (another sample from the posterior), which can be interpreted as slicing the posterior at a different location. Note that conditional_corrcoeff() can directly compute the matrix for several conditions and then outputs the average over them. This can be done by passing a batch of \\(N\\) conditions as the condition argument.","title":"Conditional distributions"},{"location":"tutorial/07_conditional_distributions/#analysing-variability-and-compensation-mechansims-with-conditional-distributions","text":"A central advantage of sbi over parameter search methods such as genetic algorithms is that the posterior captures all models that can reproduce experimental data. This allows us to analyse whether parameters can be variable or have to be narrowly tuned, and to analyse compensation mechanisms between different parameters. See also Marder and Taylor, 2011 for further motivation to identify all models that capture experimental data. In this tutorial, we will show how one can use the posterior distribution to identify whether parameters can be variable or have to be finely tuned, and how we can use the posterior to find potential compensation mechanisms between model parameters. To investigate this, we will extract conditional distributions from the posterior inferred with sbi . Note, you can find the original version of this notebook at https://github.com/mackelab/sbi/blob/main/tutorials/07_conditional_distributions.ipynb in the sbi repository.","title":"Analysing variability and compensation mechansims with conditional distributions"},{"location":"tutorial/07_conditional_distributions/#main-syntax","text":"from sbi.analysis import conditional_pairplot , conditional_corrcoeff # Plot slices through posterior, i.e. conditionals. _ = conditional_pairplot ( density = posterior , condition = posterior . sample (( 1 ,)), limits = torch . tensor ([[ - 2. , 2. ], [ - 2. , 2. ]]), ) # Compute the matrix of correlation coefficients of the slices. cond_coeff_mat = conditional_corrcoeff ( density = posterior , condition = posterior . sample (( 1 ,)), limits = torch . tensor ([[ - 2. , 2. ], [ - 2. , 2. ]]), ) plt . imshow ( cond_coeff_mat , clim = [ - 1 , 1 ])","title":"Main syntax"},{"location":"tutorial/07_conditional_distributions/#analysing-variability-and-compensation-mechanisms-in-a-toy-example","text":"Below, we use a simple toy example to demonstrate the above described features. For an application of these features to a neuroscience problem, see figure 6 in Gon\u00e7alves, Lueckmann, Deistler et al., 2019 . from sbi import utils as utils from sbi.analysis import pairplot , conditional_pairplot , conditional_corrcoeff import torch import numpy as np import matplotlib.pyplot as plt from mpl_toolkits.mplot3d import Axes3D from matplotlib import animation , rc from IPython.display import HTML , Image _ = torch . manual_seed ( 0 ) Let\u2019s say we have used SNPE to obtain a posterior distribution over three parameters. In this tutorial, we just load the posterior from a file: from toy_posterior_for_07_cc import ExamplePosterior posterior = ExamplePosterior () First, we specify the experimental observation \\(x_o\\) at which we want to evaluate and sample the posterior \\(p(\\theta|x_o)\\) : x_o = torch . ones ( 1 , 20 ) # simulator output was 20-dimensional posterior . set_default_x ( x_o ) As always, we can inspect the posterior marginals with the pairplot() function: posterior_samples = posterior . sample (( 5000 ,)) fig , ax = pairplot ( samples = posterior_samples , limits = torch . tensor ([[ - 2. , 2. ]] * 3 ), upper = [ 'kde' ], diag = [ 'kde' ], figsize = ( 5 , 5 ) ) The 1D and 2D marginals of the posterior fill almost the entire parameter space! Also, the Pearson correlation coefficient matrix of the marginal shows rather weak interactions (low correlations): corr_matrix_marginal = np . corrcoef ( posterior_samples . T ) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 4 , 4 )) im = plt . imshow ( corr_matrix_marginal , clim = [ - 1 , 1 ], cmap = 'PiYG' ) _ = fig . colorbar ( im ) It might be tempting to conclude that the experimental data barely constrains our parameters and that almost all parameter combinations can reproduce the experimental data. As we will show below, this is not the case. Because our toy posterior has only three parameters, we can plot posterior samples in a 3D plot: rc ( 'animation' , html = 'html5' ) # First set up the figure, the axis, and the plot element we want to animate fig = plt . figure ( figsize = ( 6 , 6 )) ax = fig . add_subplot ( 111 , projection = '3d' ) ax . set_xlim (( - 2 , 2 )) ax . set_ylim (( - 2 , 2 )) def init (): line , = ax . plot ([], [], lw = 2 ) line . set_data ([], []) return ( line ,) def animate ( angle ): num_samples_vis = 1000 line = ax . scatter ( posterior_samples [: num_samples_vis , 0 ], posterior_samples [: num_samples_vis , 1 ], posterior_samples [: num_samples_vis , 2 ], zdir = 'z' , s = 15 , c = '#2171b5' , depthshade = False ) ax . view_init ( 20 , angle ) return ( line ,) anim = animation . FuncAnimation ( fig , animate , init_func = init , frames = range ( 0 , 360 , 5 ), interval = 150 , blit = True ) plt . close () HTML ( anim . to_html5_video ()) Your browser does not support the video tag. Clearly, the range of admissible parameters is constrained to a narrow region in parameter space, which had not been evident from the marginals. If the posterior has more than three dimensions, inspecting all dimensions at once will not be possible anymore. One way to still reveal structures in high-dimensional posteriors is to inspect 2D-slices through the posterior. In sbi , this can be done with the conditional_pairplot() function, which computes the conditional distributions within the posterior. We can slice (i.e. condition) the posterior at any location, given by the condition . In the plot below, for all upper diagonal plots, we keep all but two parameters constant at values sampled from the posterior, and inspect what combinations of the remaining two parameters can reproduce experimental data. For the plots on the diagonal (the 1D conditionals), we keep all but one parameter constant. condition = posterior . sample (( 1 ,)) _ = conditional_pairplot ( density = posterior , condition = condition , limits = torch . tensor ([[ - 2. , 2. ]] * 3 ), figsize = ( 5 , 5 ) ) This plot looks completely different from the marginals obtained with pairplot() . As it can be seen on the diagonal plots, if all parameters but one are kept constant, the remaining parameter has to be tuned to a narrow region in parameter space. In addition, the upper diagonal plots show strong correlations: deviations in one parameter can be compensated through changes in another parameter. We can summarize these correlations in a conditional correlation matrix, which computes the Pearson correlation coefficient of each of these pairwise plots. This matrix (below) shows strong correlations between many parameters, which can be interpreted as potential compensation mechansims: cond_coeff_mat = conditional_corrcoeff ( density = posterior , condition = condition , limits = torch . tensor ([[ - 2. , 2. ]] * 3 ), ) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 4 , 4 )) im = plt . imshow ( cond_coeff_mat , clim = [ - 1 , 1 ], cmap = 'PiYG' ) _ = fig . colorbar ( im ) So far, we have investigated the conditional distribution only at a specific condition sampled from the posterior. In many applications, it makes sense to repeat the above analyses with a different condition (another sample from the posterior), which can be interpreted as slicing the posterior at a different location. Note that conditional_corrcoeff() can directly compute the matrix for several conditions and then outputs the average over them. This can be done by passing a batch of \\(N\\) conditions as the condition argument.","title":"Analysing variability and compensation mechanisms in a toy example"},{"location":"tutorial/08_restriction_estimator/","text":"Efficient handling of invalid simulation outputs \u00b6 For many simulators, the output of the simulator can be ill-defined or it can have non-sensical values. For example, in neuroscience models, if a specific parameter set does not produce a spike, features such as the spike shape can not be computed. When using sbi , such simulations that have NaN or inf in their output are discarded during neural network training. This can lead to inefficetive use of simulation budget: we carry out many simulations, but a potentially large fraction of them is discarded. In this tutorial, we show how we can use sbi to learn regions in parameter space that produce valid simulation outputs, and thereby improve the sampling efficiency. The key idea of the method is to use a classifier to distinguish parameters that lead to valid simulations from regions that lead to invalid simulations. After we have obtained the region in parameter space that produes valid simulation outputs, we train the deep neural density estimator used in SNPE . The method was originally proposed in Lueckmann, Goncalves et al. 2017 and later used in Deistler et al. 2021 (in preparation). Main syntax \u00b6 from sbi.inference import SNPE from sbi.utils import RestrictionEstimator restriction_estimator = RestrictionEstimator ( prior = prior ) proposals = [ prior ] for r in range ( num_rounds ): theta , x = simulate_for_sbi ( simulator , proposals [ - 1 ], 1000 ) restriction_estimator . append_simulations ( theta , x ) if r < num_rounds - 1 : # training not needed in last round because classifier will not be used anymore. classifier = restriction_estimator . train () proposals . append ( restriction_estimator . restrict_prior ()) all_theta , all_x , _ = restriction_estimator . get_simulations () inference = SNPE ( prior = prior ) density_estimator = inference . append_simulations ( all_theta , all_x ) . train () posterior = inference . build_posterior () Further explanation in a toy example \u00b6 from sbi.inference import SNPE , simulate_for_sbi from sbi.utils import RestrictionEstimator , BoxUniform from sbi.analysis import pairplot import torch _ = torch . manual_seed ( 2 ) We will define a simulator with two parameters and two simulation outputs. The simulator produces NaN whenever the first parameter is below 0.0 . If it is above 0.0 the simulator simply perturbs the parameter set with Gaussian noise: def simulator ( theta ): perturbed_theta = theta + 0.5 * torch . randn ( 2 ) perturbed_theta [ theta [:, 0 ] < 0.0 ] = torch . as_tensor ([ float ( \"nan\" ), float ( \"nan\" )]) return perturbed_theta The prior is a uniform distribution in [-2, 2]: prior = BoxUniform ( - 2 * torch . ones ( 2 ), 2 * torch . ones ( 2 )) We then begin by drawing samples from the prior and simulating them. Looking at the simulation outputs, half of them contain NaN : theta , x = simulate_for_sbi ( simulator , prior , 1000 ) print ( \"Simulation outputs: \" , x ) HBox(children=(FloatProgress(value=0.0, description='Running 1000 simulations.', max=1000.0, style=ProgressSty\u2026 Simulation outputs: tensor([[ 0.0411, -0.5656], [ 0.0096, -1.0841], [ 1.2937, 0.9448], ..., [ nan, nan], [ nan, nan], [ 2.7940, 0.6461]]) The simulations that contain NaN are wasted, and we want to learn to \u201crestrict\u201d the prior such that it produces only valid simulation outputs. To do so, we set up the RestrictionEstimator : restriction_estimator = RestrictionEstimator ( prior = prior ) The RestrictionEstimator trains a classifier to distinguish parameters that lead to valid simulation outputs from parameters that lead to invalid simulation outputs restriction_estimator . append_simulations ( theta , x ) classifier = restriction_estimator . train () Training neural network. Epochs trained: 31 We can inspect the restricted_prior , i.e. the parameters that the classifier believes will lead to valid simulation outputs, with: restricted_prior = restriction_estimator . restrict_prior () samples = restricted_prior . sample (( 10_000 ,)) _ = pairplot ( samples , limits = [[ - 2 , 2 ], [ - 2 , 2 ]], fig_size = ( 4 , 4 )) The classifier rejected 51.1% of all samples. You will get a speed-up of 104.6%. /home/michael/Documents/sbi/sbi/utils/plot.py:197: UserWarning: You passed an argument `fig_size`. Since sbi v0.15.0, the argument should be called `figsize`. In future versions, `fig_size` will no longer be supported. warn( Indeed, parameter sets sampled from the restricted_prior always have a first parameter larger than 0.0 . These are the ones that produce valid simulation outputs (see our definition of the simulator above). We can then use the restricted_prior to generate more simulations. Almost all of them will have valid simulation outputs: new_theta , new_x = simulate_for_sbi ( simulator , restricted_prior , 1000 ) print ( \"Simulation outputs: \" , new_x ) The classifier rejected 51.0% of all samples. You will get a speed-up of 104.2%. HBox(children=(FloatProgress(value=0.0, description='Running 1000 simulations.', max=1000.0, style=ProgressSty\u2026 Simulation outputs: tensor([[0.8354, 1.2736], [0.7019, 0.8654], [3.1989, 0.4628], ..., [2.8816, 2.2535], [0.8272, 1.9156], [0.5428, 0.8408]]) We can now use all simulations and run SNPE as always: restriction_estimator . append_simulations ( new_theta , new_x ) # Gather the new simulations in the `restriction_estimator`. all_theta , all_x , _ = restriction_estimator . get_simulations () # Get all simulations run so far. inference = SNPE ( prior = prior ) density_estimator = inference . append_simulations ( all_theta , all_x ) . train () posterior = inference . build_posterior () posterior_samples = posterior . sample (( 10_000 ,), x = torch . ones ( 2 )) _ = pairplot ( posterior_samples , limits = [[ - 2 , 2 ], [ - 2 , 2 ]], fig_size = ( 3 , 3 )) WARNING:root:Found 523 NaN simulations and 0 Inf simulations. They will be excluded from training. Neural network successfully converged after 41 epochs. HBox(children=(FloatProgress(value=0.0, description='Drawing 10000 posterior samples', max=10000.0, style=Prog\u2026 /home/michael/Documents/sbi/sbi/utils/plot.py:197: UserWarning: You passed an argument `fig_size`. Since sbi v0.15.0, the argument should be called `figsize`. In future versions, `fig_size` will no longer be supported. warn( Further options for tuning the algorithm \u00b6 the whole procedure can be repeated many times (see the loop shown in \u201cMain syntax\u201d in this tutorial) the classifier is trained to be relatively conservative, i.e. it will try to be very sure that a specific parameter set can indeed not produce valid simulation outputs. If you are ok with the restricted prior potentially ignoring a small fraction of parameter sets that might have produced valid data, you can use restriction_estimator.restrict_prior(allowed_false_negatives=...) . The argument allowed_false_negatives sets the fraction of potentially ignored parameter sets. A higher value will lead to more valid simulations. By default, the algorithm considers simulations that have at least one NaN of inf as invalid . You can specify custom criterions with RestrictionEstimator(decision_criterion=...) For differencese between the RestrictionEstimator and multi-round inference , please see Deistler et al. 2021 (in preparation).","title":"Efficient handling of invalid simulation outputs"},{"location":"tutorial/08_restriction_estimator/#efficient-handling-of-invalid-simulation-outputs","text":"For many simulators, the output of the simulator can be ill-defined or it can have non-sensical values. For example, in neuroscience models, if a specific parameter set does not produce a spike, features such as the spike shape can not be computed. When using sbi , such simulations that have NaN or inf in their output are discarded during neural network training. This can lead to inefficetive use of simulation budget: we carry out many simulations, but a potentially large fraction of them is discarded. In this tutorial, we show how we can use sbi to learn regions in parameter space that produce valid simulation outputs, and thereby improve the sampling efficiency. The key idea of the method is to use a classifier to distinguish parameters that lead to valid simulations from regions that lead to invalid simulations. After we have obtained the region in parameter space that produes valid simulation outputs, we train the deep neural density estimator used in SNPE . The method was originally proposed in Lueckmann, Goncalves et al. 2017 and later used in Deistler et al. 2021 (in preparation).","title":"Efficient handling of invalid simulation outputs"},{"location":"tutorial/08_restriction_estimator/#main-syntax","text":"from sbi.inference import SNPE from sbi.utils import RestrictionEstimator restriction_estimator = RestrictionEstimator ( prior = prior ) proposals = [ prior ] for r in range ( num_rounds ): theta , x = simulate_for_sbi ( simulator , proposals [ - 1 ], 1000 ) restriction_estimator . append_simulations ( theta , x ) if r < num_rounds - 1 : # training not needed in last round because classifier will not be used anymore. classifier = restriction_estimator . train () proposals . append ( restriction_estimator . restrict_prior ()) all_theta , all_x , _ = restriction_estimator . get_simulations () inference = SNPE ( prior = prior ) density_estimator = inference . append_simulations ( all_theta , all_x ) . train () posterior = inference . build_posterior ()","title":"Main syntax"},{"location":"tutorial/08_restriction_estimator/#further-explanation-in-a-toy-example","text":"from sbi.inference import SNPE , simulate_for_sbi from sbi.utils import RestrictionEstimator , BoxUniform from sbi.analysis import pairplot import torch _ = torch . manual_seed ( 2 ) We will define a simulator with two parameters and two simulation outputs. The simulator produces NaN whenever the first parameter is below 0.0 . If it is above 0.0 the simulator simply perturbs the parameter set with Gaussian noise: def simulator ( theta ): perturbed_theta = theta + 0.5 * torch . randn ( 2 ) perturbed_theta [ theta [:, 0 ] < 0.0 ] = torch . as_tensor ([ float ( \"nan\" ), float ( \"nan\" )]) return perturbed_theta The prior is a uniform distribution in [-2, 2]: prior = BoxUniform ( - 2 * torch . ones ( 2 ), 2 * torch . ones ( 2 )) We then begin by drawing samples from the prior and simulating them. Looking at the simulation outputs, half of them contain NaN : theta , x = simulate_for_sbi ( simulator , prior , 1000 ) print ( \"Simulation outputs: \" , x ) HBox(children=(FloatProgress(value=0.0, description='Running 1000 simulations.', max=1000.0, style=ProgressSty\u2026 Simulation outputs: tensor([[ 0.0411, -0.5656], [ 0.0096, -1.0841], [ 1.2937, 0.9448], ..., [ nan, nan], [ nan, nan], [ 2.7940, 0.6461]]) The simulations that contain NaN are wasted, and we want to learn to \u201crestrict\u201d the prior such that it produces only valid simulation outputs. To do so, we set up the RestrictionEstimator : restriction_estimator = RestrictionEstimator ( prior = prior ) The RestrictionEstimator trains a classifier to distinguish parameters that lead to valid simulation outputs from parameters that lead to invalid simulation outputs restriction_estimator . append_simulations ( theta , x ) classifier = restriction_estimator . train () Training neural network. Epochs trained: 31 We can inspect the restricted_prior , i.e. the parameters that the classifier believes will lead to valid simulation outputs, with: restricted_prior = restriction_estimator . restrict_prior () samples = restricted_prior . sample (( 10_000 ,)) _ = pairplot ( samples , limits = [[ - 2 , 2 ], [ - 2 , 2 ]], fig_size = ( 4 , 4 )) The classifier rejected 51.1% of all samples. You will get a speed-up of 104.6%. /home/michael/Documents/sbi/sbi/utils/plot.py:197: UserWarning: You passed an argument `fig_size`. Since sbi v0.15.0, the argument should be called `figsize`. In future versions, `fig_size` will no longer be supported. warn( Indeed, parameter sets sampled from the restricted_prior always have a first parameter larger than 0.0 . These are the ones that produce valid simulation outputs (see our definition of the simulator above). We can then use the restricted_prior to generate more simulations. Almost all of them will have valid simulation outputs: new_theta , new_x = simulate_for_sbi ( simulator , restricted_prior , 1000 ) print ( \"Simulation outputs: \" , new_x ) The classifier rejected 51.0% of all samples. You will get a speed-up of 104.2%. HBox(children=(FloatProgress(value=0.0, description='Running 1000 simulations.', max=1000.0, style=ProgressSty\u2026 Simulation outputs: tensor([[0.8354, 1.2736], [0.7019, 0.8654], [3.1989, 0.4628], ..., [2.8816, 2.2535], [0.8272, 1.9156], [0.5428, 0.8408]]) We can now use all simulations and run SNPE as always: restriction_estimator . append_simulations ( new_theta , new_x ) # Gather the new simulations in the `restriction_estimator`. all_theta , all_x , _ = restriction_estimator . get_simulations () # Get all simulations run so far. inference = SNPE ( prior = prior ) density_estimator = inference . append_simulations ( all_theta , all_x ) . train () posterior = inference . build_posterior () posterior_samples = posterior . sample (( 10_000 ,), x = torch . ones ( 2 )) _ = pairplot ( posterior_samples , limits = [[ - 2 , 2 ], [ - 2 , 2 ]], fig_size = ( 3 , 3 )) WARNING:root:Found 523 NaN simulations and 0 Inf simulations. They will be excluded from training. Neural network successfully converged after 41 epochs. HBox(children=(FloatProgress(value=0.0, description='Drawing 10000 posterior samples', max=10000.0, style=Prog\u2026 /home/michael/Documents/sbi/sbi/utils/plot.py:197: UserWarning: You passed an argument `fig_size`. Since sbi v0.15.0, the argument should be called `figsize`. In future versions, `fig_size` will no longer be supported. warn(","title":"Further explanation in a toy example"},{"location":"tutorial/08_restriction_estimator/#further-options-for-tuning-the-algorithm","text":"the whole procedure can be repeated many times (see the loop shown in \u201cMain syntax\u201d in this tutorial) the classifier is trained to be relatively conservative, i.e. it will try to be very sure that a specific parameter set can indeed not produce valid simulation outputs. If you are ok with the restricted prior potentially ignoring a small fraction of parameter sets that might have produced valid data, you can use restriction_estimator.restrict_prior(allowed_false_negatives=...) . The argument allowed_false_negatives sets the fraction of potentially ignored parameter sets. A higher value will lead to more valid simulations. By default, the algorithm considers simulations that have at least one NaN of inf as invalid . You can specify custom criterions with RestrictionEstimator(decision_criterion=...) For differencese between the RestrictionEstimator and multi-round inference , please see Deistler et al. 2021 (in preparation).","title":"Further options for tuning the algorithm"},{"location":"tutorial/09_sensitivity_analysis/","text":"Active subspaces for sensitivity analysis \u00b6 A standard method to analyse dynamical systems such as models of neural dynamics is to use a sensitivity analysis. We can use the posterior obtained with sbi , to perform such analyses. Main syntax \u00b6 from sbi.analysis import ActiveSubspace sensitivity = ActiveSubspace ( posterior . set_default_x ( x_o )) e_vals , e_vecs = sensitivity . find_directions ( posterior_log_prob_as_property = True ) projected_data = sensitivity . project ( theta_project , num_dimensions = 1 ) Example and further explanation \u00b6 import torch from torch.distributions import MultivariateNormal from sbi.analysis import ActiveSubspace , pairplot from sbi.simulators import linear_gaussian from sbi.inference import simulate_for_sbi , infer _ = torch . manual_seed ( 0 ) Let\u2019s define a simple Gaussian toy example: prior = MultivariateNormal ( 0.0 * torch . ones ( 2 ), 2 * torch . eye ( 2 )) def simulator ( theta ): return linear_gaussian ( theta , - 0.8 * torch . ones ( 2 ), torch . tensor ([[ 1.0 , 0.98 ], [ 0.98 , 1.0 ]])) posterior = infer ( simulator , prior , num_simulations = 2000 , method = 'SNPE' ) . set_default_x ( torch . zeros ( 2 )) HBox(children=(FloatProgress(value=0.0, description='Running 2000 simulations.', max=2000.0, style=ProgressSty\u2026 Neural network successfully converged after 96 epochs. posterior_samples = posterior . sample (( 2000 ,)) HBox(children=(FloatProgress(value=0.0, description='Drawing 2000 posterior samples', max=2000.0, style=Progre\u2026 _ = pairplot ( posterior_samples , limits = [[ - 3 , 3 ], [ - 3 , 3 ]], figsize = ( 4 , 4 )) When performing a sensitivity analysis on this model, we would expect that there is one direction that is less sensitive (from bottom left to top right, along the vector [1, 1]) and one direction that is more sensitive (from top left to bottom right, along [1, -1]). We can recover these directions with the ActiveSubspace module in sbi . sensitivity = ActiveSubspace ( posterior ) e_vals , e_vecs = sensitivity . find_directions ( posterior_log_prob_as_property = True ) HBox(children=(FloatProgress(value=0.0, description='Drawing 1000 posterior samples', max=1000.0, style=Progre\u2026 The method .find_active() returns eigenvalues and the corresponding eigenvectors. It does so by computing the matrix: \\(M = \\mathbb{E}_{p(\\theta|x_o)}[\\nabla_{\\theta}p(\\theta|x_o)^T \\nabla_{\\theta}p(\\theta|x_o)\\) ] It then does an eigendecomposition: \\(M = Q \\Lambda Q^{-1}\\) A strong eigenvalue indicates that the gradient of the posterior density is large, i.e. the system output is sensitive to changes along the direction of the corresponding eigenvector (or active ). The eigenvalue corresponding to the vector [0.68, -0.73] is much larger than the eigenvalue of [0.73, 0.67] . This matches the intuition we developed above. print ( \"Eigenvalues: \\n \" , e_vals , \" \\n \" ) print ( \"Eigenvectors: \\n \" , e_vecs ) Eigenvalues: tensor([2.9249e-06, 8.7961e-05]) Eigenvectors: tensor([[ 0.7343, 0.6789], [ 0.6789, -0.7343]]) Lastly, we can project the data into the active dimensions. In this case, we will just use one active dimension: projected_data = sensitivity . project ( posterior_samples , num_dimensions = 1 ) Some technical details \u00b6 The gradients and thus the eigenvectors are computed in z-scored space. The mean and standard deviation are computed w.r.t. the prior distribution. Thus, the gradients (and thus the eigenvales) reflect changes on the scale of the prior. The expected value used to compute the matrix \\(M\\) is estimated using 1000 posterior samples. This value can be set with the .find_active(num_monte_carlo_samples=...) variable. How does this relate to Principal Component Analysis (PCA)? In the example above, the results of PCA would be very similar. However, there are two main differences to PCA: First, PCA ignores local changes in the posterior, whereas the active subspace can change a lot (since it computes the gradient, which is a local quantity). Second, active subspaces can be used characterize the sensitivity of any other quantity w.r.t. circuit parameters. This is outlined below: Computing the sensitivity of a specific summary statistic \u00b6 Above, we have shown how to identify directions along which the posterior probability changes rapidly. Notably, the posterior probability reflects how consistent a specific parameter set is with all summary statistics, i.e. the entire \\(x_o\\) . Sometimes, we might be interested in investigating how a specific features is influenced by the parameters. This feature could be one of the values of \\(x_o\\) , but it could also be a different property. As a neuroscience example, in Deistler et al. 2021, we obtained the posterior distribution given burst durations and delays between bursts. After having obtained the posterior, we then wanted to analyse the sensitivity of metabolic cost w.r.t. circuit parameters. The framework we presented above can easily be extended to study such questions. prior = MultivariateNormal ( 0.0 * torch . ones ( 2 ), 2 * torch . eye ( 2 )) def simulator ( theta ): return linear_gaussian ( theta , - 0.8 * torch . ones ( 2 ), torch . eye ( 2 )) posterior = infer ( simulator , prior , num_simulations = 2000 , method = 'SNPE' ) . set_default_x ( torch . zeros ( 2 )) HBox(children=(FloatProgress(value=0.0, description='Running 2000 simulations.', max=2000.0, style=ProgressSty\u2026 Neural network successfully converged after 33 epochs. _ = pairplot ( posterior . sample (( 10_000 ,)), limits = [[ - 3 , 3 ], [ - 3 , 3 ]], figsize = ( 4 , 4 )) HBox(children=(FloatProgress(value=0.0, description='Drawing 10000 posterior samples', max=10000.0, style=Prog\u2026 sensitivity = ActiveSubspace ( posterior ) This time, we begin by drawing samples from the posterior and then computing the desired property for each of the samples (i.e. you will probably have to run simulations for each theta and extract the property from the simulation output). As an example, we assume that the property is just the cube of the first dimension of the simulation output: theta , x = simulate_for_sbi ( simulator , posterior , 5000 ) property_ = x [:, : 1 ] ** 3 # E.g. metabolic cost. HBox(children=(FloatProgress(value=0.0, description='Drawing 5000 posterior samples', max=5000.0, style=Progre\u2026 HBox(children=(FloatProgress(value=0.0, description='Running 5000 simulations.', max=5000.0, style=ProgressSty\u2026 To investigate the sensitivity of a given parameter, we train a neural network to predict the property_ from the parameters and then analyse the neural network as above: \\(M = \\mathbb{E}_{p(\\theta|x_o)}[\\nabla_{\\theta}f(\\theta)^T \\nabla_{\\theta}f(\\theta)\\) ] where \\(f(\\cdot)\\) is the trained neural network. _ = sensitivity . add_property ( theta , property_ ) . train () e_vals , e_vecs = sensitivity . find_directions () Training neural network. Epochs trained: 73 HBox(children=(FloatProgress(value=0.0, description='Drawing 1000 posterior samples', max=1000.0, style=Progre\u2026 print ( \"Eigenvalues: \\n \" , e_vals , \" \\n \" ) print ( \"Eigenvectors: \\n \" , e_vecs ) Eigenvalues: tensor([2.3021e-06, 6.5982e-05]) Eigenvectors: tensor([[-0.0191, 0.9998], [ 0.9998, 0.0191]]) As we can see, one of the eigenvalues is much smaller than the other one. The larger eigenvalue represents (approximately) the vector [1.0, 0.0] . This makes sense, because only the property_ is influenced only by the first output which, in turn, is influenced only by the first parameter.","title":"Active subspaces for sensitivity analysis"},{"location":"tutorial/09_sensitivity_analysis/#active-subspaces-for-sensitivity-analysis","text":"A standard method to analyse dynamical systems such as models of neural dynamics is to use a sensitivity analysis. We can use the posterior obtained with sbi , to perform such analyses.","title":"Active subspaces for sensitivity analysis"},{"location":"tutorial/09_sensitivity_analysis/#main-syntax","text":"from sbi.analysis import ActiveSubspace sensitivity = ActiveSubspace ( posterior . set_default_x ( x_o )) e_vals , e_vecs = sensitivity . find_directions ( posterior_log_prob_as_property = True ) projected_data = sensitivity . project ( theta_project , num_dimensions = 1 )","title":"Main syntax"},{"location":"tutorial/09_sensitivity_analysis/#example-and-further-explanation","text":"import torch from torch.distributions import MultivariateNormal from sbi.analysis import ActiveSubspace , pairplot from sbi.simulators import linear_gaussian from sbi.inference import simulate_for_sbi , infer _ = torch . manual_seed ( 0 ) Let\u2019s define a simple Gaussian toy example: prior = MultivariateNormal ( 0.0 * torch . ones ( 2 ), 2 * torch . eye ( 2 )) def simulator ( theta ): return linear_gaussian ( theta , - 0.8 * torch . ones ( 2 ), torch . tensor ([[ 1.0 , 0.98 ], [ 0.98 , 1.0 ]])) posterior = infer ( simulator , prior , num_simulations = 2000 , method = 'SNPE' ) . set_default_x ( torch . zeros ( 2 )) HBox(children=(FloatProgress(value=0.0, description='Running 2000 simulations.', max=2000.0, style=ProgressSty\u2026 Neural network successfully converged after 96 epochs. posterior_samples = posterior . sample (( 2000 ,)) HBox(children=(FloatProgress(value=0.0, description='Drawing 2000 posterior samples', max=2000.0, style=Progre\u2026 _ = pairplot ( posterior_samples , limits = [[ - 3 , 3 ], [ - 3 , 3 ]], figsize = ( 4 , 4 )) When performing a sensitivity analysis on this model, we would expect that there is one direction that is less sensitive (from bottom left to top right, along the vector [1, 1]) and one direction that is more sensitive (from top left to bottom right, along [1, -1]). We can recover these directions with the ActiveSubspace module in sbi . sensitivity = ActiveSubspace ( posterior ) e_vals , e_vecs = sensitivity . find_directions ( posterior_log_prob_as_property = True ) HBox(children=(FloatProgress(value=0.0, description='Drawing 1000 posterior samples', max=1000.0, style=Progre\u2026 The method .find_active() returns eigenvalues and the corresponding eigenvectors. It does so by computing the matrix: \\(M = \\mathbb{E}_{p(\\theta|x_o)}[\\nabla_{\\theta}p(\\theta|x_o)^T \\nabla_{\\theta}p(\\theta|x_o)\\) ] It then does an eigendecomposition: \\(M = Q \\Lambda Q^{-1}\\) A strong eigenvalue indicates that the gradient of the posterior density is large, i.e. the system output is sensitive to changes along the direction of the corresponding eigenvector (or active ). The eigenvalue corresponding to the vector [0.68, -0.73] is much larger than the eigenvalue of [0.73, 0.67] . This matches the intuition we developed above. print ( \"Eigenvalues: \\n \" , e_vals , \" \\n \" ) print ( \"Eigenvectors: \\n \" , e_vecs ) Eigenvalues: tensor([2.9249e-06, 8.7961e-05]) Eigenvectors: tensor([[ 0.7343, 0.6789], [ 0.6789, -0.7343]]) Lastly, we can project the data into the active dimensions. In this case, we will just use one active dimension: projected_data = sensitivity . project ( posterior_samples , num_dimensions = 1 )","title":"Example and further explanation"},{"location":"tutorial/09_sensitivity_analysis/#some-technical-details","text":"The gradients and thus the eigenvectors are computed in z-scored space. The mean and standard deviation are computed w.r.t. the prior distribution. Thus, the gradients (and thus the eigenvales) reflect changes on the scale of the prior. The expected value used to compute the matrix \\(M\\) is estimated using 1000 posterior samples. This value can be set with the .find_active(num_monte_carlo_samples=...) variable. How does this relate to Principal Component Analysis (PCA)? In the example above, the results of PCA would be very similar. However, there are two main differences to PCA: First, PCA ignores local changes in the posterior, whereas the active subspace can change a lot (since it computes the gradient, which is a local quantity). Second, active subspaces can be used characterize the sensitivity of any other quantity w.r.t. circuit parameters. This is outlined below:","title":"Some technical details"},{"location":"tutorial/09_sensitivity_analysis/#computing-the-sensitivity-of-a-specific-summary-statistic","text":"Above, we have shown how to identify directions along which the posterior probability changes rapidly. Notably, the posterior probability reflects how consistent a specific parameter set is with all summary statistics, i.e. the entire \\(x_o\\) . Sometimes, we might be interested in investigating how a specific features is influenced by the parameters. This feature could be one of the values of \\(x_o\\) , but it could also be a different property. As a neuroscience example, in Deistler et al. 2021, we obtained the posterior distribution given burst durations and delays between bursts. After having obtained the posterior, we then wanted to analyse the sensitivity of metabolic cost w.r.t. circuit parameters. The framework we presented above can easily be extended to study such questions. prior = MultivariateNormal ( 0.0 * torch . ones ( 2 ), 2 * torch . eye ( 2 )) def simulator ( theta ): return linear_gaussian ( theta , - 0.8 * torch . ones ( 2 ), torch . eye ( 2 )) posterior = infer ( simulator , prior , num_simulations = 2000 , method = 'SNPE' ) . set_default_x ( torch . zeros ( 2 )) HBox(children=(FloatProgress(value=0.0, description='Running 2000 simulations.', max=2000.0, style=ProgressSty\u2026 Neural network successfully converged after 33 epochs. _ = pairplot ( posterior . sample (( 10_000 ,)), limits = [[ - 3 , 3 ], [ - 3 , 3 ]], figsize = ( 4 , 4 )) HBox(children=(FloatProgress(value=0.0, description='Drawing 10000 posterior samples', max=10000.0, style=Prog\u2026 sensitivity = ActiveSubspace ( posterior ) This time, we begin by drawing samples from the posterior and then computing the desired property for each of the samples (i.e. you will probably have to run simulations for each theta and extract the property from the simulation output). As an example, we assume that the property is just the cube of the first dimension of the simulation output: theta , x = simulate_for_sbi ( simulator , posterior , 5000 ) property_ = x [:, : 1 ] ** 3 # E.g. metabolic cost. HBox(children=(FloatProgress(value=0.0, description='Drawing 5000 posterior samples', max=5000.0, style=Progre\u2026 HBox(children=(FloatProgress(value=0.0, description='Running 5000 simulations.', max=5000.0, style=ProgressSty\u2026 To investigate the sensitivity of a given parameter, we train a neural network to predict the property_ from the parameters and then analyse the neural network as above: \\(M = \\mathbb{E}_{p(\\theta|x_o)}[\\nabla_{\\theta}f(\\theta)^T \\nabla_{\\theta}f(\\theta)\\) ] where \\(f(\\cdot)\\) is the trained neural network. _ = sensitivity . add_property ( theta , property_ ) . train () e_vals , e_vecs = sensitivity . find_directions () Training neural network. Epochs trained: 73 HBox(children=(FloatProgress(value=0.0, description='Drawing 1000 posterior samples', max=1000.0, style=Progre\u2026 print ( \"Eigenvalues: \\n \" , e_vals , \" \\n \" ) print ( \"Eigenvectors: \\n \" , e_vecs ) Eigenvalues: tensor([2.3021e-06, 6.5982e-05]) Eigenvectors: tensor([[-0.0191, 0.9998], [ 0.9998, 0.0191]]) As we can see, one of the eigenvalues is much smaller than the other one. The larger eigenvalue represents (approximately) the vector [1.0, 0.0] . This makes sense, because only the property_ is influenced only by the first output which, in turn, is influenced only by the first parameter.","title":"Computing the sensitivity of a specific summary statistic"},{"location":"tutorial/10_crafting_summary_statistics/","text":"Crafting summary statistics \u00b6 Many simulators produce outputs that are high-dimesional. For example, a simulator might generate a time series or an image. In a previous tutorial , we discussed how a neural networks can be used to learn summary statistics from such data. In this notebook, we will instead focus on hand-crafting summary statistics. We demonstrate that the choice of summary statistics can be crucial for the performance of the inference algorithm. import numpy as np import torch import matplotlib.pyplot as plt import matplotlib as mpl # sbi import sbi.utils as utils from sbi.inference.base import infer from sbi.inference import SNPE , prepare_for_sbi , simulate_for_sbi from sbi.utils.get_nn_models import posterior_nn from sbi.analysis import pairplot # remove top and right axis from plots mpl . rcParams [ 'axes.spines.right' ] = False mpl . rcParams [ 'axes.spines.top' ] = False This notebook is not intended to provide a one-fits-all approach. In fact it argues against this: it argues for the user to carefully construct their summary statistics to (i) further help the user understand his observed data, (ii) help them understand exactly what they want the model to recover from the observation and (iii) help the inference framework itself. Example 1: The quadratic function \u00b6 Assume we have a simulator that is given by a quadratic function: \\(x(t) = a\\cdot t^2 + b\\cdot t + c + \\epsilon\\) , where \\(\\epsilon\\) is Gaussian observation noise and \\(\\theta = \\{a, b, c\\}\\) are the parameters. Given an observed quadratic function \\(x_o\\) , we would like to recover the posterior over parameters \\(a_o\\) , \\(b_o\\) and \\(c_o\\) . 1.1 Prior over parameters \u00b6 First we define a prior distribution over parameters \\(a\\) , \\(b\\) and \\(c\\) . Here, we use a uniform prior for \\(a\\) , \\(b\\) and \\(c\\) to go from \\(-1\\) to \\(1\\) . prior_min = [ - 1 , - 1 , - 1 ] prior_max = [ 1 , 1 , 1 ] prior = utils . torchutils . BoxUniform ( low = torch . as_tensor ( prior_min ), high = torch . as_tensor ( prior_max ) ) 1.2 Simulator \u00b6 Defining some helper functions first: def create_t_x ( theta , seed = None ): ''' Return an t, x array for plotting based on params ''' if theta . ndim == 1 : theta = theta [ np . newaxis ,:] if seed is not None : rng = np . random . RandomState ( seed ) else : rng = np . random . RandomState () t = np . linspace ( - 1 , 1 , 200 ) ts = np . repeat ( t [:, np . newaxis ], theta . shape [ 0 ], axis = 1 ) x = theta [:, 0 ] * ts ** 2 + theta [:, 1 ] * ts + theta [:, 2 ] + 0.01 * rng . randn ( ts . shape [ 0 ], theta . shape [ 0 ]) return t , x def eval ( theta , t , seed = None ): ''' Evaluate the quadratic function at 't' ''' if theta . ndim == 1 : theta = theta [ np . newaxis ,:] if seed is not None : rng = np . random . RandomState ( seed ) else : rng = np . random . RandomState () return theta [:, 0 ] * t ** 2 + theta [:, 1 ] * t + theta [:, 2 ] + 0.01 * rng . randn ( 1 ) In this example, we generate the observation \\(x_o\\) from parameters \\(\\theta_o=(a_o, b_o, c_o)=(0.3, -0.2, -0.1)\\) . The observation as follows. theta_o = np . array ([ 0.3 , - 0.2 , - 0.1 ]) t , x = create_t_x ( theta_o ) plt . plot ( t , x , 'k' ) [<matplotlib.lines.Line2D at 0x7fbfda9d6e50>] 1.3 Summary statistics \u00b6 We will compare two methods for defining summary statistics. One method uses three summary statistics which are function evaluations at three points in time. The other method uses a single summary statistic: the mean squared error between the observed and the simulated trace. In the second case, one then tries to obtain the posterior \\(p(\\theta | 0)\\) , i.e. the error being zero. These two methods are implemented below: \\(\\textbf{get_3_values()}\\) returns 3 function evaluations at \\(x=-0.5, x=0\\) and \\(x=0.75\\) . \\(\\textbf{get_MSE()}\\) returns the mean squared error between true and a quadratic function corresponding to a prior distributions sample. def get_3_values ( theta , seed = None ): ''' Return 3 'y' values corresponding to x=-0.5,0,0.75 as summary statistic vector ''' return np . array ([ eval ( theta , - 0.5 , seed = seed ), eval ( theta , 0 , seed = seed ), eval ( theta , 0.75 , seed = seed )]) . T def get_MSE ( theta , theta_o , seed = None ): ''' Return the mean-squared error (MSE) i.e. Euclidean distance from the observation function ''' _ , y = create_t_x ( theta_o , seed = seed ) # truth _ , y_ = create_t_x ( theta , seed = seed ) # simulations return np . mean ( np . square ( y_ - y ), axis = 0 , keepdims = True ) . T # MSE Let\u2019s try a couple of samples from our prior and see their summary statistics. Notice that these indeed change in small amounts every time you rerun it due to the noise, except if you set the seed. 1.4 Simulating data \u00b6 Let us see various plots of prior samples and their summary statistics versus the truth, i.e. our artificial observation. t , x_truth = create_t_x ( theta_o ) plt . plot ( t , x_truth , 'k' , zorder = 1 , label = 'truth' ) n_samples = 100 theta = prior . sample (( n_samples ,)) t , x = create_t_x ( theta . numpy ()) plt . plot ( t , x , 'grey' , zorder = 0 ); plt . legend () <matplotlib.legend.Legend at 0x7fbfd88724c0> In summary, we defined reasonable summary statistics and, a priori, there might be an appararent reason why one method would be better than another. When we do inference, we\u2019d like our posterior to focus around parameter samples that have their simulated MSE very close to 0 (i.e. the truth MSE summary statistic) or their 3 extracted \\((t, x)\\) coordinates to be the truthful ones. 1.5 Inference \u00b6 1.5.1 Using the MSE \u00b6 Let\u2019s see if we can use the MSE to recover the true observation parameters \\(\\theta_o=(a_0,b_0,c_0)\\) . theta = prior . sample (( 1000 ,)) x = get_MSE ( theta . numpy (), theta_o ) theta = torch . as_tensor ( theta , dtype = torch . float32 ) x = torch . as_tensor ( x , dtype = torch . float32 ) inference = SNPE ( prior ) _ = inference . append_simulations ( theta , x ) . train () posterior = inference . build_posterior () Neural network successfully converged after 124 epochs. Now that we\u2019ve build the posterior as such, we can see how likely it finds certain parameters given that we tell it that we\u2019ve observed a certain summary statistic (in this case the MSE). We can then sample from it. x_o = torch . as_tensor ([[ 0.0 ,]]) theta_p = posterior . sample (( 10000 ,), x = x_o ) Drawing 10000 posterior samples: 0%| | 0/10000 [00:00<?, ?it/s] fig , axes = pairplot ( theta_p , limits = list ( zip ( prior_min , prior_max )), ticks = list ( zip ( prior_min , prior_max )), figsize = ( 7 , 7 ), labels = [ 'a' , 'b' , 'c' ], points_offdiag = { 'markersize' : 6 }, points_colors = 'r' , points = theta_o ); The posterior seems to pretty broad: i.e. it is not so certain about the \u2018true\u2019 parameters (here showcased in red). x_o_t , x_o_x = create_t_x ( theta_o ) plt . plot ( x_o_t , x_o_x , 'k' , zorder = 1 , label = 'truth' ) theta_p = posterior . sample (( 10 ,), x = x_o ) x_t , x_x = create_t_x ( theta_p . numpy ()) plt . plot ( x_t , x_x , 'grey' , zorder = 0 ) plt . legend () Drawing 10 posterior samples: 0%| | 0/10 [00:00<?, ?it/s] <matplotlib.legend.Legend at 0x7fbfd4241640> The functions are a bit closer to the observation than prior samples, but many posterior samples generate activity that is very far off from the observation. We would expect sbi do better on such a simple example. So what\u2019s going on? Do we need more simulations? Feel free to try, but below we will show that one can use the same number of simulation samples with different summary statistics and do much better. 1.5.2 Using 3 coordinates as summary statistics \u00b6 x = get_3_values ( theta . numpy ()) x = torch . as_tensor ( x , dtype = torch . float32 ) inference = SNPE ( prior ) _ = inference . append_simulations ( theta , x ) . train () posterior = inference . build_posterior () Neural network successfully converged after 107 epochs. The observation is now given by the values of the observed trace at three different coordinates: x_o = torch . as_tensor ( get_3_values ( theta_o ), dtype = float ) theta_p = posterior . sample (( 10000 ,), x = x_o ) fig , axes = pairplot ( theta_p , limits = list ( zip ( prior_min , prior_max )), ticks = list ( zip ( prior_min , prior_max )), figsize = ( 7 , 7 ), labels = [ 'a' , 'b' , 'c' ], points_offdiag = { 'markersize' : 6 }, points_colors = 'r' , points = theta_o ); Drawing 10000 posterior samples: 0%| | 0/10000 [00:00<?, ?it/s] x_o_x , x_o_y = create_x_y ( theta_o ) plt . plot ( x_o_x , x_o_y , 'k' , zorder = 1 , label = 'truth' ) theta_p = posterior . sample (( 100 ,), x = x_o ) ind_10_highest = np . argsort ( np . array ( posterior . log_prob ( theta = theta_p , x = x_o )))[ - 10 :] theta_p_considered = theta_p [ ind_10_highest ,:] x_x , x_y = create_x_y ( theta_p_considered . numpy ()) plt . plot ( x_x , x_y , 'grey' , zorder = 0 ); plt . legend () HBox(children=(FloatProgress(value=0.0, description='Drawing 100 posterior samples', style=ProgressStyle(descr\u2026 <matplotlib.legend.Legend at 0x7f6d038db290> Ok this definitely seems to work! The posterior correctly focuses on the true parameters with greater confidence. You can experiment yourself how this improves further with more training samples or you could try to see how many you\u2019d exactly need to keep having a satisfyingly looking posterior and high posterior sample simulations. So, what\u2019s up with the MSE? Why does it not seem so informative to constrain the posterior? In 1.6, we\u2019ll see both the power and pitfalls of summary statistics. 1.6 Prior simulations\u2019 summary statistics vs observed summary statistics \u00b6 Let\u2019s try to understand this\u2026Let\u2019s look at a histogram of the four summary statistics we\u2019ve experimented with, and see how they compare to our observed truth summary statistic vector: stats = np . concatenate (( get_3_values ( theta . numpy ()), get_MSE ( theta . numpy (), theta_o )), axis = 1 ) x_o = np . concatenate (( get_3_values ( theta_o ), np . asarray ([[ 0.0 ]])), axis = 1 ) features = [ 'y @ x=-0.5' , 'y @ x=0' , 'y @ x=0.7' , 'MSE' ] fig , axes = plt . subplots ( 1 , 4 , figsize = ( 10 , 3 )) xlabelfontsize = 10 for i , ax in enumerate ( axes . reshape ( - 1 )): ax . hist ( stats [:, i ], color = [ \"grey\" ], alpha = 0.5 , bins = 30 , density = True , histtype = 'stepfilled' , label = [ \"simulations\" ]) ax . axvline ( x_o [:, i ], label = \"observation\" ) ax . set_xlabel ( features [ i ], fontsize = xlabelfontsize ) if i == 3 : ax . legend () plt . tight_layout () We see that for the coordinates (three plots on the left), simulations cover the observation. That is: it covers it from the left and right side in each case. For the MSE, simulations never truly reach the observation \\(0.0\\) . For the trained neural network, it is strongly preferable if the simulations cover the observation. In that case, the neural network can interpolate between simulated data. Contrary to that, for the MSE, the neural network has to extrapolate : it never observes a simulation that is to the left of the observation and has to extrapolate to the region of MSE= \\(0.0\\) . This seems like a technical point but, as we saw above, it makes a huge difference in performance. 1.7 Explicit recommendations \u00b6 We give some explicit recommendation when using summary statistics - Visualize the histogram of each summary statistic and plot the value of the observation. If, for some summary statistics, the observation is not covered (or is at the very border, e.g. the MSE above), the trained neural network will struggle. - Do not use an \u201cerror\u201d as summary statistic. This is common in optimization (e.g. genetic algorithms), but it often leads to trouble in sbi due to the reason above. - Only use summary statistics that are necessary. The less summary statistics you use, the less can go wrong with them. Of course, you have to ensure that the summary statistics describe the raw data sufficiently well.","title":"Crafting summary statistics"},{"location":"tutorial/10_crafting_summary_statistics/#crafting-summary-statistics","text":"Many simulators produce outputs that are high-dimesional. For example, a simulator might generate a time series or an image. In a previous tutorial , we discussed how a neural networks can be used to learn summary statistics from such data. In this notebook, we will instead focus on hand-crafting summary statistics. We demonstrate that the choice of summary statistics can be crucial for the performance of the inference algorithm. import numpy as np import torch import matplotlib.pyplot as plt import matplotlib as mpl # sbi import sbi.utils as utils from sbi.inference.base import infer from sbi.inference import SNPE , prepare_for_sbi , simulate_for_sbi from sbi.utils.get_nn_models import posterior_nn from sbi.analysis import pairplot # remove top and right axis from plots mpl . rcParams [ 'axes.spines.right' ] = False mpl . rcParams [ 'axes.spines.top' ] = False This notebook is not intended to provide a one-fits-all approach. In fact it argues against this: it argues for the user to carefully construct their summary statistics to (i) further help the user understand his observed data, (ii) help them understand exactly what they want the model to recover from the observation and (iii) help the inference framework itself.","title":"Crafting summary statistics"},{"location":"tutorial/10_crafting_summary_statistics/#example-1-the-quadratic-function","text":"Assume we have a simulator that is given by a quadratic function: \\(x(t) = a\\cdot t^2 + b\\cdot t + c + \\epsilon\\) , where \\(\\epsilon\\) is Gaussian observation noise and \\(\\theta = \\{a, b, c\\}\\) are the parameters. Given an observed quadratic function \\(x_o\\) , we would like to recover the posterior over parameters \\(a_o\\) , \\(b_o\\) and \\(c_o\\) .","title":"Example 1: The quadratic function"},{"location":"tutorial/10_crafting_summary_statistics/#11-prior-over-parameters","text":"First we define a prior distribution over parameters \\(a\\) , \\(b\\) and \\(c\\) . Here, we use a uniform prior for \\(a\\) , \\(b\\) and \\(c\\) to go from \\(-1\\) to \\(1\\) . prior_min = [ - 1 , - 1 , - 1 ] prior_max = [ 1 , 1 , 1 ] prior = utils . torchutils . BoxUniform ( low = torch . as_tensor ( prior_min ), high = torch . as_tensor ( prior_max ) )","title":"1.1 Prior over parameters"},{"location":"tutorial/10_crafting_summary_statistics/#12-simulator","text":"Defining some helper functions first: def create_t_x ( theta , seed = None ): ''' Return an t, x array for plotting based on params ''' if theta . ndim == 1 : theta = theta [ np . newaxis ,:] if seed is not None : rng = np . random . RandomState ( seed ) else : rng = np . random . RandomState () t = np . linspace ( - 1 , 1 , 200 ) ts = np . repeat ( t [:, np . newaxis ], theta . shape [ 0 ], axis = 1 ) x = theta [:, 0 ] * ts ** 2 + theta [:, 1 ] * ts + theta [:, 2 ] + 0.01 * rng . randn ( ts . shape [ 0 ], theta . shape [ 0 ]) return t , x def eval ( theta , t , seed = None ): ''' Evaluate the quadratic function at 't' ''' if theta . ndim == 1 : theta = theta [ np . newaxis ,:] if seed is not None : rng = np . random . RandomState ( seed ) else : rng = np . random . RandomState () return theta [:, 0 ] * t ** 2 + theta [:, 1 ] * t + theta [:, 2 ] + 0.01 * rng . randn ( 1 ) In this example, we generate the observation \\(x_o\\) from parameters \\(\\theta_o=(a_o, b_o, c_o)=(0.3, -0.2, -0.1)\\) . The observation as follows. theta_o = np . array ([ 0.3 , - 0.2 , - 0.1 ]) t , x = create_t_x ( theta_o ) plt . plot ( t , x , 'k' ) [<matplotlib.lines.Line2D at 0x7fbfda9d6e50>]","title":"1.2 Simulator"},{"location":"tutorial/10_crafting_summary_statistics/#13-summary-statistics","text":"We will compare two methods for defining summary statistics. One method uses three summary statistics which are function evaluations at three points in time. The other method uses a single summary statistic: the mean squared error between the observed and the simulated trace. In the second case, one then tries to obtain the posterior \\(p(\\theta | 0)\\) , i.e. the error being zero. These two methods are implemented below: \\(\\textbf{get_3_values()}\\) returns 3 function evaluations at \\(x=-0.5, x=0\\) and \\(x=0.75\\) . \\(\\textbf{get_MSE()}\\) returns the mean squared error between true and a quadratic function corresponding to a prior distributions sample. def get_3_values ( theta , seed = None ): ''' Return 3 'y' values corresponding to x=-0.5,0,0.75 as summary statistic vector ''' return np . array ([ eval ( theta , - 0.5 , seed = seed ), eval ( theta , 0 , seed = seed ), eval ( theta , 0.75 , seed = seed )]) . T def get_MSE ( theta , theta_o , seed = None ): ''' Return the mean-squared error (MSE) i.e. Euclidean distance from the observation function ''' _ , y = create_t_x ( theta_o , seed = seed ) # truth _ , y_ = create_t_x ( theta , seed = seed ) # simulations return np . mean ( np . square ( y_ - y ), axis = 0 , keepdims = True ) . T # MSE Let\u2019s try a couple of samples from our prior and see their summary statistics. Notice that these indeed change in small amounts every time you rerun it due to the noise, except if you set the seed.","title":"1.3 Summary statistics"},{"location":"tutorial/10_crafting_summary_statistics/#14-simulating-data","text":"Let us see various plots of prior samples and their summary statistics versus the truth, i.e. our artificial observation. t , x_truth = create_t_x ( theta_o ) plt . plot ( t , x_truth , 'k' , zorder = 1 , label = 'truth' ) n_samples = 100 theta = prior . sample (( n_samples ,)) t , x = create_t_x ( theta . numpy ()) plt . plot ( t , x , 'grey' , zorder = 0 ); plt . legend () <matplotlib.legend.Legend at 0x7fbfd88724c0> In summary, we defined reasonable summary statistics and, a priori, there might be an appararent reason why one method would be better than another. When we do inference, we\u2019d like our posterior to focus around parameter samples that have their simulated MSE very close to 0 (i.e. the truth MSE summary statistic) or their 3 extracted \\((t, x)\\) coordinates to be the truthful ones.","title":"1.4 Simulating data"},{"location":"tutorial/10_crafting_summary_statistics/#15-inference","text":"","title":"1.5 Inference"},{"location":"tutorial/10_crafting_summary_statistics/#151-using-the-mse","text":"Let\u2019s see if we can use the MSE to recover the true observation parameters \\(\\theta_o=(a_0,b_0,c_0)\\) . theta = prior . sample (( 1000 ,)) x = get_MSE ( theta . numpy (), theta_o ) theta = torch . as_tensor ( theta , dtype = torch . float32 ) x = torch . as_tensor ( x , dtype = torch . float32 ) inference = SNPE ( prior ) _ = inference . append_simulations ( theta , x ) . train () posterior = inference . build_posterior () Neural network successfully converged after 124 epochs. Now that we\u2019ve build the posterior as such, we can see how likely it finds certain parameters given that we tell it that we\u2019ve observed a certain summary statistic (in this case the MSE). We can then sample from it. x_o = torch . as_tensor ([[ 0.0 ,]]) theta_p = posterior . sample (( 10000 ,), x = x_o ) Drawing 10000 posterior samples: 0%| | 0/10000 [00:00<?, ?it/s] fig , axes = pairplot ( theta_p , limits = list ( zip ( prior_min , prior_max )), ticks = list ( zip ( prior_min , prior_max )), figsize = ( 7 , 7 ), labels = [ 'a' , 'b' , 'c' ], points_offdiag = { 'markersize' : 6 }, points_colors = 'r' , points = theta_o ); The posterior seems to pretty broad: i.e. it is not so certain about the \u2018true\u2019 parameters (here showcased in red). x_o_t , x_o_x = create_t_x ( theta_o ) plt . plot ( x_o_t , x_o_x , 'k' , zorder = 1 , label = 'truth' ) theta_p = posterior . sample (( 10 ,), x = x_o ) x_t , x_x = create_t_x ( theta_p . numpy ()) plt . plot ( x_t , x_x , 'grey' , zorder = 0 ) plt . legend () Drawing 10 posterior samples: 0%| | 0/10 [00:00<?, ?it/s] <matplotlib.legend.Legend at 0x7fbfd4241640> The functions are a bit closer to the observation than prior samples, but many posterior samples generate activity that is very far off from the observation. We would expect sbi do better on such a simple example. So what\u2019s going on? Do we need more simulations? Feel free to try, but below we will show that one can use the same number of simulation samples with different summary statistics and do much better.","title":"1.5.1 Using the MSE"},{"location":"tutorial/10_crafting_summary_statistics/#152-using-3-coordinates-as-summary-statistics","text":"x = get_3_values ( theta . numpy ()) x = torch . as_tensor ( x , dtype = torch . float32 ) inference = SNPE ( prior ) _ = inference . append_simulations ( theta , x ) . train () posterior = inference . build_posterior () Neural network successfully converged after 107 epochs. The observation is now given by the values of the observed trace at three different coordinates: x_o = torch . as_tensor ( get_3_values ( theta_o ), dtype = float ) theta_p = posterior . sample (( 10000 ,), x = x_o ) fig , axes = pairplot ( theta_p , limits = list ( zip ( prior_min , prior_max )), ticks = list ( zip ( prior_min , prior_max )), figsize = ( 7 , 7 ), labels = [ 'a' , 'b' , 'c' ], points_offdiag = { 'markersize' : 6 }, points_colors = 'r' , points = theta_o ); Drawing 10000 posterior samples: 0%| | 0/10000 [00:00<?, ?it/s] x_o_x , x_o_y = create_x_y ( theta_o ) plt . plot ( x_o_x , x_o_y , 'k' , zorder = 1 , label = 'truth' ) theta_p = posterior . sample (( 100 ,), x = x_o ) ind_10_highest = np . argsort ( np . array ( posterior . log_prob ( theta = theta_p , x = x_o )))[ - 10 :] theta_p_considered = theta_p [ ind_10_highest ,:] x_x , x_y = create_x_y ( theta_p_considered . numpy ()) plt . plot ( x_x , x_y , 'grey' , zorder = 0 ); plt . legend () HBox(children=(FloatProgress(value=0.0, description='Drawing 100 posterior samples', style=ProgressStyle(descr\u2026 <matplotlib.legend.Legend at 0x7f6d038db290> Ok this definitely seems to work! The posterior correctly focuses on the true parameters with greater confidence. You can experiment yourself how this improves further with more training samples or you could try to see how many you\u2019d exactly need to keep having a satisfyingly looking posterior and high posterior sample simulations. So, what\u2019s up with the MSE? Why does it not seem so informative to constrain the posterior? In 1.6, we\u2019ll see both the power and pitfalls of summary statistics.","title":"1.5.2 Using 3 coordinates as summary statistics"},{"location":"tutorial/10_crafting_summary_statistics/#16-prior-simulations-summary-statistics-vs-observed-summary-statistics","text":"Let\u2019s try to understand this\u2026Let\u2019s look at a histogram of the four summary statistics we\u2019ve experimented with, and see how they compare to our observed truth summary statistic vector: stats = np . concatenate (( get_3_values ( theta . numpy ()), get_MSE ( theta . numpy (), theta_o )), axis = 1 ) x_o = np . concatenate (( get_3_values ( theta_o ), np . asarray ([[ 0.0 ]])), axis = 1 ) features = [ 'y @ x=-0.5' , 'y @ x=0' , 'y @ x=0.7' , 'MSE' ] fig , axes = plt . subplots ( 1 , 4 , figsize = ( 10 , 3 )) xlabelfontsize = 10 for i , ax in enumerate ( axes . reshape ( - 1 )): ax . hist ( stats [:, i ], color = [ \"grey\" ], alpha = 0.5 , bins = 30 , density = True , histtype = 'stepfilled' , label = [ \"simulations\" ]) ax . axvline ( x_o [:, i ], label = \"observation\" ) ax . set_xlabel ( features [ i ], fontsize = xlabelfontsize ) if i == 3 : ax . legend () plt . tight_layout () We see that for the coordinates (three plots on the left), simulations cover the observation. That is: it covers it from the left and right side in each case. For the MSE, simulations never truly reach the observation \\(0.0\\) . For the trained neural network, it is strongly preferable if the simulations cover the observation. In that case, the neural network can interpolate between simulated data. Contrary to that, for the MSE, the neural network has to extrapolate : it never observes a simulation that is to the left of the observation and has to extrapolate to the region of MSE= \\(0.0\\) . This seems like a technical point but, as we saw above, it makes a huge difference in performance.","title":"1.6 Prior simulations' summary statistics vs observed summary statistics"},{"location":"tutorial/10_crafting_summary_statistics/#17-explicit-recommendations","text":"We give some explicit recommendation when using summary statistics - Visualize the histogram of each summary statistic and plot the value of the observation. If, for some summary statistics, the observation is not covered (or is at the very border, e.g. the MSE above), the trained neural network will struggle. - Do not use an \u201cerror\u201d as summary statistic. This is common in optimization (e.g. genetic algorithms), but it often leads to trouble in sbi due to the reason above. - Only use summary statistics that are necessary. The less summary statistics you use, the less can go wrong with them. Of course, you have to ensure that the summary statistics describe the raw data sufficiently well.","title":"1.7 Explicit recommendations"}]}