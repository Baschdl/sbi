import torch
import torch.distributions as distrib
from torch.distributions import transform_to

import pyro
from pyro.distributions import transforms, TransformedDistribution

from sbi.vi.mixture_of_flows import MixtureOfFlows


def build_flow(
    event_shape: torch.Size,
    support: torch.distributions.constraints.Constraint,
    num_flows: int = 5,
    type: str = "spline_autoregressive",
    permute: bool = True,
    batch_norm: bool = False,
    base_dist: torch.distributions.Distribution = None,
    **kwargs,
) -> pyro.distributions.TransformedDistribution:
    r"""Generates a Transformed Distribution where the base_dist is transformed by
       num_flows normalizing flows of specified type.
    
    
    
    Args:
        event_shape: Dimension of the events generated by the distribution.
        support: The support of the distribution.
        num_flows: Number of normalizing flows that are concatenated.
        type: The type of normalizing flow. Should be one of iaf, planar, radial,
        affine_coupling, spline, spline_autoregressive, spline_coupling
        permute: Permute dimension after each layer. This may helpfull for
        autoregressive or coupling nets.
        batch_norm: Perform batch normalization.
        base_dist: Base distribution.
    
    Returns:
        pyro.distributions.TransformedDistribution: [description]
    
    """

    # Base distribution is standard normal if not specified
    if base_dist is None:
        base_dist = distrib.Independent(
            distrib.Normal(torch.zeros(event_shape), torch.ones(event_shape)), 1
        )
    # Base dist must have same dimension
    assert base_dist.event_shape == event_shape
    # Generate normalizing flow
    dim = event_shape[0]
    flows = []
    for i in range(num_flows):
        flows.append(flow_block(dim, type, **kwargs).with_cache())
        if permute and i < num_flows - 1:
            flows.append(transforms.permute(dim).with_cache())
        if batch_norm and i < num_flows - 1:
            flows.append(transforms.batchnorm(dim))
    link_flow = transform_to(support)
    flows.append(link_flow.with_cache())
    dist = pyro.distributions.TransformedDistribution(base_dist, flows)
    return dist


def build_mof(
    event_shape: torch.Size,
    support: torch.distributions.constraints.Constraint,
    num_comps: int,
    num_flows: int = 3,
    **kwargs,
) -> MixtureOfFlows:
    """ This builds a mixture of normalizing flows.

    Args:
        event_shape: Dimension of the events
        support: Support of the distribution.
        num_comps: Number of mixture components
        num_flows: Number of flows per mixture component
    
    Returns:
        MixtureOfFlows: Pytorch module that implements a trainable mixture of flows.
    
    """
    components = []
    for k in range(num_comps):
        flow = build_flow(event_shape, support, num_flows=3, **kwargs)
        components.append(flow)
    return MixtureOfFlows(components)


def build_q(
    event_shape: torch.Size,
    support: torch.distributions.constraints.Constraint,
    flow: str = "spline_autoregressive",
    num_comps: int = 1,
    **kwargs,
):
    if num_comps > 1:
        return build_mof(event_shape, support, num_comps=num_comps, type=flow, **kwargs)
    else:
        return build_flow(event_shape, support, type=flow, **kwargs)


def flow_block(dim, type, **kwargs):
    r""" Gives pyro flow of specified type.
    Args:
        dim: Event shape of input. 
        type: Type, should be one of 
    
    Returns:
        pyro.distributions.transform: Transform object of specified type
    
    """
    if type.lower() == "planar":
        flow = transforms.planar(dim, **kwargs)
    elif type.lower() == "radial":
        flow = transforms.radial(dim, **kwargs)
    elif type.lower() == "sylvester":
        flow = transforms.sylvester(dim, **kwargs)
    elif type.lower() == "polynomial":
        flow = transforms.polynomial(dim, **kwargs)
    elif type.lower() == "affine_coupling":
        flow = transforms.affine_coupling(dim, **kwargs)
    elif type.lower() == "affine_autoregressive":
        flow = transforms.affine_autoregressive(dim, **kwargs)
    elif type.lower() == "neural_autoregressive":
        flow = transforms.neural_autoregressive(dim, **kwargs)
    elif type.lower() == "block_autoregressive":
        flow = transforms.block_autoregressive(dim, **kwargs)
    elif type.lower() == "spline":
        flow = transforms.spline(dim, **kwargs)
    elif type.lower() == "spline_autoregressive":
        flow = transforms.spline_autoregressive(dim, **kwargs)
    elif type.lower() == "spline_coupling":
        flow = transforms.spline_coupling(dim, **kwargs)
    else:
        raise NotImplementedError()
    return flow
