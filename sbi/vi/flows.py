import torch
from torch import nn
from torch.distributions import Distribution, Normal, Independent, transforms
from torch.distributions.constraints import Constraint, real
from torch.distributions.transforms import ComposeTransform, Transform
from torch.distributions import biject_to

from typing import Optional, Iterable

import pyro
from pyro.distributions import transforms

TYPES = [
    "planar",
    "radial",
    "sylvester",
    "affine_diag",
    "affine_coupling",
    "affine_autoregressive",
    "neural_autoregressive",
    "block_autoregressive",
    "spline",
    "spline_coupling",
    "spline_autoregressive",
]


def get_parameters(t: Transform):
    """ Rekursive helper function to determine all possible parameters """
    if hasattr(t, "parameters"):
        yield from t.parameters()
    elif isinstance(t, ComposeTransform):
        for part in t.parts:
            yield from get_parameters(part)
    else:
        pass


def get_modules(t: Transform):
    """ Rekursive helper function to determine all modules """
    if isinstance(t, nn.Module):
        yield t
    elif isinstance(t, ComposeTransform):
        for part in t.parts:
            yield from get_modules(part)
    else:
        pass


class TransformedDistribution(torch.distributions.TransformedDistribution):
    """ This is TransformedDistribution with the capability to return parameters!"""

    def parameters(self):
        for t in self.transforms:
            yield from get_parameters(t)

    def modules(self):
        for t in self.transforms:
            yield from get_modules(t)


class AffineTransform(transforms.AffineTransform):
    """ Trainable version of an Affine transform. This can be used to get diagonal
    gaussian approximation """

    def parameters(self):
        self.loc.requires_grad_(True)
        self.scale.requires_grad_(True)
        yield self.loc
        yield self.scale

    def with_cache(self, cache_size=1):
        if self._cache_size == cache_size:
            return self
        return AffineTransform(self.loc, self.scale, cache_size=cache_size)


class LowerCholeskyAffine(transforms.LowerCholeskyAffine):
    """ Trainable version of a Lower Cholesky Affine transform. This can be used to get
full Gaussian approximations."""

    def parameters(self):
        self.loc.requires_grad_(True)
        self.scale_tril.requires_grad_(True)
        yield self.loc
        yield self.scale_tril

    def with_cache(self, cache_size=1):
        if self._cache_size == cache_size:
            return self
        return LowerCholeskyAffine(self.loc, self.scale_tril, cache_size=cache_size)

    def log_abs_det_jacobian(self, x, y):
        """ This modification allows batched scale_tril matrices. """
        dim = self.scale_tril.dim()
        return torch.ones(
            x.size()[:-1], dtype=x.dtype, layout=x.layout, device=x.device
        ) * torch.diagonal(self.scale_tril, dim1=dim - 2, dim2=dim - 1).log().sum(1)


def build_flow(
    event_shape: torch.Size,
    support: Constraint = real,
    num_flows: int = 5,
    type: str = "affine_autoregressive",
    permute: bool = True,
    batch_norm: bool = False,
    base_dist: Distribution = None,
    **kwargs,
) -> TransformedDistribution:
    f"""Generates a Transformed Distribution where the base_dist is transformed by
       num_flows normalizing flows of specified type.
    
    
    
    Args:
        event_shape: Dimension of the events generated by the distribution.
        support: The support of the distribution.
        num_flows: Number of normalizing flows that are concatenated.
        type: The type of normalizing flow. Should be one of {TYPES}
        permute: Permute dimension after each layer. This may helpfull for
        autoregressive or coupling nets.
        batch_norm: Perform batch normalization.
        base_dist: Base distribution.
        kwargs
    Returns:
        TransformedDistribution
    
    """

    # Base distribution is standard normal if not specified
    if base_dist is None:
        base_dist = Independent(
            Normal(torch.zeros(event_shape), torch.ones(event_shape)), 1
        )
    # Generate normalizing flow
    if isinstance(event_shape, int):
        dim = event_shape
    elif isinstance(event_shape, Iterable):
        dim = event_shape[-1]
    else:
        raise ValueError("The eventshape must either be an Integer or a Iterable.")

    flows = []
    for i in range(num_flows):
        flows.append(flow_block(dim, type, **kwargs).with_cache())
        if permute and i < num_flows - 1:
            flows.append(transforms.permute(dim).with_cache())
        if batch_norm and i < num_flows - 1:
            flows.append(transforms.batchnorm(dim))
    link_flow = biject_to(support)
    flows.append(link_flow.with_cache())
    dist = TransformedDistribution(base_dist, flows)
    return dist


def flow_block(dim, type, **kwargs):
    r""" Gives pyro flow of specified type.
    Args:
        dim: Event shape of input. 
        type: Type, should be one of 
    
    Returns:
        pyro.distributions.transform: Transform object of specified type
    
    """
    if type.lower() == "planar":
        flow = transforms.planar(dim, **kwargs)
    elif type.lower() == "radial":
        flow = transforms.radial(dim, **kwargs)
    elif type.lower() == "sylvester":
        flow = transforms.sylvester(dim, **kwargs)
    elif type.lower() == "affine_diag":
        flow = AffineTransform(torch.zeros(dim), torch.ones(dim))
    elif type.lower() == "affine_tril":
        flow = LowerCholeskyAffine(torch.zeros(dim), torch.eye(dim))
    elif type.lower() == "affine_coupling":
        flow = transforms.affine_coupling(dim, **kwargs)
    elif type.lower() == "affine_autoregressive":
        flow = transforms.affine_autoregressive(dim, **kwargs)
    elif type.lower() == "neural_autoregressive":
        flow = transforms.neural_autoregressive(dim, **kwargs)
    elif type.lower() == "block_autoregressive":
        flow = transforms.block_autoregressive(dim, **kwargs)
    elif type.lower() == "spline":
        flow = transforms.spline(dim, **kwargs)
    elif type.lower() == "spline_coupling":
        flow = transforms.spline_coupling(dim, **kwargs)
    elif type.lower() == "spline_autoregressive":
        flow = transforms.spline_autoregressive(dim, **kwargs)
    else:
        raise NotImplementedError()
    return flow
